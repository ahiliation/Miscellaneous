<!DOCTYPE HTML PUBLIC "-W3C//DTD HTML 4.01//EN"
        "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <title>Class Notes for Basic Algorithms</title>
</head>

<body>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #1 ================
</div></big></strong>

<div align=center>
    <h1>Basic Algorithms</h1>
    2002-03 Fall<br>
    Monday and Wednesday 11-12:15<br>
    Ciww 102
</div>

<h1>Chapter 0 Administrivia</h1>

<p>I start at 0 so that when we get to chapter 1, the numbering will
agree with the text.

<h2>0.1 Contact Information</h2>

<ul>
<li>gottlieb@nyu.edu (best method)
<li>http://allan.ultra.nyu.edu/~gottlieb <strong>two</strong> el's in allan
<li>715 Broadway, Room 712
</ul>

<h2>0.2 Course Web Page</h2>

<p>
There is a web site for the course.  You can find it from my home
page, which is http://allan.ultra.nyu.edu/~gottlieb
<ul>
<li>You can find these lecture notes on the course home page.
    Please let me know if you can't find it.
<li>I mirror my home page on the CS web site.
<li>I also mirror the course pages on the CS web site.
<li><strong>But</strong>, the official site is allan.ultra.nyu.edu.
    It is the one I personally manage.
<li>The notes will be updated as bugs are found.
<li>I will also produce a separate page for each lecture after the
    lecture is given.  These individual pages
    might not get updated as quickly as the large page
</ul>

<h2>0.3 Textbook</h2>

<p>
The course text is Goodrich and Tamassia: ``Algorithm Design:
Foundations, Analysis, and Internet Examples.
<ul>
<li>Available in bookstore.
<li>I expect to cover nearly all of the first 7 chapters.
</ul>

<h2>0.4 Computer Accounts and Mailman Mailing List</h2>

<ul>
<li>You are entitled to a computer account, please get it asap.
<li>Sign up for the Mailman mailing list for the course.
    http://www.cs.nyu.edu/mailman/listinfo/v22_0310_002_fa02
<li>If you want to send mail just to me, use gottlieb@nyu.edu not
    the mailing list.
<li>Questions about the lectures or homeworks should go to the mailing list.
    You may answer questions  posed on the list as well.
<li>I will respond to all questions; if another student has answered the
    question before I get to it, I will confirm if the answer given is
    correct.
</ul>

<h2>0.5 Grades</h2>

<p>The major components of the grade will be the midterm, the final,
and homeworks.  I will post (soon) the weights for each.

<h2>0.6 Midterm</h2>

<p>We will have a midterm.  As the time approaches we will vote in
class for the exact date.  Please do not schedule any trips during
days when the class meets until the midterm date is scheduled.

<a id=hw-ps></a>
<h2>0.7 Homeworks, Problem Sets, and Labs</h2>

<p>If you had me for 202, you know that in systems courses I also
assign labs.  Basic algorithms is <strong>not</strong> a systems
course; there are no labs.  There are homeworks and problem sets,
very few if any of these will require the computer.  There is a
distinction between homeworks and problem sets.

<p>
Problem sets are
<ul>
<li><em>Required</em>.
<li>Due several lectures later (date given on assignment).
<li>Graded and form part of your final grade.
<li>Penalized for lateness, up to one week.
</ul>
Homeworks are
<ul>
<li>Optional.
<li>Due the beginning of <em>Next</em> lecture.
<li>Not accepted late.
<li>Mostly from the book.
<li>Collected and returned.
<li>Able to help, but not hurt, your grade.
</ul>

<h2>0.8 Recitation</h2>

<p>There is a recitation session on tuesdays from 9:30 to 10:45 in
room 109.  The recitation leader is Sean McLaughlin
&LT;seanmcl@cs.nyu.edu&GT;.

<h2>0.9 Obtaining Help</h2>

<p>Good methods for obtaining help include

<ol>
<li>Asking me during office hours (see web page for my hours).
<li>Asking the mailing list.
<li>Asking another student, but ...<br>
    <strong>Your homeworks must be your own</strong>.
</ol>

<h2>0.10 The Upper Left Board</h2>

<p>I use the upper left board for homework assignments and
announcements.  I should never erase that board.
Viewed as a file it is group readable (the group is those in the
room), appendable by just me, and (re-)writable by no one.
If you see me start to erase an announcement, let me know.

<h2>0.11 A Grade of ``Incomplete''</h2>

<p>It is university policy that a student's request for an incomplete
be granted only in exceptional circumstances and only if applied for
in advance.  Naturally, the application must be before the final exam.

<h1 align="center" style="background:pink">Part I: Fundamental Tools</h1>

<h1>Chapter 1 Algorithm Analysis</h1>

<p>We are interested in designing <q>good</q>
<strong>algorithms</strong> (a step-by-step procedure for performing
some task in a finite amount of time) and <q>good</q>
<strong>data structures</strong> (a systematic way of organizing and
accessing data).

<p>Unlike v22.102, however, we wish to determine rigorously just
<strong>how good</strong> our algorithms and data structures really
are and whether <strong>significantly better</strong> algorithms are
possible.

<h2>1.1 Methodologies for Analyzing Algorithms</h2>

<p>We will be primarily concerned with the speed (<em>time
complexity</em>) of algorithms.

<ul>
<li>Sometimes the <em>space complexity</em> is studied.
<li>The time depends on the input, most often on the size of the
    input.
<li>We can run experiments.
    <ul>
    <li>Must choose <em>sufficiently many, representative</em> inputs.
    <li>Must use identical hardware to compare algorithms.
    <li>Must <em>implement</em> the algorithm.
    </ul>
</ul>

<p>We will emphasize instead and analytic framework that is
independent of input and hardware, and does not require an
implementation.  The disadvantage is that we can only estimate the
time required.

<ul>
<li>Often we ignore multiplicative constants and small input values.
<li>So we consider <tt>f(x)=x<sup>3</sup>-20x<sup>2</sup></tt>
    equivalent to <tt>g(x)=10x<sup>3</sup>+10x<sup>2</sup></tt>
<li>Huh??
<li>Easy to see that for say <tt>x &gt; 100, f(x) &lt; 10 g(x)</tt> and
    <tt>g(x) &lt; 10 f(x)</tt>.
</ul>

<p><strong>Homework:</strong> Unless otherwise stated homework
problems are from the last section in the current book chapter.
R-1.1 and R-1.2.

<h3>1.1.1 Pseudo-Code</h3>

<p>Designed for human understanding.  Suppress unimportant details and
describe some parts in natural language (English in this course).

<h3>1.1.2 The Random Access Machine (RAM) Model</h3>

<p>The key difference from reality is the assumption of a very simple
memory model: Accessing any memory element takes a constant amount of
time.  This ignores caching and paging for example.  (It also assumes
the word-size of a computer is large enough to hold any address.  This
last assumption is generally valid for modern-day computers, but was
not always the case.)

<p>The time required is simply a count of the <strong>primitive
operations</strong> executed.  Primitive operations include

<ol>
<li>Assign a value to a variable (independent of the size of the
    value; but the variable must be a scalar).
<li>Method invocation, i.e., calling a function or subroutine.
<li>Performing a (simple) arithmetic operation (divide is OK,
    logarithm is not).
<li>Indexing into an array (for now just one dimensional; scalar
    access is free).
<li>Following an object reference.
<li>Returning from a method.
</ol>

<h3>1.1.3 Counting Primitive Operations</h3>

<p>Let's start with a simple algorithm (the book does a different
simple algorithm, maximum).

<pre>
Algorithm innerProduct
    Input: Non-negative integer n and two integer arrays A and B of size n.
    Output: The inner product of the two arrays

prod &larr; 0
for i &larr; 0 to n-1 do
    prod &larr; prod + A[i]*B[i]
return prod
</pre>

<ul>
<li>Line 1 is one op (assigning a value).
<li>Loop initializing is one op (assigning a value).
<li>Line 3 is five ops per iteration (mult, add, 2 array refs, assign).
<li>Line 3 is executed n times; total is 5n.
<li>Loop incrementation is two ops (an addition and an assignment)
<li>Loop incrementation is done n times; total is 2n.
<li>Loop termination test is one op (a comparison i&lt;n) each time.
<li>Loop termination is done n+1 times (n successes, one failure);
    total is n+1.
<li>Return is one op.
</ul>

<p>The total is thus <tt>1+1+5n+2n+(n+1)+1 = 8n+4</tt>.

<p>Let's improve it (a very little bit)

<pre>
Algorithm innerProductBetter
    Input: Non-negative integer n and two integer arrays A and B of size n.
    Output: The inner product of the two arrays

prod &larr; A[0]*B[0]
for i &larr; 1 to n-1 do
    prod &larr; prod + A[i]*B[i]
return prod
</pre>

<p>The cost is <tt>4+1+5(n-1)+2(n-1)+n+1 = 8n-1</tt>

<p><Big><strong>THIS ALGORITHM IS WRONG!!</strong></Big>

<p>If n=0, we access A[0] and B[0], which do not exist.  The original
version returns zero as the inner product of empty arrays, which is
arguably correct.  The best fix is perhaps to change <q>Non-negative</q>
to <q>Positive</q>.  Let's call this algorithm innerProductBetterFixed.

<p>What about if statements?

<pre>
Algorithm countPositives
    Input: Non-negative integer n and an integer array A of size n.
    Output: The number of positive elements in A

pos &larr; 0
for i &larr; 0 to n-1 do
    if A[i] &gt; 0 then
        pos &larr; pos + 1
return pos
</pre>

<ul>
<li>Line 1 is one op.
<li>Loop initialization is one op
<li>Loop termination test is n+1 ops
<li>The if test is performed n times; each is 2 ops
<li>Return is one op
<li>The update of pos is 2 ops but is done ??? times.
<li>What do we do?
</ul>

<P>Let <tt>U</tt> be the number of updates done.

<ul>
<li>The total number of steps is <tt>1+1+(n+1)+2n+1+2U = 4+3n+2U</tt>.
<li>The <strong>best case</strong> occurs when <tt>U=0</tt> (i.e., no
    numbers are positive and gives an answer of 4+3n.
<li>The <strong>worst case</strong> occurs when <tt>U=n</tt> (i.e., all
    numbers are positive and gives an answer of 4+5n.
<li>To determine the <strong>average case</strong> result is much
    harder as it requires knowing the input distribution (i.e., are
    positive numbers likely) and requires probability theory.
</ul>

<h3>1.1.4 Analyzing Recursive Algorithms</h3>

<p>Consider a recursive version of innerProduct.  If the arrays are of
size 1, the answer is clearly A[0]B[0].  If n&gt;1, we recursively get
the inner product of the first n-1 terms and then add in the last term.

<pre>
Algorithm innerProductRecursive
    Input: Positive integer n and two integer arrays A and B of size n.
    Output: The inner product of the two arrays

if n=1 then
    return A[0]B[0]
return innerProductRecursive(n-1,A,B) + A[n-1]B[n-1]
</pre>

<p>How many steps does the algorithm require?  Let T(n) be
the number of steps required.

<ul>
<li>If n=1 we do a comparison, two fetches, a product, and a return.
<li>So T(1)=5.
<li>If n&gt;1, we do a comparison, a subtraction, a method call, the
    recursive computation, two fetches, a product, a sum and a return.
<li>So T(n) = 1 + 1 + 1 + T(n-1) + 2 + 1 + 1 + 1 = T(n-1)+8.
<li>This is called a <strong>recurrence equation</strong>.  In general
    these are quite difficult to solve in <strong>closed
    form</strong>, i.e. without T on the right hand side.
<li>For this simple recurrence, one can see that T(n)=8n-3 is the
    solution.
<li>We will learn more about recurrences later.
</ul>

<p><strong>Homework:</strong> R-1.27

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #2 ================
</div></big></strong>

<p><strong>Homework:</strong> I should have given some last time.
It is listed in the notes (search for homework).  Also some will be
listed this time.  <strong>BUT</strong>, due to the Jewish holiday,
none is officially assigned.  You can get started if you wish since
all will eventually be assigned, but none will be collected next class.

<h2>1.2 Asymptotic Notation</h2>

<p>Now we are going to be less precise and worry only about approximate
answers for large inputs.

<h3>1.2.1 The <q>Big-Oh</q> Notation</h3>

<p><strong>Definition</strong>: Let f(n) and g(n) be real-valued
functions of a single non-negative integer argument.
We write <tt>f(n)</tt> is <tt>O(g(n))</tt> if there is a positive real
number <tt>c</tt> and a positive integer <tt>n<sub>0</sub></tt>
such that <tt>f(n)&le;cg(n)</tt> for all <tt>n&ge;n<sub>0</sub></tt>.

<p>What does this mean?

<p>For large inputs (<tt>n&le;n<sub>0</sub></tt>), <tt>f</tt> is not much
bigger than <tt>g</tt> (<tt>f(n)&le;cg(n)</tt>).

<p>Examples to do on the board

<ol>
<li><tt>3n-6</tt> is <tt>O(n)</tt>.  Some less common ways of
    saying the same thing follow.
<li><tt>3x-6</tt> is <tt>O(x)</tt>.
<li>If <tt>f(y)=3y-6</tt> and <tt>id(y)=y</tt>,
    then <tt>f(y)</tt> is <tt>O(id(y))</tt>.
<li><tt>3n-6</tt> is <tt>O(2n)</tt>
<li><tt>9n<sup>4</sup>+12n<sup>2</sup>+1234</tt> is
<tt>O(n<sup>4</sup>)</tt>.
<li><tt>innerProduct</tt> is <tt>O(n)</tt>
<li><tt>innerProductBetter</tt> is <tt>O(n)</tt>
<li><tt>innerProductFixed</tt> is <tt>O(n)</tt>
<li><tt>countPositives</tt> is <tt>O(n)</tt>
<li><tt>n+log(n)</tt> is <tt>O(n)</tt>.
<li><tt>log(n)+5log(log(n))</tt> is <tt>O(log(n))</tt>.
<li><tt>12345<sup>54321</sup></tt> is <tt>O(1)</tt>.
<li><tt>3/n</tt> is <tt>O(1)</tt>.  True but not the best.
<li><tt>3/n</tt> is <tt>O(1/n)</tt>.  Much better.
<li><tt>innerProduct</tt> is <tt>O(100n+log(n)+34.5)</tt>.  True, but
    awful.
</ol>

<p>A few theorems give us rules that make calculating big-Oh easier.

<p><strong>Theorem</strong> (arithmetic): Let <tt>d(n)</tt>,
<tt>e(n)</tt>, <tt>f(n)</tt>, and <tt>g(n)</tt> be nonnegative
real-valued functions of a nonnegative integer argument and assume
<tt>d(n)</tt> is <tt>O(f(n))</tt> and <tt>e(n)</tt> is
<tt>O(g(n))</tt>.  Then

<ol>
<li><tt>ad(n)</tt> is <tt>O(f(n))</tt> for any nonnegative a
<li><tt>d(n)+e(n)</tt> is <tt>O(f(n)+g(n))</tt>
<li><tt>d(n)e(n)</tt> is <tt>O(f(n)g(n))</tt>
</ol>

<p><strong>Theorem</strong> (transitivity): Let <tt>d(n)</tt>,
<tt>f(n)</tt>, and <tt>g(n)</tt> be nonnegative real-valued functions
of a nonnegative integer argument and assume <tt>d(n)</tt> is
<tt>O(f(n))</tt> and <tt>f(n)</tt> is <tt>O(g(n))</tt>.  Then
<tt>d(n)</tt> is <tt>O(g(n))</tt>.

<p><strong>Theorem</strong> (special functions): (Only <tt>n</tt> varies)

<ol>
<li>If <tt>f(n)</tt> is a polynomial of degree d, then
    <tt>f(n)</tt> is <tt>O(n<sup>d</sup>)</tt>.
<li><tt>n<sup>x</sup></tt> is <tt>O(a<sup>n</sup>)</tt> for any
    <tt>x&gt;0</tt> <tt>and a&gt;1</tt>.
<li><tt>log(n<sup>x</sup>)</tt> is <tt>O(log(n))</tt> for any <tt>x&gt;0</tt>
<li><tt>(log(n))<sup>x</sup></tt> is <tt>O(n<sup>y</sup>)</tt> for any
    <tt>x&gt;0</tt> and <tt>y&gt;0</tt>.
</ol>

<p><strong>Homework:</strong> R-1.19 R-1.20

<p><strong>Definitions</strong>: (Common names)
<ol>

<li>If a function is <tt>O(log(n))</tt> we call it <strong>logarithmic</strong>.
<li>If a function is <tt>O(n)</tt> we call it <strong>linear</strong>.
<li>If a function is <tt>O(n<sup>2</sup>)</tt> we call it
    <strong>quadratic</strong>.
<li>If a function is <tt>O(n<sup>k</sup></tt>) with <tt>k&ge;1</tt>, we call it
    <strong>polynomial</strong>.
<li>If a function is <tt>O(a<sup>n</sup>)</tt> with <tt>a&gt;1</tt>,
    we call it <strong>exponential</strong>.
</ol>

<strong>Remark</strong>: The last definitions would be better with a
relative of big-Oh, namely big-Theta, since, for example <tt>3log(n)</tt> is
<tt>O(n<sup>2</sup>)</tt>, but we do <em>not</em> call
<tt>3log(n)</tt> quadratic.

<p><strong>Homework:</strong> R-1.10 and R-1.12.

<p>R-1.13: The outer (i) loop is done 2n times.  For each outer
iteration the inner loop is done i times.  Each inner iteration is a
constant number of steps so each inner loop is O(i), which is the time
for the ith iteration of the outer loop.  So the entire outer loop is
&sigma;O(i) i from 0 to 2n, which is O(n<sup>2</sup>).

<h3>1.2.2 <q>Relatives</q> of the Big-Oh</h3>

<h4>Big-Omega and Big-Theta</h4>

<p>Recall that f(n) is O(g(n)) if for large n, f is not much bigger
than g.  That is g is some sort of <strong>upper</strong> bound on f.
How about a definition for the case when g is (in the same sense) a
<strong>lower</strong> bound for f?

<p><Strong>Definition</Strong>: Let f(n) and g(n) be real valued
functions of an integer value.  Then <strong>f(n) is
&Omega;(g(n))</strong> if g(n) is O(f(n)).

<p><strong>Remarks</strong>:
<ol>
<li>We pronounce f(n) is &Omega;(g(n)) as "f(n) is big-Omega of g(n)".
<li>What the last definition says is that we say f(n) is not much smaller
    than g(n) if g(n) is not much bigger than f(n), which sounds
    reasonable to me.
<li>What if f(n) and g(n) are about equal, i.e., neither is much
    bigger than the other?
</ol>

<p><strong>Definition</strong>: We write
<strong>f(n) is &Theta;(g(n))</strong> if <strong>both</strong>
f(n) is O(g(n)) <strong>and</strong> f(n) is &Omega;(g(n)).

<p><strong>Remarks</strong>
We pronounce f(n) is &Theta;(g(n)) as "f(n) is big-Theta of g(n)"

<p>Examples to do on the board.
<ol>
<li>2x<sup>2</sup>+3x is &theta;(x<sup>2</sup>).
<li>2x<sup>3</sup>+3x is <strong>not</strong> &theta;(x<sup>2</sup>).
<li>2x<sup>3</sup>+3x is &Omega;(x<sup>2</sup>).
<li>innerProductRecutsive is &Theta;(n).
<li>binarySearch is &Theta;(log(n)).  Unofficial for now.
<li>If f(n) is &Theta;(g(n)), the f(n) is &Omega(g(n)).
<li>If f(n) is &Theta;(g(n)), then f(n) is O(g(n)).
</ol>

<p><strong>Homework:</strong> R-1.6

<h4>Little-Oh and Little-Omega</h4>

<p>Recall that big-Oh captures the idea that for large n, f(n) is not
much bigger than g(n).  Now we want to capture the idea that, for
large n, f(n) is tiny compared to g(n).

<p>If you remember limits from calculus, what we want is that
f(n)/g(n)&rarr;0 as n&rarr;&infin;.  However, the definition we give
does not use limits (it essentially has the definition of a limit
built in).

<p><strong>Definition</strong>:  Let f(n) and g(n) be real valued
functions of an integer variable.  We say
<strong>f(n) is o(g(n))</strong>
if for <strong>any</strong> c&gt;0,
there is an n<sub>0</sub> such that
f(n)&le;g(n) for all n&gt;n<sub>0</sub>.
This is pronounced as "f(n) is little-oh of g(n)".

<p><strong>Definition</strong>: Let f(n) and g(n) be real valued
functions of an integer variable.  We say
<strong>f(n) is &omega;(g(n)</strong> if
g(n) is o(f(n)).  This is pronounced as "f(n) is little-omega of g(n)".

<p><strong>Examples</strong>: log(n) is o(n) and x<sup>2</sup> is
&omega;(nlog(n)).

<p><strong>Homework:</strong> R-1.4. R-1.22

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #3 ================
</div></big></strong>

<p><strong>Remark</strong>: I changed my mind about homework.  Too many
to have each one really graded.  We now have homeworks and problem
sets as explained <a href="#hw-ps">here</a>.

<h4>What is "fast" or "efficient"?</h4>

<p>If the asymptotic time complexity is bad, say n<sup>5</sup>, or
horrendous, say 2<sup>n</sup>, then for large n, the algorithm will
definitely be slow.  Indeed for exponential algorithms even modest n's
(say n=50) are hopeless.

<p>Algorithms that are o(n) (i.e., faster than linear, a.k.a. sub-linear),
e.g. logarithmic algorithms, are very fast and quite rare.
Note that such algorithms do not even inspect most of the input data
once.  Binary search has this property.  When you look a name in the
phone book you do not even glance at a majority of the names present.

<p>Linear algorithms (i.e., &Theta;(n)) are also fast.  Indeed, if the
time complexity is O(nlog(n)), we are normally quite happy.

<p>Low degree polynomial (e.g., &Theta;(n<sup>2</sup>),
&Theta;(n<sup>3</sup>), &Theta;(n<sup>4</sup>)) are interesting.  They
are certainly not fast but speeding up a computer system by a factor
of 1000 (feasible today with parallelism) means that a
&Theta;(n<sup>3</sup>) algorithm can solve a problem 10 times larger.
Many science/engineering problems are in this range.

<h3>1.2.3 The Importance of Asymptotics</h3>

<p>It really is true that if algorithm A is o(algorithm B) then for
large problems A will take <strong>much</strong> less time than B.

<p><strong>Definition</strong>: If (the number of operations in)
algorithm A is o(algorithm B), we call A
<strong>asymptotically faster</strong> than B.

<p><strong>Example:</strong>: The following sequence of functions are
ordered by <strong>growth rate</strong>, i.e., each function is
little-oh of the subsequent function.
<br>log(log(n)), log(n), (log(n))<sup>2</sup>, n<sup>1/3</sup>,
n<sup>1/2</sup>, n, nlog(n), n<sup>2</sup>/(log(n)), n<sup>2</sup>,
n<sup>3</sup>, 2<sup>n</sup>.

<h4>What about those constants that we have swept under the rug?</h4>

<p>Modest multiplicative constants (as well as immodest additive
constants) don't cause too much trouble.  But there are algorithms
(e.g. the AKS logarithmic sorting algorithm) in which the
multiplicative constants are astronomical and hence, despite its
wonderful asymptotic complexity, the algorithm is not used in
practice.

<h4>A Great Table</h4>

<p>See table 1.10 on page 20.

<p><strong>Homework:</strong> R-1.7

<h2>1.3 A Quick Mathematical Review</h2>

<p>This is hard to type in using html.  The book is fine and I will
write the formulas on the board.

<h3>1.3.1 Summations</h3>

<p><strong>Definition</strong>:
The sigma notation: &sum;f(i) with i going from a to b.

<p><strong>Theorem</strong>: Assume 0&lt;a&ne;1.
Then &sum;a<sup>i</sup> i from 0 to n = (1-a<sup>n+1</sup>)/(1-a).

<p><strong>Proof</strong>:
Cute trick.  Multiply by a and subtract.

<p><strong>Theorem</strong>:
&sum;i from 1 to n = n(n+1)/2.

<p><strong>Proof</strong>:
Pair the 1 with the n, the 2 with the (n-1), etc.  This gives a bunch
of (n+1)s.  For n even it is clearly n/2 of them.  For odd it is the
same (look at it).

<h3>1.3.2 Logarithms and Exponents</h3>

<p>Recall that log<sub>b</sub>a = c means that b<sup>c</sup>=a.
b is called the base and c is called the exponent.

<p>What is meant by log(n) when we don't specify the base?

<ul>
<li>Some people use base 10 by default.
<li>Mathematicians use base e.
<li>We will use base 2 (common in computer science)
</ul>

<p>I assume you know what a<sup>b</sup> is.  (Actually this is not so
obvious.  Whatever 2 raised to the square root of 3 means it is
<strong>not</strong> writing 2 down the square root of 3 times and
multiplying.)  So you also know that
a<sup>x+y</sup>=a<sup>x</sup>a<sup>y</sup>.

<p><strong>Theorem</strong>:
Let a, b, and c be positive real numbers.  To ease writing, I will use
base 2 often.  This is not needed.  Any base would do.

<ol>
<li>log(ac) = log(a)+log(c)
<li>log(a/c) = log(a) - log(c)
<li>log(a<sup>c</sup>) = c log(a)
<li>log<sub>c</sub>(a) = (log(a))/log(c):
consider a = c<sup>log<sub>c</sub>a</sup> and take log of both sides.
<li>c<sup>log(a)</sup> = a <sup>log(c)</sup>: take log of both sides.
<li>(b<sup>a</sup>)<sup>c</sup> = b<sup>ac</sup>
<li>b<sup>a</sup>b<sup>c</sup> = b<sup>a+c</sup>
<li>b<sup>a</sup>/b<sup>c</sup> = b<sup>a-c</sup>
</ol>

<h4>Examples</h4>

<ul>
<li>log(2nlog(n)) = 1 + log(n) + log(log(n)) is &Theta;(log(n))
<li>log(log(sqrt(n))) = log(.5log(n)) = log(.5)+log(log(n))
    = -1 + log(log(n)) = &Theta;(log(log(n))
<li>log(2<sup>n</sup>) = n = 2<sup>log(n)</sup>
</ul>

<p><strong>Homework:</strong> C-1.12

<h4>Floor and Ceiling</h4>

<p>&lfloor;x&rfloor; is the greatest integer not greater than x.
&lceil;x&rceil; is the least integer not less than x.

<p>&lfloor;5&rfloor; = &lceil;5&rceil; = 5
<p>&lfloor;5.2&rfloor; = 5 and  &lceil;5.2&rceil; = 6
<p>&lfloor;-5.2&rfloor; = -6 and  &lceil;-5.2&rceil; = -5

<h3>1.3.3 Simple Justification Techniques</h3>

<h4>By example</h4>

<p>To prove the claim that <strong>there is an n</strong> greater than
1000, we merely have to note that 1001 is greater than 1001.

<h4>By counterexample</h4>

<p>To refute the claim that <strong>all n are</strong> greater than
1000, we merely have to note that 999 is not greater than 1000.

<h4>By contrapositive</h4>

<p>"P implies Q" is the same as "not Q implies not P".  So to show
that no prime is a square we note that "prime implies not square" is
the same is "not (not square) implies not prime", i.e.
"square implies not prime", which is obvious.

<h4>By contradiction</h4>

<p>Assume what you want to prove is <strong>false</strong> and derive
a contradiction.

<p><strong>Theorem</strong>:
There are an infinite number of primes.

<p><strong>Proof</strong>:
Assume not.  Let the primes be p<sub>1</sub> up to p<sub>k</sub> and
consider the number
A=p<sub>1</sub>p<sub>2</sub>&hellip;p<sub>k</sub>+1.
A has remainder 1 when divided by any p<sub>i</sub> so cannot have any
p<sub>i</sub> as a factor.  Factor A into primes.  None can be
p<sub>i</sub> (A may or may not be prime).  But we assumed that all
the primes were p<sub>i</sub>.  Contradiction.  Hence our assumption
that we could list all the primes was false.

<h4>By (complete) induction</h4>

<p>The goal is to show the truth of some statement for all integers
n&ge;1.  It is enough to show two things.
<ol>
<li>The statement is true for n=1
<li><strong>IF</strong> the statement is true for all k&lt;n,
    then it is true for n.
</ol>

<p><strong>Theorem</strong>:
A complete binary tree of height h has 2<sup>h</sup>-1 nodes.

<p><strong>Proof</strong>:
We write NN(h) to mean the number of nodes in a complete binary tree
of height h.
A complete binary tree of height 1 is just a root so NN(1)=1 and
2<sup>1</sup>-1 = 1.
Now we assume NN(k)=2<sup>k</sup>-1 nodes for all k&lt;h and consider a complete
binary tree of height h.  It is just a complete binary tree of height
h-1 with new leaf nodes added.  How many new leaves?
<br>Ans. 2<sup>h-1</sup> (this could be proved by induction as a lemma, but
is fairly clear without induction).

<p>Hence NN(h)=NN(h-1)+2<sup>h-1</sup> =
(2<sup>h-1</sup>-1)+2<sup>h-1</sup> =
2(2<sup>h-1</sup>)-1=2<sup>h</sup>-1.

<p><strong>Homework:</strong> R-1.9

<h4>Loop Invariants</h4>

<p>Very similar to induction.  Assume we have a loop with controlling
variable i.  For example a "<tt>for i&larr;0 to n-1</tt>" loop.  We then
associate with the loop a statement S(j) depending on j such that
<ol>
<li>S(0) is true (just) before the loop begins
<li><strong>IF</strong> S(j-1) holds before iteration j begins,
    then S(j) will hold when iteration j ends.
</ol>
By induction we see that S(n) will be true when the nth iteration
ends, i.e., when the loop ends.

<p>I favor having array and loop indexes
starting at zero.  However, here it causes us some grief.  We must
remember that iteration j occurs when i=j-1.

<p><strong>Example:</strong>:
Recall the countPositives algorithm

<pre>
Algorithm countPositives
    Input: Non-negative integer n and an integer array A of size n.
    Output: The number of positive elements in A

pos &larr; 0
for i &larr; 0 to n-1 do
    if A[i] &gt; 0 then
        pos &larr; pos + 1
return pos
</pre>

<p>Let S(j) be "pos equals the number of positive values in the first
j elements of A".

<p>Just before the loop starts S(0) is true
<strong>vacuously</strong>.  Indeed that is the purpose of the first
statement in the algorithm.

<p>Assume S(j-1) is true before iteration j, then iteration j (i.e.,
i=j-1) checks A[j-1] which is the jth element and updates pos
accordingly.  Hence S(j) is true after iteration j finishes.

<p>Hence we conclude that S(n) is true when iteration n concludes,
i.e. when the loop terminates.  Thus pos is the correct value to return.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #4 ================
</div></big></strong>

<h3>1.3.4 Basic Probability</h3>

<p>Skipped for now.

<h2>1.4  Case Studies in Algorithm Analysis</h2>

<h3>1.4.1 A Quadratic-Time Prefix Averages Algorithm</h3>

<p>We trivially improved innerProduct (same asymptotic complexity
before and after).  Now we will see a real improvement.
For simplicity I do a slightly simpler algorithm, prefix sums.

<pre>
Algorithm partialSumsSlow
    Input: Positive integer n and a real array A of size n
    Output: A real array B of size n with B[i]=A[0]+&hellip;+A[i]

for i &larr; 0 to n-1 do
    s &larr; 0
    for j &larr; 0 to i do
        s &larr; s + A[j]
    B[i] &larr; s
return B
</pre>

<p>The update of s is performed 1+2+&hellip;+n times.  Hence the
running time is &Omega;(1+2+&hellip;+n)=&Omega(n<sup>2</sup>).
In fact it is easy to see that the time is &Theta(n<sup>2</sup>).

<h3>1.4.2 A Linear-Time Prefix Averages Algorithm</h3>

<pre>
Algorithm partialSumsFast
    Input: Positive integer n and a real array A of size n
    Output: A real array B of size n with B[i]=A[0]+&hellip;+A[i]

s &larr; 0
for i &larr; 0 to n-1 do
    s &larr; s + A[i]
    B[i] &larr; s
return B
</pre>

<p>We just have a single loop and each statement inside is O(1), so
the algorithm is O(n) (in fact &Theta;(n)).

<p><strong>Homework:</strong> Write partialSumsFastNoTemps, which is also
&Theta;(n) time but avoids the use of s (it still uses i so my name is not
great).

<h2>1.5 Amortization</h2>

<p>Often we have a data structure supporting a number of operations
that will be applied many times.  For some data structures, the
worst-case running time of the operations may not give a good estimate
of how long a sequence of operations will take.

<p>If we divide the running time of the sequence by the number of
operations performed we get the average time for each operation in the
sequence,, which is called the <strong>amortized running
time</strong>.

<p>Why amortized?
<br>Because the cost of the occasional expensive application is
amortized over the numerous cheap application (I think).

<p><strong>Example:</strong>: (From the book.)  The clearable table.
This is essentially an array.  The table is initially empty (i.e., has
size zero).  We want to support three operations.
<ol>
<li>Add(e): Add a new entry to the table at the end (extending its size).
<li>Get(i): Return the ith entry in the table.
<li>Clear(): Remove all the entries by setting each entry to zero (for
    security) and setting the size to zero.
</ol>

<p>The obvious implementation is to use a large array A and an integer
s indicating the current size of A.  More precisely A is (always)
of size N (large) and s indicates the extent of A that is currently in
use.

<p>We are ignoring a number of error cases.

<p>We start with a size zero table and assume we perform n (legal)
operations.  Question: What is the worst-case running time for all n
operations.

<p>One possibility is that the sequence consists of n-1 add(e)
operations followed by one Clear().  The Clear() takes &Theta;(n),
which is the worst-case time for any operation (assuming n operations
in total).  Since there are n operations and the worst-case is
&Theta;(n) for one of them, we might think that the worst-case
sequence would take &Theta;(n<sup>2</sup>).

<p>But this is wrong.

<p>It is easy to see that Add(e) and Get(i) are &Theta;(n).

<p>The <strong>total</strong> time for all the Clear() operations is
O(n) since in total O(n) entries were cleared (since at most n entries
were added).

<p>Hence, the amortized time for each operation in the clearable ADT
(abstract data type) is O(1), in fact &Theta;(1).

<h3>1.5.1 Amortization Techniques</h3>

<h4>The Accounting Method</h4>

<p>Overcharge for cheap operations and undercharge expensive so that
the excess charged for the cheap (the profit) covers the undercharge
(the loss).  This is called in accounting an amortization schedule.

<p>Assume the get(i) and add(e) really cost one ``cyber-dollar'',
i.e., there is a constant K so that they each take fewer than K
primitive operations and we let a ``cyber-dollar'' be K.  Similarly,
assume that clear() costs P cyber-dollars when the table has P
elements in it.

<p>We charge 2 cyber-dollars for every operation.  So we have a profit
of 1 on each add(e) and we see that the profit is enough to cover next
clear() since if we clear P entries, we had P add(e)s.

<p>All operations cost 2 cyber-dollars so n operations cost 2n.  Since
we have just seen that the real cost is no more than the cyber-dollars
spent, the total cost is &Theta;(n) and the amortized cost is
&Theta;(1).

<h4>Potential Functions</h4>

<p>Very similar to the accounting method.  Instead of banking money,
you increase the potential energy.  I don't believe we will use this
methods so we are skipping it.

<a id="extendable-array">
    <h3>1.5.2 Analyzing an Extendable Array Implementation</h3>
</a>

<p>We want to let the size of an array grow dynamically (i.e., during
execution).
The implementation is quite simple.  Copy the old array into a new one
twice the size.  Specifically, on an array overflow instead of
signaling an error perform the following steps (assume the array is A
and the size is N)

<ol>
<li>Allocate a new array B of size 2N
<li>For i&larr;0 to N-1 do B[i]&larr;A[i]
<li>Make A refer to B (this is A=B in C and java).
<li>Deallocate the old A (automatic in java; error prone in C)
</ol>

<p>The cost of this growing operation is &Theta;(N).

<p><p><strong>Theorem</strong>:
Given an extendable array A that is initially empty and of size N, the
amortized time to perform n add(e) operations is &Theta;(1).

<p><strong>Proof</strong>:
Assume one cyber dollar is enough for an add w/o the grow and that N
cyber-dollars are enough to grow from N to 2N.
Charge 2 cyber dollars for each add; so a profit of 1 for each add w/o
growing.
When you must do a grow, you had N adds so have N dollars banked.

<h2>1.6 Experimentation</h2>

<h3>1.6.1 Experimental Setup</h3>

<p>Book is quite clear.  I have little to add.

<h4>Choosing the question</h4>

<p>You might want to know
<ul>
<li>Average running time.
<li>Compare two algorithms for speed.
<li>Determine the running time dependence of parameters of the
    algorithm.
<li>For algorithms that generate approximations, test how close they
    come to the correct value.
</ul>

<h4>Deciding what to measure</h4>

<ul>
<li>Memory references (increasingly important--unofficial hardware
    comment).
<li>Comparisons (for sorting, searching, etc).
<li>Arithmetic ops (for numerical problems).
</ul>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #5 ================
</div></big></strong>

<h3>1.6.2 Data Analysis and Visualization</h3>

<h4>Ratio test</h4>

<p>Assume you believe the running time t(n) of an algorithm is
&Theta;(n<sup>d</sup>) for some specific d and you want to both
verify your assumption and find the multiplicative constant.

<p>Make a plot of (n, t(n)/n<sup>d</sup>).  If you are right the
points should tend toward a horizontal line and the height of this
line is the multiplicative constant.

<p><strong>Homework:</strong> R-1.29

<p>What if you believe it is polynomial but don't have a guess for d?
<br>Ans: Use ...

<h4>The power test</h4>

<p>Plot (n, t(n)) on log log paper.   If t(n) is
&Theta;(n<sup>d</sup>), say t(n) approaches bn<sup>d</sup>, then
log(t(n)) approaches log(b)+d(log(n)).

<p>So when you plot (log(n), log(t(n)) (i.e., when you use log log
paper), you will see the points approach (for large n) a straight line
whose slope is the exponent d and whose y intercept is the
multiplicative constant b.

<p><strong>Homework:</strong> R-1.30

<h1>Chapter 2 Basic Data Structures</h1>

<h2>2.1 Stacks and Queues</h2>

<h3>2.1.1 Stacks</h3>

<p><strong>Stacks</strong> implement a LIFO (last in first out)
policy.  All the action occurs at the <strong>top</strong> of the
stack, primarily with the <strong>push(e)</strong> and
<strong>pop</strong> operation.

<p>The stack ADT supports
<ul>
<li>push(e): Insert e at TOS (top of stack).
<li>pop(): Remove and return TOS.  Signal error if stack empty.
<li>top(): Return TOS.  Signal error if empty.
<li>size(): Return current numbers of elements in stack.
<li>isEmpty(): Shortcut for the boolean expression size()=0.
</ul>

<p>There is a simple implementation using an array A and an integer s
(the current size).  A[s-1] contains the TOS.

<p><strong>Objection</strong> (your honor).
The ADT says we can always push.
A simple array implementation would need to signal an error if the
stack is full.

<p><strong>Sustained!</strong>  What do you propose instead?

<p>An extendable array.

<p> Good idea.

<p><strong>Homework:</strong> Assume a software system has 100 stacks
and 100,000 elements that can be on any stack.  You do not know how
the elements are distributed on the stacks.  If you used a normal
array based implementation for the stacks, how much memory will you
need.  What if you use an extendable array based implementation?
Now answer the same question, but assume you have &theta;(S) stacks and
&Theta;(E) elements.

<h4>Applications for procedure calls</h4>

<p>Stacks work great for implementing procedure calls since procedures
have stack based semantics.  That is, last called is first returned and
local variables allocated with a procedure are deallocated when the
procedure returns.

<p>So have a stack of "activation records" in which you keep the
return address and the local variables.

<p>Support for recursive procedures comes for free.  For languages
with static memory allocations (e.g., fortran) one can store the local
variables with the method.  Fortran forbids recursion so that memory
allocation can be static.  Recursion adds considerably flexibility to
a language as some cost in efficiency (<em>not</em> part of this
course).

<p><img src="figs/mod.png" align="right">
<h3>A Review of (the Real) Mod</h3>

<p>I am review modulo since I believe is no longer taught in high
school.

<p>The top diagram shows an almost ordinary analog clock.  The major
difference is that instead of 12 we have 0.  The hands would be useful
if this was a video, but I omitted them for the static picture.
Positive numbers go clockwise (cw) and negative counter-clockwise
(ccw).  The numbers shown are the values mod 12.  This example is good
to show arithmetic.  (2-5) mod 12 is obtained by starting at 2 and
moving 5 hours ccw, which gives 9.  (-7) mod 12 is (0-7) mod 12 is
obtained by starting at 0 and going 7 hours ccw, which gives 5.

<p>To get mod 8, divide the circle into 8 <q>hours</q> instead of 12.

<p>The bottom picture shows mod 5 in a linear fashion.  In pink are
the 5 values one can get when doing mod 5, namely 0, 1, 2, 3, and 4.
I only illustrate numbers from -3 to 11 but that is just due to space
limitations.  Each blue bar is 5 units long so the numbers at its
endpoints are equal mod 5 (since they differ by 5).  So you just lay
off the blue bar until you wind up in the pink.

<h3>End of Mod Review</h3>
<br clear="right">

<h3>2.1.2 Queues</h3>

<p>Queues implement a FIFO (first in first out) policy.  Elements are
inserted at the <strong>rear</strong> and removed from the
<strong>front</strong> using the <strong>enqueue</strong> and
<strong>dequeue</strong> operations respectively.

<p>The <strong>queue</strong> ADT supports
<ul>
<li>enqueue(e): Insert e at the rear.
<li>dequeue(): Remove and return the front element.  Signal an error if
    the queue is empty.
<li>front(): Return the front element.  Signal an error if empty.
<li>size(): Return the number of elements currently present.
<li>isEmpty(): Abbreviation for size()=0
</ul>

<h4>Simple circular-array implementation</h4>

<strong>Personal rant:</strong>
<br>I object to programming languages using
the well known and widely used function name mod and changing its
meaning.  My math training prevents me from accepting that mod(-3,10)
is -3.  The correct value is 7.
The book, following java, defines mod(x,y) as
x-floor(x/y)y.  This is not mod but remainder.
Think of x as dividend, y as divisor and then
floor(x/y) is the quotient.  We remember from elementary school
<br>dividend = quotient * divisor + remainder
<br>remainder = dividend - quotient * divisor
<br>The last line is exactly the book's and java's definition of mod.

<p>My favorite high level language, ada, gets it right in the
obvious way:  Ada defines both mod <strong>and</strong> remainder
(ada extends the math definition of mod to the case where the second
argument is negative).

<p>In the familiar case when x&ge;0 and y&gt;0 mod and remainder are
equal.  Unfortunately the book uses mod sometimes when x&lt;0 and
consequently needs to occasionally add an extra y to get the true mod.
<br><strong>End of personal rant</strong>

<p>Returning to relevant issues we note that for queues we need a
front and rear "pointers" f and r.  Since we are using arrays f and r
are actually indexes not pointers.  Calling the array Q, Q[f] is the
front element of the queue, i.e., the element that would be returned
by dequeue().  Similarly, Q[r] is the element into which enqueue(e)
would place e.  There is one exception: if f=r, the queue is empty so
Q[f] is not the front element.

<p>Without writing the code, we see that f will be increased by each
dequeue and r will be increased by every enqueue.

<p>Assume Q has n slots Q[0]&hellip;Q[N-1] and the queue is initially
empty with f=r=0.  Now consider enqueue(1); dequeue(); enqueue(2);
dequeue(); enqueue(3); dequeue(); &hellip;.  There is never more than
one element in the queue, but f and r keep growing so after N
enqueue(e);dequeue() pairs, we cannot issue another operation.

<p>The solution to this problem is to treat the array as circular,
i.e., right after Q[N-1] we find Q[0].  The way to implement this is
to arrange that when either f or r is N-1, adding 1 gives 0 not N.
Similarly for r.  So the increment statements become
<br>f&larr;(f+1) mod N
<br>r&larr;(r+1) mod N

<p><strong>Note:</strong> Recall that we had some grief due to our
starting arrays and loops at 0.  For example, the fifth slot of A is
A[4] and the fifth iteration of "for i&larr;0 to 30" occurs when i=4.
The updates of f and r directly above show one of the advantages of
starting at 0; they are less pretty if the array starts at 1.

<p>The size() of the queue seems to be r-f, but this is not always
correct since the array is circular.
For example let N=10 and consider an initially empty queue with f=r=0 that has
<br>enqueue(10)enqueue(20);dequeue();enqueue(30);dequeue();enqueue(40);dequeue()
applied.  The queue has one element, f=4, and r=3.
Now apply 6 more enqueue(e) operations
<br>enqueue(50);enqueue(60);enqueue(70);enqueue(80);enqueue(90);enqueue(100)
At this point the array has 7 elements, f=0, and r=3.
Clearly the size() of the queue is not f-r=-3.
It is instead 7, the number of elements in the queue.

<p>The problem is that f in some sense is 10 not 0 since there were 10
enqueue(e) operations.  In fact if we kept 2 values for f and 2 for r,
namely the value before the mod and after, then size() would be
fBeforeMod-rBeforeMod.  Instead we, use the following inelegant formula.
<br>size() = (r-f+N) mod N

<p><strong>Remark</strong>: If java's definition of -3 mod 10 gave 7 (as it
should) instead of -3, we could use the more attractive formula
<br>size() = (r-f) mod N.

<p>Since isEmpty() is simply an abbreviation for the test size()=0, it
is just testing if r=f.

<pre>
Algorithm front():
    if isEmpty() then
        signal an error // throw QueueEmptyException
    return Q[f]
</pre>

<pre>
Algorithm dequeue():
    if isEmpty() then
        signal an error // throw QueueEmptyException
    temp&larr;Q[f]
    Q[f]&larr;NULL      // for security or debugging
    f&larr;(f+1) mod N
    return temp
</pre>

<pre>
Algorithm enqueue(e):
    if size() = N-1 then
        signal an error // throw QueueFullException
    Q[r]&larr;e
    r&larr;(r+1) mod N
</pre>

<h4>Examples in OS</h4>

<p>Round Robin processor scheduling is queue based as is fifo disk
arm scheduling.

<p>More general processor or disk arm scheduling policies often use
priority queues (with various definitions of priority).  We will learn
how to implement priority queues later this chapter (section 2.4).

<p><strong>Homework:</strong> (You may refer to your 202 notes if you
wish; mine are on-line based on my home page).
How can you interpret Round Robin processor scheduling and fifo disk
scheduling as priority queues.  That is what is the priority?
Same question for SJF (shortest job first) and SSTF (shortest seek
time first).

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #6 ================
</div></big></strong>

<p><strong>Problem Set</strong> #1, Problem 1.
<br>The problem set will be officially assigned a little later, but the first
problem in the set is C-2.2


<h2>2.2 Vectors, Lists, and Sequences</h2>

<p>Unlike stacks and queues, the structures in this section support
operations in the middle, not just at one or both ends.

<h3>2.2.1 Vectors</h3>

<P>The <strong>rank</strong> of an element in a sequence is the number
of elements before it.  So if the sequence contains n elements,
0&le;rank&lt;n.

<p>A <strong>vector</strong> storing n elements supports:
<ul>
<li>elemAtRank(r): Return the element with rank r.  Report an error
    r&lt;0 or r&gt;n-1.
<li>replaceAtRank(r,e): Replace the element at rank r with e and
    return it.  Report an error r&lt;0 or r&gt;n-1.
<li>insertAtRank(r,e): Insert e at rank r moving up existing
    elements with rank r and above.  Report an error r&lt;0 or
    r&gt;n.  If r=n, then we are inserting at the end (i.e., after all
    existing elements).
<li>removeAtRank(r): Remove the element at rank r moving down existing
    elements with rank exceeding r.  Report an error r&lt;0 or
    r&gt;n-1.
<li>size().
<li>isEmpty().
</ul>

<h4>A Simple Array-Based Implementation</h4>

<p>Use an array A and store the element with rank r in A[r].
<ul>
<li>Must shift elements when doing an insert or delete, which is
    expensive.
<li>The code is below but does not include the error checks in the ADT
<li>We limit the number of elements to N, the declared size of the
    array.
<li>How can we remove the above limitation?<br>
    Ans: Use an extendable array.
</ul>

<pre>
Algorithm insertAtRank(r,e)
   for i = n-1, n-2, ..., r do
      A[i+1]&larr;A[i]
   A[r]&larr;e
   n&larr;n+1

Algorithm removeAtRank(r)
   e&larr;A[r]
   for i = r, r+1, ..., n-2 do
      A[i]&larr;A[i+1]
   n&larr;n-1
   return e
</pre>

<p>The worst-case time complexity of these two algorithms is
&Theta;(n); the remaining algorithms are all &Theta;(1).

<p><strong>Homework:</strong> When does the worst case occur for
insertAtRank(r,e) and removeAtRank(r)?

<p>By using a circular array we can achieve &Theta;(1) time for
insertAtRank(0,e) and removeAtRank(0).  Indeed, that is the second
problem of the first problem set.

<p><strong>Problem Set</strong> #1, Problem 2:
<br>Part 1: C-2.5 from the book
<br>Part 2: This implementation still has worst case complexity
    &Theta;(n).  When does the worst case occur?

<h3>2.2.2 Lists</h3>


<p>So far we have been considering what Knuth refers to as sequential
allocation, when the next element is stored in the next location.
Now we will be considering linked allocation, where each element
refers explicitly to the next and/or preceding element(s).

<h4>Positions and Nodes</h4>

<p>We think of each element as contained in a <strong>node</strong>,
which is a placeholder that also contains references to the preceding
and/or following node.
<ul>
<li>In C: struct node { int element; struct node *next; struct node *prev;}
<li>In java: class Node { int element; Node next; Node prev;}
</ul>

<p>But in fact we don't want to expose Nodes to user's algorithms
since this would freeze the possible implementation.
Instead we define the idea (i.e., ADT) of a
<strong>position</strong> in a list.
The only method available to users is
<ul>
<li>element(): Return the element stored in this position.
</ul>

<h4>The List ADT</h4>

<p>Given the position ADT, we can now define the methods for the list
ADT.  The first methods only query a list; the last ones actually
modify it.

<ul>
<li>-------------------------- read-only ------------------------
<li>first(): Return the position of the first element; error if L is
    empty.
<li>last(): Return the position of the last element; error if empty.
<li>isFirst(p): Abbreviates p=first().
<li>isLast(p): Abbreviates p=last().
<li>before(p): Return the position preceding p; error if L is empty.
<li>after(p): Return the position following p; error if empty.
<li>size():
<li>isEmpty():
<li>-------------------------- updates ------------------------
<li>replaceElement(p,e): Store e at p, return replaced element.
<li>swapElements(p,q): Swap the elements at p and q.
<li>insertBefore(p,e): Insert e in the position before p.  The book
    says it is an error if p is the first position; that seems wrong,
    especially considering the next method.  I explain what they
    likely meant <a href="#explain">below</a>
<li>insertFirst(e): Abbreviates insertBefore(first(),e).
<li>insertAfter(p,e): Insert e in the position after p (analogous
    error comment).
<li>insertLast(e): Abbreviates insertAfter(last(),e).
<li>remove(p): Remove the element at position p.
</ul>

<h4>A Linked List Implementation</h4>

<p>Now when we are <em>implementing</em> a list we can certainly use
the concept of nodes.
In a singly linked list each node contains a <strong>next</strong>
link that references the next node.
A doubly linked list contains, in addition <strong>prev</strong> link
that references the previous node.

<p>Singly linked lists work well for stacks and queues, but do not
perform well for general lists.  Hence we use doubly linked lists

<p><strong>Homework:</strong> What is the worst case time complexity
of insertBefore for a singly linked list implementation and when does
it occur?

<img src="figs/linked.png" align="right">

<p>It is convenient to add two special nodes, a
<strong>header</strong> and <strong>trailer</strong>.  The header has
just a next component, which links to the first node and the trailer
has just a prev component, which links to the last node.  For an empty
list, the header and trailer link to each other and for a list of size
1, they both link to the only normal node.

<p>In order to proceed from the top (empty) list to the bottom list
(with one element), one would need to execute one of the insert
methods.  Ignoring the abbreviations, this means either
insertBefore(p,e) or inserAfter(p,e).  But this means that header
and/or trailer must be an example of a position, one for which there
is no element.

<p><a id="explain"></a>
This observation explains the authors' comment above
that insertBefore(p,e) cannot be applied if p is the first position.
What they mean is that when we permit header and trailer to be
positions, then we cannot insertBefore the first position, since that
position is the header and the header has no prev.  Similarly we
cannot insertAfter the final position since that position is the
trailer and the trailer has no next.
Clearly not the authors' finest hour.

<strong>Implementation Comment</strong> I have not done the
implementation.  It is probably easiest to have header and trailer
have the same three components as a normal node, but have the prev of
header and the next of trailer be some special value (say NULL) that
can be tested for.

<h4>The insertAfter(p,e) Algorithm</h4>

<p>The position p can be header, but cannot be trailer.

<pre>
Algorithm insertAfter(p,e):
   If p is trailer then signal an error
   Create a new node v
   v.element&larr;e
   v.prev&larr;p
   v.next&larr;p.next
   (p.next).prev&larr;v
   p.next&larr; v
   return v
</pre>

<p>Do on the board the pointer updates for two cases:  Adding a node
after an ordinary node and after header.  Note that they are the
same.  Indeed, that is what makes having the header and trailer so
convenient.

<p><strong>Homework:</strong> Write pseudo code for insertBefore(p,e).


<p>Note that insertAfter(header,e) and insertBefore(trailer,e) appear
to be the only way to insert an element into an empty list.  In
particular, insertFirst(e) fails for an empty list since it performs
insertBefore(first()) and first() generates an error for an empty list.

<h4>The remove(p) Algorithm</h4>

<p>We cannot remove the header or trailer.  Notice that removing the
only element of a one-element list correctly produces an empty list.

<pre>
Algorithm remove(p)
   if p is either header or trailer signal an error
   t&larr;p.element
   (p.prev).next&larr;p.next
   (p.next).prev&larr;p.prev
   p.prev&larr;NULL             // for security or debugging
   p.next&larr;NULL
   return t
</pre>

<h3>2.2.3 Sequences</h3>

<table border=1 cellspacing=5 align=right>
<tr><th>Operation<th>Array<th>List
<tr><td>size, isEmpty<td>O(1)<td>O(1)
<tr><td>atRank, rankOf, elemAtRank<td>O(1)<td>O(n)
<tr><td>first, last, before, after<td>O(1)<td>O(1)
<tr><td>replaceElement, swapElements<td>O(1)<td>O(1)
<tr><td>replaceAtRank<td>O(1)<td>O(n)
<tr><td>insertAtRank, removeAtRank<td>O(n)<td>O(n)
<tr><td>insertFirst, insertLast<td>O(1)<td>O(1)
<tr><td>insertAfter, insertBefore<td>O(n)<td>O(1)
<tr><td>remove<td>O(n)<td>O(1)
<caption align="bottom">
  Asymptotic complexity of the methods for both the array and
  list based implementations
</caption>
</table>

<p>Define a sequence ADT that includes all the methods of both vector
and list ADTs as well as
<ul>
<li>atRank(r): Return the position of the element at rank r.
<li>rankOf(p): Return the rank of the element at position p.
</ul>

<p>Sequences can be implemented as either circular arrays, as we did
for vectors) or doubly linked lists, as we did for lists.  Neither
clearly dominates the other.  Instead it depends on the relative
frequency of the various operations.  Circular arrays are faster for
some and doubly liked lists are faster for others as the following
table illustrates.

<br clear=right>

<h4>Iterators</h4>

<p>An ADT for looping through a sequence one element at a time.  It
has two methods.
<ul>
<li>hasNext: Test whether there are elements left in the iterator
<li>nextObject: Return and remove the next object in the iterator
</ul>

<p>When you create the iterator it has all the elements of the
sequence.  So a typical usage pattern would be

<pre>
create iterator I for sequence S
while I hasNext
   process nextObject
</pre>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #7 ================
</div></big></strong>

<h2>2.3 Trees</h2>

<p>The <strong>tree</strong> ADT stores elements hierarchically.
There is a distinguished <strong>root</strong> node.  All other nodes
have a <strong>parent</strong> of which they are a
<strong>child</strong>.  We use nodes and positions interchangeably
for trees.

<p>The definition above precludes an empty tree.  This is a matter of
taste some authors permit empty trees, others do not.

<p>Some more definitions.
<ul>
<li>Nodes with the same parent are called <strong>siblings</strong>.
<li>A node without children is called <strong>external</strong> by the
    authors.  Also common is to call such nodes
    <strong>leaves</strong>.
<li>A node with children is called <strong>internal</strong>.
<li>An <strong>ancestor</strong> of a node is either the node itself
    or an ancestor of the parent of the node.  This says that the
    ancestors include the node, the node's parent, the parent's
    parent, ... up to the root.
<li>A node v is a <strong>descendent</strong> of a node u if v is an
    ancestor of u.
<li>The <strong>subtree rooted</strong> at v is the tree consisting of
    all the descendents of v.
<li>A tree is <strong>ordered</strong> if there is a linear ordering
    of the children of each node.  That is, there is a first child, a
    second child, etc.
<li>A <strong>binary</strong> tree is one in which all nodes have
    at most two children.
<li>A <strong>proper</strong> binary tree is one
    in which no node has exactly one child.  This means that all
    internal nodes have two children.
<li>In a binary tree we label each child as either a <strong>left
    child</strong> or as a <strong>right child</strong>.
<li>The subtree routed at the left child of v is called the
    <strong>left subtree</strong> of v.
<li>Similarly we define the <strong>right subtree</strong> of v.
<li>The <strong>depth</strong> of a node is the number of ancestors
    not including the node itself.
<li>The <strong>height</strong> of a node is the length of a longest
    path to a leaf.
<li>The <strong>height</strong> of a tree is the height of the root.
</ul>

<p>We order the children of a binary tree so that the left child comes
before the right child.

<p>There are many examples of trees.  You learned tree-structured file
systems in 202.  However, despite what the book says, for Unix/Linux
at least the file system does <em>not</em> form a tree (due to hard
and symbolic links).

<p>These notes can be thought of as a tree with nodes corresponding to
the chapters, sections, subsections, etc.

<p>Games like chess are analyzed in terms of trees.  The root is the
current position.  For each node its children are the positions
resulting from the possible moves.  Chess playing programs often limit
the depth so that the number of examined moves is not too large.

<h4>An arithmetic expression tree</h4>

<p>The leaves are constants or variables and the internal nodes are binary
arithmetic operations (+,-,*,/).
The tree is a proper ordered binary tree (since we are considering
binary operators).
The value of a leaf is the value of the constant or variable.
The value of an internal node is obtained by applying the operator to
the values of the children (in order).

<p>Evaluate an arithmetic expression tree on the board.

<p><strong>Homework:</strong> R-2.2, but made easier by replacing 21
by 10.  If you wish you can do the problem in the book instead (I
think it is harder).

<h3>2.3.1 The Tree Abstract Data Type</h3>

<p>We have three <strong>accessor methods</strong> (i.e., methods that
permit us to access the nodes of the tree.
<ul>
<li>root(): Return the root of the tree.
<li>parent(v): Return the parent of the node v.  Error if v is
    the root.
<li>children(v): Return an iterator of the children of v (in order if
    the tree is ordered).
</ul>

<p>We have four <strong>query methods</strong> that test status.
<ul>
<li>isInternal(v): Tests if v is internal.
<li>isLeaf(v): Tests if v is a leaf.
<li>isExternal(v): Same as isLeaf(v).
<li>isRoot(v): Tests if v is the root.
</ul>

<p>Finally <strong>generic methods</strong> that are useful but not
related to the tree structure.
<ul>
<li>size(): Return the number of nodes
<li>elements(): Return an iterator of all the elements stored
    in nodes (in no particular order).
<li>positions: Return an iterator of all the nodes of the tree (nodes
    of a tree correspond to position in a linked list).
<li>swapElements(v,w): Swap the elements in nodes v and w.
<li>replaceElement(v,e): Replace with e and return the element stored
    at v.
</ul>

<h3>2.3.2 Tree Traversal</h3>

<p><strong>Traversing</strong> a tree is a systematic method for
accessing or "visiting" each node.  We will see and analyze three tree
traversal algorithms, inorder, preorder, and postorder.  They differ in
when we visit an internal node relative to its children.  In
preorder we visit the node first, in postorder we visit it last, and
in inorder, which is only defined for binary trees, we visit the node
between visiting the left and right children.

<p><strong>Recursion will be a very big deal in traversing
trees!!</strong>

<p><img src="figs/trees.png" align="right">

<h4>Motivating the Recursive Routines</h4>

<p>On the right are three trees.  The left one just has a root, the
right has a root with one leaf as a child, and the middle one has
six nodes.  For each node, the element in that node is shown inside
the box.  All three roots are labeled and 2 other nodes are
also labeled.  That is, we give a name to the position, e.g. the left
most root is position v.  We write the name of the position under the
box.
We call the left tree T0 to remind us it has height zero.  Similarly
the other two are labeled T2 and T1 respectively.

<p>Our goal in this motivation is to calculate the sum the elements in
all the nodes of each tree.  The answers are, from left to right, 8,
28, and 9.

<p>For a start, lets write an algorithm called treeSum0
that calculates the sum for
trees of height zero.  In fact the algorithm, will contain two
parameters, the tree and a node (position) in that tree, and our
algorithm will calculate the sum in the subtree rooted at the given
position assuming the position is at height 0.  Note this is trivial:
since the node has height zero, it has no children and the sum desired
is simply the element in this node.  So legal invocations would
include treeSum0(T0,s) and treeSum0(T2,t).  Illegal invocations would
include treeSum0(T0,t) and treeSum0(T1,r).

<pre>
Algorithm treeSum0(T,v)
  Inputs: T a tree; v a height 0 node of T
  Output: The sum of the elements of the subtree routed at v

  Sum&larr;v.element()
  return Sum
</pre>

<p>Now lets write treeSum1(T,v), which calculates the sum for a node
at height 1.  It will use treeSum0 to calculate the sum for each
child.

<pre>
Algorithm treeSum1(T,v)
   Inputs: T a tree; v a height 1 node of T
   Output: the sum of the elements of the subtree routed at v

   Sum&larr;v.element()
   for each child c of v
      Sum&larr;Sum+treeSum0(T,c)
   return Sum
</pre>

<p>OK.  How about height 2?

<pre>
Algorithm treeSum2(T,v)
   Inputs: T a tree; v a height 2 node of T
   Output: the sum of the elements of the subtree routed at v

   Sum&larr;v.element()
   for each child c of v
      Sum&larr;Sum+treeSum1(T,c)
   return Sum
</pre>

<p>So all we have to do is to write treeSum3, treSum4, ... , where
treSum3 invokes treeSum2, treeSum4 invokes treeSum3, ... .

<p>That would be, literally, an infinite amount of work.

<p>Do a diff of treeSum1 and treeSum2.
<br>What do you find are the differences.
<br>In the Algorithm line and in the first comment a 1 becomes a 2.

<p>Why can't we write treeSumI and let I vary?
<br>Because it is illegal to have a varying name for an algorithm.

<p>The solution is to make the I a parameter and write

<pre>
Algorithm treeSum(i,T,v)
   Inputs: i&ge;0; T a tree; v a height i node of T
   Output: the sum of the elements of the subtree routed at v

   Sum&larr;v.element()
   for each child c of v
      Sum&larr;Sum+treeSum(i-1,T,c)
   return Sum
</pre>


<p>This is <strong>wrong</strong>, why?
<br>Because treeSum(0,T,v) invokes treeSum(-1,c,v), which doesn't
exist because i&lt;0

<p>But treeSum(0,T,v) doesn't have to call anything since v can't have
any children (the height of v is 0).  So we get

<pre>
Algorithm treeSum(i,T,v)
   Inputs: i&ge;0; T a tree; v a height i node of T
   Output: the sum of the elements of the subtree routed at v

   Sum&larr;v.element()
   if i&gt;0 then
      for each child c of v
         Sum&larr;Sum+treeSum(i-1,T,c)
   return Sum
</pre>

<p>The last two algorithms are recursive; they call themselves.
Note that when treeSum(3,T,v) calls treeSum(2,T,c), the new treeSum
has new variables Sum and c.

<p>We are pretty happy with our treeSum routine, but ...

<p>The algorithm is <strong>wrong!</strong>  Why?
<br>The children of a height i node need not all be of height i-1.
For example s is hight 2, but its left child w is height 0.

<p>But the only real use we are making of i is to prevent us from
recursing when we are at a leaf (the i&gt;0 test).
But we can use isInternal instead, giving our final algorithm

<pre>
Algorithm treeSum(T,v)
   Inputs: T a tree; v a node of T
   Output: the sum of the elements of the subtree routed at v

   Sum&larr;v.element()
   if T.isInternal(v) then
      for each child c of v
         Sum&larr;Sum+treeSum(T,c)
   return Sum
</pre>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #8 ================
</div></big></strong>

<p><strong>Remark</strong>:
<p>The department has asked me to make the following announcements.
<ol>
<li>The prerequisites for this course are V22.0102 and V63.0120.
<li>The final exam is Monday, December 16 10:00-11:50am.
</ol>

<h4>Complexity of Primitive Operations</h4>

<p>Our complexity analysis will proceed in a somewhat unusual order.
Instead of starting with the bottom (the tree methods in
2.3.1, e.g., is Internal(v)) or the top (the traversals),
we will begin by analyzing some middle level procedures assuming the
complexities of the low level are as we assert them to be.
Then we will analyze the traversals using the middle level routines
and finally we will give data structures for trees that achieve our
assumed complexity for the low level.

<p>Let's begin!

<h4>Complexity Assumptions for the Tree ADT</h4>

<p>These will be verified later.

<ul>
<li>root(), parent(v), isInternal(v), isLeaf(v), isRoot(v),
    swapElements(v,w), replaceElement(v,e) each take O(1) time.
<li>The methods returning iterators, namely children(v), elements(),
    and positions(), each take time O(k), where k is the number of
    items being iterated over.  k=#children for the first method and
    #nodes for the other two.
<li>For each iterator, the methods hasNext() and nextObject() take
    O(1) time.  nextObject() sometimes has other names like
    nextPosition() or nextNode() or nextChild().
</ul>

<h4>Middle level routines depth and height</h4>

<p><strong>Definitions</strong> of <strong>depth</strong> and
<strong>height</strong>.
<ul>
<li>The depth of the root is 0.
<li>The depth of a non-root v is 1 plus the depth of parent(v).
<li>The height of a leaf is 0.
<li>The height of an internal node v is 1 plus the maximum
    height of the children of v.
<li>The height of a tree is the height of its root.
</ul>

<p><strong>Remark</strong>: Even our definitions are recursive!

<p>From the recursive definition of depth, the recursive algorithm for
its computation essentially writes itself.

<pre>
Algorithm depth(T,v)
   if T.isRoot(v) then
      return 0
   else
      return 1 + depth(T,T.parent(v))
</pre>

<p>The complexity is &Theta;(the answer), i.e. &Theta;(d<sub>v</sub>),
where d<sub>v</sub> is the depth of v in the tree T.

<p><strong>Problem Set</strong> #1, Problem 3:
<br>Rewrite depth(T,v) without using recursion.
<br>This is quite easy.  I include it in the problem set to ensure
that you get practice understanding recursive definitions.

<p>The following algorithm computes the height of a position in a tree.

<pre>
Algorithm height(T,v):
   if T.isLeaf(v) then
      return 0
   else
      h&larr;0
      for each w in T.children(v) do
         h&larr;max(h,height(T,w))
      return h+1
</pre>

<p><strong>Remarks on the above algorithm</strong>
<ol>
<li>The loop could (perhaps should) be written as an iterator.
    Indeed, it <strong>is</strong> an iterator.
<li>This algorithm is not so easy to convert to non-recursive form
    <br>Why?
    <br>It is not tail-recursive, i.e. the recursive invocation is not
        just at the end.
<li>To get the height of the tree, execute height(T,T.root())
</ol>

<p><strong>Theorem</strong>: Let T be a tree with n nodes and let
c<sub>v</sub> be the number of children of node v.  The sum of
c<sub>v</sub> over all nodes of the tree is n-1.

<p><strong>Proof</strong>:
This is trivial! ... once you figure out what it is saying.
The sum gives the total number of children in a tree.  But this almost
all nodes.  Indeed, there is just one exception.
<br>What is the exception?
<br>The root.

<p><strong>Corollary</strong>: Computing the height of an n-node tree
has time complexity &Theta;(n).

<p><strong>Proof</strong>:
Look at the code.
<br>The while loop has c<sub>v</sub> iterations, so by the theorem the
total number of iterations executed is n-1.
<br>Everything else is &Theta;(1) per iteration.

<p>Do a few on the board.  As mentioned above, becoming facile with
recursion is <strong>vital</strong> for tree analyses.

<p><strong>Definition</strong>: A <strong>traversal</strong> is a
systematic way of "visiting" every node in a tree.

<h4>Preorder Traversal</h4>

<p>Visit the root and then recursively traverse each child.  More
formally we first give the procedure for a preorder traversal starting
at any node and then define a preorder traversal of the entire tree as
a preorder traversal of the root.

<pre>
Algorithm preorder(T,v):
   visit node v
   for each child c of v
      preorder(T,c)

Algorithm preorder(T):
   preorder(T,T.root())
</pre>

<p><strong>Remarks</strong>: 
<ol>
<li>In a preorder traversal, parents come
    before children (which is as it should be :-)).
<li>If you describe a book as an ordered tree, with nodes for each
    chapter, section, etc., then the pre-order traversal visits the
    nodes in the order they would appear in a table of contents.
</ol>

<p>Do a few on the board.  As mentioned above, becoming facile with
recursion is <strong>vital</strong> for tree analyses.

<p><strong>Theorem</strong>: Preorder traversal of a tree with n nodes
has complexity &Theta;(n).

<p><strong>Proof</strong>:
Just like height.
<br>The nonrecursive part of each invocation takes O(1+c<sub>v</sub>)
<br>There are n invocations and the sum of the c's is n-1.

<p><strong>Homework:</strong> R-2.3

<h4>Postorder Traversal</h4>

<p>First recursively traverse each child then visit the root.
More formerly

<pre>
Algorithm postorder(T,v):
   for each child c of v
      postorder(T,c)
   visit node v

Algorithm postorder(T):
   postorder(T,T.root())
</pre>

<p><strong>Theorem</strong>: Preorder traversal of a tree with n nodes
has complexity &Theta;(n).

<p><strong>Proof</strong>:
The same as for preorder.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #9 ================
</div></big></strong>

<p><strong>Note:</strong> The following homework should have been
assigned last time but wasn't so it is part of
homework 9.

<p><strong>Homework:</strong> R-2.3

<p><strong>Remarks</strong>: 
<ol>
<li>This is how you evaluate an arithmetic expression tree.
<li>Evaluate some arithmetic expression trees.
<li>When you write out the nodes in the order visited your get what is
    called either "reverse polish notation" or "polish notation".
<li>If you do preorder traversal you get the other one.
</ol>

<p><strong>Problem Set</strong> 2, Problem 1.
Note that the height of a tree is the depth of a deepest node.
Extend the height algorithm so that it returns in addition to the
height the v.element() for some v that is of maximal depth.

<h3>2.3.3 Binary Trees</h3>

<p>Recall that a binary tree is an ordered tree in which no node has
more than two children.  The left child is ordered before the right
child.

<p>The book adopts the convention that, unless otherwise mentioned,
the term "binary tree" will mean "proper binary tree", i.e., all
internal nodes have two children.
This is a little convenient, but not a big deal.
If you instead permitted non-proper binary trees, you would test if a
left child existed before traversing it (similarly for right child.)

<p>Will do binary preorder (first visit the node, then the left
subtree, then the right subtree, binary postorder (left subtree, right
subtree, node) and then inorder (left subtree, node, right subtree).

<h4>The Binary Tree ADT</h4>

<p>We have three (accessor) methods in addition to the general tree
methods.
<ol>
<li>leftChild(v): Return the (position of the) left child; signal an
    error if v is a leaf.
<li>rightChild(v): Similar
<li>sibling(v): Return the (unique) sibling of v; signal an error if v
    is the root (and hence has no sibling).
</ol>

<p><strong>Remark</strong>: I will <strong>not</strong> hold you
responsible for the
<strong>proofs</strong> of the theorems.

<p><strong>Theorem</strong>: Let T be a binary tree having height h
and n nodes.  Then
<ol>
<li>The number of leaves in T is at least h+1 and at most 2<sup>h</sup>.
<li>The number of internal nodes in T is at least h and at most
    2<sup>h</sup>-1.
<li>The number of nodes in T is at least 2h+1 and at most
    2<sup>h+1</sup>-1.
<li>log(n+1)-1&le;h&le;(n-1)/2
</ol>

<p><strong>Proof</strong>:
<ol>
<li>Induction on n the number of nodes in the tree.
    <p>Base case n=1: Clearly true for all trees having only one
    node.
    <p>Induction hypothesis: Assume true for all trees having at most
    k nodes.
    <p>Main inductive step: prove the assertion for all trees having k+1
    nodes.  Let T be a tree with k nodes and let h be the height of T.
    <p>Remove the root of T.  The two subtrees produced each have no
    more than k nodes so satisfy the assertion.  Since each has height
    at most h-1, each has at most 2<sup>h-1</sup> leaves.  At least
    one of the subtrees has height exactly h-1 and hence has at least
    h leaves.  Put the original tree back together.<br> One subtree
    has at least h leaves, the other has at least 1, so the original
    tree has at least h+1.  Each subtree has at most 2<sup>h-1</sup>
    leaves and the original root is not a leaf, so the original has at
    most 2<sup>h</sup> leaves.<br>
<li>Same idea.  Induction on the number of nodes.  Clearly true if T
    has one node.  Remove the root.  At least one of the subtrees is
    height h-1 and hence has at least h-1 internal nodes.  Each of the
    subtrees has height at most h-1 so has at most 2<sup>h-1</sup>-1
    internal nodes.  Put the original tree back together.  One subtree
    has at least h-1 internal nodes, the other has at least 1, so the
    original tree has at least h.  Each subtree has at most
    2<sup>h-1</sup>-1 internal nodes and the original root is an
    internal node, so the original has at most 2(2<sup>h</sup>-1)+1
    = 2<sup>h-1</sup>-1 internal nodes.<br>
<li>Add parts 1 and 2.<br>
<li>Take the log of part 3.
</ol>

<img align="right" src="figs/NumLeaves.png">

<p><strong>Theorem</strong>:In a binary tree T, the number of leaves
is 1 more than the number of internal nodes.

<p><strong>Proof</strong>:
Again induction on the number of nodes.  Clearly true for one node.
Assume true for trees with up to n nodes and let T be a tree with n+1
nodes.  For example T is the top tree on the right.
<ol>
<li>Choose a leaf and its parent (which of course is internal).
    For example, the leaf t and parent s in red.
<li>Remove the leaf and its parent (middle diagram)
<li>Splice the tree back without the two nodes (bottom diagram).
<li>Since S has n-1 nodes, S satisfies the assertion.
<li>Note that T is just S + one leaf + one internal so also satisfies
    the assertion.
</ol>

<p><strong>Alternate Proof</strong> (does not use the pictures):
<ol>
<li>Place two tokens on each internal node.
<li>Push these tokens to the two children.
<li>Notice that now all nodes but the root have one token; the root
    has none.
<li>Hence 2*(number internal) = (number internal) + (number leaves) + 1
<li>Done (slick!)
</ol>

<p><strong>Corollary</strong>:
A binary tree has an odd number of nodes.
<p><strong>Proof</strong>:
#nodes = #leaves + #internal = 2(#internal)+1.

<h4>Preorder traversal of a binary tree</h4>

<pre>
Algorithm binaryPreorder(T,v)
   Visit node v
   if T.isInternal(v) then
      binaryPreorder(T,T.leftChild(v))
      binaryPreorder(T,T.rightChild(v))
</pre>

<pre>
Algorithm binaryPretorder(T)
   binaryPreorder(T,T.root())
</pre>

<h4>Postorder traversal of a binary tree</h4>

<pre>
Algorithm binaryPostorder(T,v)
   if T.isInternal(v) then
      binaryPostorder(T,T.leftChild(v))
      binaryPostorder(T,T.rightChild(v))
   Visit node v
</pre>

<pre>
Algorithm binaryPosttorder(T)
   binaryPostorder(T,T.root())
</pre>

<h4>Inorder traversal of a binary tree</h4>

<pre>
Algorithm binaryInorder(T,v)
   if T.isInternal(v) then
      binaryInorder(T,T.leftChild(v))
   Visit node v
   if T.isInternal(v) then
      binaryInorder(T,T.rightChild(v))
</pre>

<pre>
Algorithm binaryIntorder(T)
   binaryPostorder(T,T.root())
</pre>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #10 ================
</div></big></strong>

<p><strong>Remark</strong>:
Should have mentioned last time the corollary that the number of nodes
in a binary tree is odd.

<p><strong>Definition</strong>: A binary tree is
<strong>fully complete</strong> if all the leaves are at the same (maximum)
depth.
This is the same as saying that the sibling of a leaf is a leaf.

<h4>Euler tour traversal</h4>

<img align="right" src="figs/euler-tour.png">

<p>Generalizes the above.  Visit the node three times, first when
``going left'', then ``going right'', then ``going up''.
Perhaps the words should be ``going to go
left'', ``going to go right'' and ``going to go up''.
These words work for internal nodes.  For a leaf you just visit it
three times in a row (or you could put in code to only visit a leaf
once; I don't do this).
It is
called an Euler Tour traversal because an Euler tour of a graph is a
way of drawing each edge exactly once without taking your pen off the
paper.  The Euler tour traversal would draw each edge twice but if you
add in the parent pointers, each edge is drawn once.

<p>The book uses ``on the left'', ``from below'', ``on the right''.  I
prefer my names, but you may use either.

<pre>
Algorithm eulerTour(T,v):
   visit v going left
   if T.isInternal(v) then
      eulerTour(T,T.leftChild(v))
   visit v going right
   if T.isInternal(v) then
      eulerTour(T,T.rightChild(v))
   visit v going up

Algorithm eulerTour(T):
   eulerTour(T,T.root))
</pre>

<p>Pre- post- and in-order traversals are special cases where two of
the three visits are dropped.

<p>It is quite useful to have this three visits.
For example here is a nifty algorithm to print and expression tree
with parentheses to indicate the order of the operations.
We just give the three visits.

<pre>
Algorithm visitGoingLeft(v):
   if T.isInternal(v) then
      print "("

Algorithm visitGoingRight(v)
   print v.element()

Algorithm visitGoingUp(v)
   if T.isInternal(v) then
      print ")"
</pre>

<p><strong>Homework:</strong> Plug these in to the Euler Tour and show
that what you get is the same as

<pre>
Algorithm printExpression(T,v):
   input: T an expression tree v a node in T.
   if T.isLeaf(v) then
      print v.element()  // for a leaf the element is a value
   else
      print "("
      printExpression(T,T.leftChild(v))
      print v.element()  // for an internal node the element is an operator
      printExpression(T,T.rightChild(v))
      print ")"
</pre>

<pre>
Algorithm printExpression(T):
   printExpression(T,T.root())
</pre>

<p><strong>Problem Set</strong> 2 problem 2.
We have seen that traversals have complexity &Theta;(N), where N is
the number of nodes in the tree.  But we didn't count the costs of the
visit()s themselves since the user writes that code.
We know that visit() will be called N times, once per node, for post-,
pre-, and in-order traversals and will be called 3N times for Euler
tour traversal.  So if each visit costs &Theta;(1), the total visit cost
will be &Theta;(N) and thus does not increase the complexity of a
traversal.
If each visit costs &Theta;(N), the total visit cost will be
&Theta;(N<sup>2</sup>) and hence the total traversal cost will be
&Theta;(N<sup>2</sup>).
The same analysis works for any visit cost providing all the visits
cost the same.  For this problem we will be considering a variable
cost visits.  In particular, assume that the cost of visiting a node v
is the height of v (so roots can be expensive to visit, but leaves are
free).

<p>Part A.  How many nodes N are in a fully complete binary tree of height
h?

<p>Part B.  How many nodes are at height i in a fully complete binary tree
of height h?  What is the total cost of visiting all the nodes at
height i?

<p>Part C.  Write a formula using &Sigma; (sum) for the total cost of
visiting all the nodes.  This is very easy given B.

<p>One point extra credit.  Show that the sum you wrote in part C is
&Theta;(N).

<p>Part D.  Continue to assume the cost of visiting a node equals its height.
Describe a class of binary trees for which the total cost of visiting
the nodes is &theta;(N<sup>2</sup>).  Naturally these will
<em>not</em> be <em>fully complete</em> binary trees.  Hint do problem 3.


<h3>2.3.4 Data Structures for representing trees</h3>

<h4>A vector-based implementation for Binary Trees</h4>

<p>We store each node as the element of a vector.  Store the root in
element 1 of the vector and the key idea is
that we store the two children of the element at rank r in the
elements at rank 2r and 2r+1.

<p>Draw a fully complete binary tree of height 3 and show where each element
is stored.

<p>Draw an incomplete binary tree of height 3 and show where each
element is stored and that there are gaps.

<p>There must be a way to tell leaves from internal nodes.  The book
doesn't make this explicit.  Here is an explicit example.
Let the vector S be given.  With a vector we have the current size.
S[0] is not used.  S[1] has a pointer to the root node (or contains
the root node if you prefer.  For each S[i], S[i] is null (a special
value) if the corresponding node doesn't exist).  Then to see if the
node v at rank i is a leaf, look at 2i.  If 2i exceeds S.size() then v
is a leaf since it has no children.  Similarly if S[2i] is null, v is
a leaf.  Otherwise v is external.
<br>How do you know that if S[2i] is null, then s[2i+1] will be null?
<br>Ans: Our binary trees are proper.

<p>This implementation is very fast.  Indeed all tree operations are
O(1) except for positions() and elements(), which produce n results
and take time &Theta;(n).

<p><strong>Homework:</strong> R-2.7

<p>However, this implementation can waste a lot of space since many of
the entries in S might be unused.  That is there may be many i for
which S[i] is null.

<p><strong>Problem Set</strong> 2 problem 3.
Give a tree with fewer than 20 nodes for which S.size() exceeds 100.
Give a tree with fewer than 25 nodes for which S.size() exceeds 1000.
Give a tree with fewer than 100 nodes for which S.size() exceeds a
million.


<h4>A linked structure for binary trees</h4>

<p>Represent each node by a quadruple.
<ol>
<li>A reference to the parent (null if we are at the root).
<li>A reference to the left child (null if at a leaf).
<li>A reference to the right child (null if at a leaf).
<li>The element().
</ol>

<p>Once again the algorithms are all O(1) except for positions() and
elements(), which are &Theta;(n).

<p>The space is &Theta;(n) which is much better that for the vector
implementation.  The constant is larger however since three pointers
are stored for each position rather than one index.

<h4>A linked structure for general trees</h4>

<p>The only difference is that we don't know how many children each
node has.  We could store k child pointers and say that we cannot
process a tree having more than k children with the same parent.

<p>Clearly we don't like this limit.
Moreover, if we choose k moderate, say k=10.  We are limited to 10-ary
trees and for 3-ary trees most of the space is wasted.

<p>So instead of storing the child references in the node, we store
just one reference to a container.  The container has references to
the children.  Imaging implementing the container as an extendable array.

<p>Since a node v contains an arbitrary number of children, say
C<sub>v</sub>, the complexity of the children(v) iterator is
&Theta;(C<sub>v</sub>).

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #11 ================
</div></big></strong>

<h2>2.4 Priority Queues and Heaps</h2>

<h3>2.4.1 The Priority Queue Abstract Data Type</h3>

<p>Up to now we have not considered elements that must be retrieved in a
fixed order.  But often in practice we assign a priority to each item
and want the most important (highest priority) item first.
(For some reason that I don't know, low numbers are often used to
represent high priority.)

<p>For example consider processor scheduling from Operating Systems (202).
The simplest scheduling policy is FCFS for which a queue of ready
processors is appropriate.
But if we want SJF (short job first) then we want to extract the ready
process that has the smallest remaining time.
Hence a FIFO queue is not appropriate.

<p>For a non-computer example,consider managing your todo list.  When
you get another item to add, you decide on its importance (priority)
and then insert the item into the todo list.
When it comes time to perform an item, you want to remove the highest
priority item.  Again the behavior is not FIFO.

<h4>Ordering properties and comparators</h4>

<p>To return items in order, we must know when one item is less than
another.  For real numbers this is of course obvious.

<p>We assume that each item has a <em>key</em> on which the priority
is to be based.  For the SJF example given above, the key is the time
remaining.  For the todo example, the key is the importance.

<p>We assume the existence of an order relation (often called a total
order) written &le;
satisfying for all keys s, t, and u.

<ul>
<li>"Fully defined" Either s&le;t or t&le;s.
<li><strong>Reflexive:</strong> s&le;s
<li><strong>Antisymmetric:</strong> s&le;t and t&le;s implies t=s.
<li><strong>Transitive:</strong> s&le;t and t&le;u implies s&le;u.
</ul>

<p><strong>Remark</strong>: For the complex numbers no such ordering exists
that extends the natural ordering on the reals and imaginaries.
This is unofficial (not part of 310).

<p>Is it OK to define s&le;t for all s and t?
<br>No.  That would not be antisymmetric.

<p><strong>Definition</strong>: A <strong>priority queue</strong> is a
container of elements each of which has an associated key supporting
the following methods.

<ul>
<li>InsertItem(k,e): Insert an element e with key k.
<li>removeMin(): Return and remove an element with a minimal key,
    i.e., a key &le; the key of any other element.
<li>minElement(): Return an element with minimal key.
<li>minKey(): Return a minimal key.
</ul>

<h4>Comparators</h4>

<p>Users may choose different comparison functions for the same data.
For example, if the keys are longitude,latitude pairs, one user may be
interested in comparing longitudes and another latitudes.  So we
consider a general comparator containing methods.

<ul>
<li>isLess(a,b)
<li>isLessOrEqual(a,b)
<li>isEqual(a,b)
<li>isGreater(a,b)
<li>isGreaterOrEqual(a,b)
<li>isComparable(a,b)
</ul>

<h3>2.4.2 PQ-Sort, Selection-Sort, and Insertion-Sort</h3>

<p>Given a priority queue it is trivial to sort a collection of
elements.  Just insert them and then do removeMin to get them in
order.
Written formally this is

<pre>
Algorithm PQ-Sort(C,P)
   Input: an n element sequence C and an empty priority queue P
   Output: C with the elements sorted

   while not C.isEmpty() do
      e&larr;C.removeFirst()
      P.insertItem(e,e)  // We are sorting on the element itself.

   while not P.isEmpty()
      C.insertLast(P.removeMin())
</pre>

<p>So whenever we give an implementation of a priority queue, we are
also giving a sorting algorithm.
Two obvious implementations of a priority queue give well known (but
slow) sorts.  A non-obvious implementation gives a fast sort.  We
begin with the obvious.

<h4>Implementing a priority queue with an unordered sequence</h4>

<p>So insertItem() takes &Theta;(1) time and hence takes &Theta;(N) to
insert all n items of C.  But remove min, requires we go through the
entire list.  This requires time &Theta;(k) when there are k items in
the list.  Hence to remove all the items requires
&Theta;(n+(n-1)+...+1) = &Theta;(N<sup>2</sup>) time.

<p>This sorting algorithm is normally called <strong>selection
sort</strong> since the dominant step is selecting the minimum each time.

<h4>Implementing a priority queue with an ordered sequence</h4>

<p>Now removeMin() is trivial since it is just removeFirst().
But insertItem is &Theta;(k) when there are k items already in the
priority queue since you must step through to find the correct
location to insert and then slide the remaining elements over.

<p>This sorting element is normally called <strong>insertion
sort</strong> since the dominant effort is inserting each element.

<h3>2.4.3 The Heap Data Structure</h3>

<img src="figs/heap.png" align="right">

<p>We now consider the non-obvious implementation of a priority queue
that gives a fast sort (and a fast priority queue).
Since the priority queue algorithm will perform steps with complexity
&Theta;(height of tree), we want to keep the height small.
The way to do this is to fully use each level.

<p><strong>Definition</strong>: A binary tree of height h is
<strong>complete</strong> if the levels 0,...,h-1 contain the maximum
number of elements and on level h-1 all the internal nodes are to the
left of all the leaves.

<p><strong>Remarks</strong>:
<ol>
<li>Level i has 2<sup>i</sup> nodes, for 0&le;i&le;h-1.
<li>Level h contains only leaves.
<li>Levels less than h-1 contain no leaves.
<li>So really, for all levels, internal nodes are to the left of leaves.
</ol>

<p><strong>Definition</strong>: A tree storing a key at each node
satisfies the <strong>heap-order property</strong> if, for every node v
other than the root, the key at v is no smaller than the key at v's
parent.

<p><strong>Definition</strong>: A <strong>heap</strong> is a complete
binary tree satisfying the heap order property.

<p><strong>Definition</strong>: The <strong>last node</strong> of a
heap is the right most internal node in level h-1.

<p><strong>Remark</strong>: As written the ``last node'' is really the
last internal node.  However, we actually don't use the leaves to
store keys so in some sense ``last node'' is the last (significant) node.

<!-- Not given 2002-03 fall
<strong><p>Homework:</strong> R-2.11, R-2.14
-->

<h4>Implementing a Priority Queue with a Heap</h4>

<p>With a heap it is clear where the minimum is located, namely at the
root.  We will also use <strong>last</strong> a reference to the last
node since insertions will occur at the first node after last.

<p><strong>Theorem</strong>:
A heap with storing n keys has height &lceil;log(n+1)&rceil;

<p><strong>Proof</strong>:
<ol>
<li>Levels 0 through h-2 are full and contain only internal nodes (which
    contain keys)
<li>This gives 1+2+4+...+2<sup>h-2</sup> keys.
<li><strong>Homework:</strong> Show this is 2<sup>h-1</sup>-1 keys.
<li>Level h-1 contains between 1 and 2<sup>h-1</sup> internal nodes
    each containing a key.
<li>Level h contains no internal nodes.
<li>So (2<sup>h-1</sup>-1)+1 &le; n &le; (2<sup>h-1</sup>-1)+2<sup>h-1</sup>
<li>So 2<sup>h-1</sup> &le; n &le; 2<sup>h</sup>-1
<li>So 2<sup>h-1</sup> &lt; n+1 &le; 2<sup>h</sup>
<li>So h-1 &lt; log(n+1) &le; h
<li>So &lceil;log(n+1)&rceil = h
</ol>

<p><strong>Corollary</strong>: If we can implement insert and
removeMin in time &Theta;(height), we will have implemented the
priority queue operations in logarithmic time (our goal).

<h4>The Vector Representation of a Heap</h4>

<p>Since we know that a heap is complete is efficient to use the
vector representation of a binary tree.  We can actually not bother
with the leaves since we don't ever use them.  We call the last node w
(remember that is the last internal node).  Its index in the vector
representation is n, the number of keys in the heap.  We call the
first leaf z; its index is n+1.  Node z is where we will insert a new
element and is called the <strong>insertion position</strong>.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #12 ================
</div></big></strong>

<p><strong>Remarks </strong>(sent to mailing list on thurs):
<ol>
<li>The lateness policy for problem sets has changed.  The absolute
    deadline is 1 week after due date.  For problem set 1, the
    absolute deadline is today.
<li>Elif Tosun <elif@cat.nyu.edu>, who graded problem set 1, has
    graciously agreed to meet with students who have questions on the
    grading.  Her office is room 1210 in 719 broadway.  She will be
    there this monday 14 oct from 3-5pm.  If you have classes then,
    please send her email to arrange an alternate time.
<li>Please do <strong>not</strong> put problem sets or homeworks
    in my WWH mailbox.  Strange things seem to happen.
<li>Write the homework solutions password on the board.
</ol>

<img src="figs/up-heap.png" align="right">
<h4>Insertion</h4>

<p>This looks trivial.  Since we know n, we can find n+1 and hence the
reference to node z in O(1) time.
But there is a problem; the result might not be a heap since the new
key inserted at z might be less than the key stored at u the parent of
z.
Reminiscent of bubble sort, we need to bubble the value in z up to the
correct location.

<h4>Up-Heap Bubbling</h4>

<p>We compare key(z) with key(u) and swap the items if necessary.  In
the diagram on the right we added 45 and then had to swap it with 70.
But now 45 is still less than its parent so we need to swap again.
At worst we need to go all the way up to the
root.  But that is only &Theta;(n) as desired.  Let's slow down and
see that this really works.
<ul>
<li>We had a heap before we inserted the new element.
<li>When we insert the new element it can ruin the heap because it is
    too small (i.e. smaller than its parent),
<li>So the blue node is the problem.
<li>Since blue is smaller than its parent, we swap them.  Call the
    parent the victim.
<li>Two nodes have been swapped we have four things to check:
    For each of these two nodes we must see that it is not larger
    than its new children and not smaller than its new parent.
    <ol>
    <li>The blue node is definitely not larger than its new children:
        One child is the victim, which we know is larger than the blue.
        The other child was not smaller than the victim so is surely
        not smaller than the blue.
    <li>The blue node might be smaller than its new parent.  Indeed in
        the diagram on the right it is.  That is why we have to keep
        bubbling up.
    <li>Before we did the insert, we had a heap so at that point the
        victim was not larger than all its descendents.  But after the
        swap, all of the children of the victim were descendents of
        the victim before.
    <li>The victim is definitely not smaller than its new parent,
        which is the blue.
    </ol>
</ul>

<p>Great.  It works (i.e., is a heap) and there can only be O(log(n))
swaps because that is the height of the tree.

<p>But wait!  What I showed is that it only takes O(n) steps.  Is
each step O(1)?

<p>Comparing is clearly O(1) and swapping two fixed elements is also
O(1).  Finding the parent of a node is easy (integer divide the vector
index by 2).  Finally, it is trivial to find the new index for the
insertion point (just increase the insertion point by 1).

<p><strong>Remark</strong>: It is not as trivial to find the new
insertion point using a linked implementation.

<p><strong>Homework:</strong> Show the steps for inserting an element
with key 2 in the heap of Figure 2.41.
<br clear="right">

<img src="figs/down-heap.png" align="right">
<h4>Removal</h4>

<p>Trivial, right?   Just remove the root since that must contain an
element with minimum key.  Also decrease n by one.
<br>Wrong!
<br>What remains is <strong>TWO</strong> trees.

<p>We do want the element stored at the root but we must put some
other element in the root.  The one we choose is our friend the last node.

<p>But the last node is likely not to be a valid root, i.e. it will
destroy the heap property since it will likely be bigger than one of
its new children.  So we have to bubble this one down.  It is shown in
pale red on the right and the procedure explained below.
We also need
to find a new last node, but that really is trivial:  It is the node
stored at the new value of n.

<h4>Down-Heap Bubbling</h4>

<p>If the new root is the only internal node then we are done.

<p>If only one child of the root is internal (it must be the left
child) compare its key with the key of the root and swap if needed.

<p>If both children of the root are internal, choose the child with
the smaller key and swap with the root if needed.

<p>The original last node, became the root, and now has been bubbled
down to level 1.  But it might still be bigger than a child so we keep
bubbling.  At worst we need &Theta;(h) bubbling steps, which is again
logarithmic in n as desired.

<p><strong>Homework:</strong> R-2.16

<br clear="right"><br>
<table border="1" align="right">
<tr><th>Operation<th>Time
<tr><td>size, isEmpty<td>O(1)
<tr><td>minElement, minKey<td>O(1)
<tr><td>insertItem<td>&Theta;(log n)
<tr><td>removeMin<td>&Theta;(log n)
</table>

<h4>Performance</h4>

<p>The table on the right gives the performance of the heap implementation
of a priority queue.
As desired, the main operations have logarithmic time complexity.
It is for this reason that heap sort is fast.

<h4>Summary of heaps</h4>

<ul>
<li>A heap containing n elements is a complete tree T with n internal nodes
    each storing a reference to a k and a reference to an element.
    The tree also contains n+1 leaves, which are not used.
<br><br>
<li>The heap is a very fast implementation of a priority queue.
    The main operations are logarithmic and the others are constant
    time.
    <ul>
    <li>The height of the heap is O(log(n)) since T is complete.
    <li>The worst case complexity of the up- and down-heap bubbling
        are &Theta;(height)=&Theta;(log(n)).
    <li>Finding the insertion position and updating the last node
        position take constant time.
    </ul>
<br>
<li>Using these insertion and removeMin algorithms makes sorting using
    a priority queue fast, i.e., logarithmic, as we shall state
    officially in the next section.
</ul>

<h3>2.4.4 Heap-Sort (and some extras)</h3>

<p>The goal is to sort a sequence S.  We return to the PQ-sort where
we insert the elements of S into a priority queue and then use
removeMin to obtain the sorted version.  When we use a heap to
implement the priority queue, each insertion and removal takes
O(log(n)) so the entire algorithm takes O(nlog(n)).  The heap
implementation of PQ-sort is called <strong>heap-sort</strong> and we
have shown

<p><strong>Theorem</strong>:
The heap-sort algorithm sorts a sequence of n comparable elements in
O(nlog(n)) time.

<h4>Implementing Heap-Sort In Place</h4>

<p><img src="figs/heap-in-place.png" align="right">
In place means that we use the space occupied by the input.
More precisely, it means that the space required is just the input +
O(1) additional memory.  The algorithm above required &Theta;(n)
addition space to store the heap.

<p>The in place heap-sort of S assumes that S is implemented as an
array and proceeds as follows (This presentation, beyond the
definition of ``in place'' is unofficial; i.e., it will not appear on
problem sets or exams)

<ol>
<li>Logically divide the array into a portion in the front that
    contains the growing heap and the rest that contains the elements
    of the array that have not yet been dealt with.
    <ul>
    <li>Initially the heap part is empty and the not-yet-dealt-with
        part of the array is the entire array.
    <li>At each insertion we remove the left most entry from the
        array part and insert it in the heap, growing the heap to
        include the memory previously used by the newly inserted
        element.  The blue line moves down.
    <li>At the end the heap uses all the space.  We are making the
        optimization discussed before that we only store the internal
        nodes of the heap and do not leave the waste the first (index
        0) component of the array used to store the heap.
    </ul>
<li>Do the insertions a with a normal heap-sort but change the
    comparison so that a maximum element is in the root (i.e., a
    parent is no smaller than a child).
<li>Now do the removals from the heap, moving the blue line back up.
    <ul>
    <li>The elements removed are in order big to small.
    <li>This is perfect since we are going to store them starting at
        the right of the array since that is the portion of the array
        that is made available by the shrinking heap.
    </ul>
</ol>

<h4>Bottom-Up Heap Constructor (unofficial)</h4>

<p>If you are given at the beginning all n elements that are to be
inserted, the total insertion time for all inserts can be reduced to
O(n) from O(nlog(n)).  The basic idea assuming n=2<sup>n</sup>-1 is

<ol>
<li>Take out the first element and call it r.
<li>Divide the remaining 2<sup>n</sup>-2 into two parts each of size
    2<sup>n-1</sup>-1.
<li>Heap-sort each of these two parts.
<li>Make a tree with r as root and the two heaps as children.
<li>Down-heap bubble r.
</ol>

<h4>Locaters (Unofficial)</h4>

<p>Sometimes we wish to extend the priority queue ADT to include a
locater that always points to the same element even when the element
moves around.  So if x is in a priority queue and another item is
inserted, x may move during the up-heap bubbling, but the locater of x
continues to refer to x.

<h4>Comparison of the Priority Queue Implementations</h4>

<p>
<table border="1">
<tr>
  <th rows=2 align="right"><br>Method
  <th rows=2>Unsorted<br>Sequence
  <th rows=2>Sorted<br>Sequence
  <th rows=2><br>Heap
<tr>
  <td align="right">size, isEmpty
  <td align="center">O(1)
  <td align="center">O(1)
  <td align="center">O(1)
<tr>
  <td align="right">minElement, minKey
  <td align="center">O(n)
  <td align="center">O(1)
  <td align="center">O(1)
<tr>
  <td align="right">insertItem
  <td align="center">O(1)
  <td align="center">O(n)
  <td align="center">O(log(n))
<tr>
  <td align="right">removeMin
  <td align="center">O(n)
  <td align="center">O(1)
  <td align="center">O(log(n))
</table>

<h2>2.5 Dictionaries and Hash Tables</h2>

<p><strong>Dictionaries</strong>, as the name implies are used to
contain data that may later be retrieved.  Associated with each
element is the <strong>key</strong> used for retrieval.

<p>For example consider an element to be one student's NYU transcript
and the key would be the student id number.  So given the key (id
number) the dictionary would return the entire element (the
transcript).

<h3>2.5.1 the Unordered Dictionary ADT</h3>

<p>A dictionary stores <strong>items</strong>, which are key-element
(k,e) pairs.

<p>We will study ordered dictionaries in the next chapter when we
consider searching.  Here we consider unordered dictionaries.  So, for
example, we do not support findSmallestKey.  the methods we do support
are

<ul>
<li>findElement(k): Return an element having key k or signal an error if
    no such element exists.
<li>insertItem(k,e): Insert an item with key k and element e.
<li>removeElement(k): Remove an item with key k and return its
    element.  Signal an error if no such item exists.
</ul>

<h4>Trivial Implementation: log files</h4>

<p>Just store the items in a sequence.

<ul>
<li>Trivial (and fast) to insert: O(1)
<li>Minimal space: O(n)
<li>Slow for finding or removing elements: O(n) per operation
</ul>

<p><strong><big><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #13 ================
</div></big></big></strong>

<p><strong>Remark</strong>: The midterm is coming and will be on
chapters 1 and 2.  Attend class on monday to vote on the exact day.

<h3>2.5.2 Hash Tables</h3>

<p>The idea of a <strong>hash table</strong> is simple: Store the
items in an array (as done for log files) but ``somehow'' be able to
figure out quickly, i.e., O(1), which array element contains the item
(k,e).

<p>We first describe the array, which is easy, and then the
``somehow'', which is not so easy.  Indeed in some sense it is
impossible.  What we can do is produce an implementation that, on the
average, performs operations in time O(1).

<h4>Bucket Arrays</h4>

<p>Allocate an array A of size N of <strong>buckets</strong>, each able
to hold an item.  Assume that the keys are integers in the range
[0,N-1] and that no two items have the same key.  Note that N may be
<em>much</em> bigger than n.  Now simply store the item (k,e) in A[k].

<h4>Analysis of Bucket Arrays</h4>

<p>If everything works as we assumed, we have a very fast
implementation: searches, insertions, and removals are O(1).
But there are problems, which is why section 2.5 is not finished.

<ol>
<li>The keys might not be unique (although in many applications they are):
    This is a simple example of a <strong>collision</strong>.
    We discuss collisions in 2.5.5.
<li>The keys might not be integers:  We can always treat any
    (computer stored) object as an integer.  Just view the object as a
    bunch of bits and then consider that a base two non-negative
    integer.
<li>But those integers might not be ``computer integers'', that is,
    they might have more bits than the largest integer type in our
    programming language: True, we discuss methods for converting long
    bit strings into computer integers in 2.5.3.
<li>But on many machines the number of computer integers is huge.  We
    can't possibly have a bucket array of size 2<sup>64</sup>: True,
    we discuss compressing computer integers into a smaller range in 2.5.4
</ol>

<h3>2.5.3 Hash Functions</h3>

<p>We need a <strong>hash function</strong> h that maps keys to
integers in the range [0,N-1].  Then we will store the item (k,e) in
bucket A[h(k)] (we are for now ignoring collisions).  This problem is
divided into two parts.  A hash code assigns to each key a computer
integer and then a compression map converts any computer integer into
one in the range [0,N-1].  Each of these steps can introduce
collisions.  So even if the keys were unique to begin with, collisions
are an important topic.

<h4>Hash Codes</h4>

<p>A <strong>hash code</strong> assigns to any key an integer value.
The problem we have to solve is that the key may have more bits than
are permitted in our integer values.  We first view the key as bunch
of integer values (to be explained) and then combine these integer
values into one.

<p>If our integer values are restricted to 32 bits and our keys are 64
bits, we simply view the high order 32 bits as one value and the low
order as another.  In general if
<br>&lceil;numBitsInKey / numBitsInIntegerValue&rceil; = k
<br>we view the key as k integer values.  How should we combine the k
values into one?

<h4>Summing Components</h4>

<p>Simply add the k values.
<br>But, but, but what about overflows?
<br>Ignore them (or use exclusive or instead of addition).

<h4>Polynomial Hash Codes</h4>

<p>The summing components method gives very many collisions when used
for character strings.  If 4 characters fill an integer value, then
`temphash' and `hashtemp' will give the same value.  If one decided to
use integer values just large enough to hold one (unicode) character,
then there would be many, many common collisions: `t21' and `t12' for
one, mite and time for another.

<p>If we call the k integer values x<sub>0</sub>,...,x<sub>k-1</sub>,
then a better scheme for combining is to choose a positive integer
value a and compute
&Sigma;x<sub>i</sub>a<sup>i</sup>=x<sub>0</sub>+x<sub>1</sub>a+..x<sub>n-1</sub>a<sup>n-1</sup>.

<p>Same comment about overflows applies.

<p>The authors have found that using a = 33, 37, 39, or 41
worked well for character strings that are English words.

<h3>2.5.4 Compression Maps</h3>

<p>The problem we wish to solve in this section is to map integers in
some, possibly large range, into integers in the range [0,N-1].
<br>This is trivial!  Why not map all the integers into 0.
<br>We want to minimize collisions.

<h4>The Division Method</h4>

<p>This is often called the mod method, especially if you use the
``correct'' definition of mod.
One simple way to turn any integer x into one in the range [0,N-1] is
to compute |x| mod N.  That is we define the hash function h by
<pre>
            h(x) = |x| mod N
</pre>
(If we used the true mod we would not need the absolute value.)

<p>Choosing N to be prime tends to lower the collision rate, but
choosing N to be a power of 2 permits a faster computation since mod
with a power of two simply means taking the low order bits.

<h4>The MAD Method</h4>

<p>MAD stands for multiply-add-divide (mod is essentially division).
We still use mod N to get the numbers in the range, but we are a
little fancier and try to spread the numbers out first.   Specifically
we define the hash function h via.
<pre>
            h(x) = |ax+b| mod N
</pre>

<p>The values a and b are chosen (often at random) as positive
integers not a multiple of N.

<h3>2.5.5 Collision-Handling Schemes</h3>

<p>The question we wish to answer is what to do when two distinct keys
map to the same value, i.e., when h(k)=h(k').
In this case we have two items to store in one bucket.
This discussion also covers the case where we permit multiple items to
have the same key.

<h4>Separate Chaining</h4>

<p>The idea is simple, each bucket instead of holding an item holds a
reference to a container of items.
That is each bucket refers to the trivial log file implementation of a
dictionary, but only for the keys that map to this container.

<p>The code is simple, you just error check and pass the work off to
the trivial implementation used for the individual bucket.

<img src="figs/separate-chaining.png" align="right">

<pre>
Algorithm findElement(k):
    B&larr;A[h(k)]
    if B is empty then
        return NO_SUCH_KEY
    // now just do the trivial linear search
    return B.findElement(k)

Algorithm insertItem(k,e):
    if A[h(k)] is empty then
        Create B, an empty sequence-based dictionary
        A[h(k)]&larr;B
    else
        B&larr;A[h(k)]
    B.insertItem(k,e)

Algorithm removeElement(k)
    B&larr;A[h(k)
    if B is empty then
        return NO_SUCH_KEY
    else
        return B.removeElement(k)
</pre>

<p><strong>Homework:</strong> R-2.19

<h5>Load Factors and Rehashing</h5>

<p>We want the number of keys hashing to a given bucket to be small
since the time to find a key at the end of the list is
proportional to the size of the list, i.e., to the number of keys that
hash to this value.

<p>We can't do much about items that have the same key, so lets
consider the (common) case where no two items have the same key.

<p>The average size of a list is n/N, called the
<strong>load factor</strong>, where n is the number of items and N is
the number of buckets.  Typically, one keeps the load factor below
1.0.
The text asserts that 0.75 is common.

<p>What should we do as more items are added to the dictionary?
We make an ``extendable dictionary''.  That is, as with an extendable
array we double N and ``fix everything up''  In the case of an
extendable dictionary, the fix up consists of recalculating the hash
of every element (since N has doubled).  In fact no one calls this an
extendable dictionary.  Instead one calls this scheme rehashing since
one must rehash (i.e., recompute the hash) of each element when N is
changed.  Also N is normally chosen to be a prime number so instead of
doubling, one chooses for the new N the smallest prime number above
twice the old N.

<h4>Open Addressing</h4>

<p>Separate chaining involves two data structures: the buckets and the
log files.
An alternative is to dispense with the log files and always store
items in buckets, one item per bucket.
Schemes of this kind are referred to as <strong>open
addressing</strong>.  The problem they need to solve is where to put
an item when the bucket it should go into is already full?
There are several different solutions.  We study three:  Linear
probing, quadratic probing, and double hashing.

<h5>Linear Probing</h5>

<img src="figs/linear-probing.png" align="right">
<p>This is the simplest of the schemes.
To insert a key k (really I should say ``to insert an item (k,e)'') we
compute h(k) and initially assign k
to A[h(k)].  If we find that A[h(k)] contains another key, we assign
k to A[h(k)+1].
It that bucket is also full, we try A[h(k)+2], etc.
Naturally, we do the additions mod N so that after trying A[N-1] we
try A[0].  So if we insert (16,e) into the dictionary at the
right, we place it into bucket 2.

<p>How about finding a key k (again I should say an item (k,e))?
We first look at A[h(k)].  If this bucket contains the key, we have
found it.  If not try A[h(k)+1], etc and of course do it mod N (I will
stop mentioning the mod N).  So if
we look for 4 we find it in bucket 1 (after encountering two keys
that hashed to 6).
<br><strong>WRONG!</strong>
<br>Or perhaps I should say incomplete.  What if the item is not on
the list?  How can we tell?
<br>Ans: If we hit an empty bucket then the item is not present (if it
were present we would have stored it in this empty bucket).  So 20
is not present.
<br>What if the dictionary is full, i.e., if there are no empty
buckets.
<br>Check to see if you have wrapped all the way around.  If so, the
key is not present

<p>What about removals?
<br>Easy, remove the item creating an empty bucket.
<br><strong>WRONG!</strong>
<br>Why?
<br>I'm sorry you asked.  This is a bit of a mess.
Assume we want to remove the (item with) key 19.
If we simply remove it, and search for 4 we will incorrectly
conclude that it is not there since we will find an empty slot.
<br>OK so we slide all the items down to fill the hole.
<br><strong>WRONG!</strong>  If we slide 6 into the whole at 5, we
will never be able to find 6.
<br>So we only slide the ones that hash to 4??
<br><strong>WRONG!</strong>  The rule is you slide all keys that are
not at their hash location until you hit an empty space.

<p>Normally, instead of this complicated procedure for removals, we
simple mark the bucket as removed by storing a special value there.
When looking for keys we skip over such slots.  When an insert hits
such a bucket, the insert uses the bucket.  (The book calls this a
``deactivated item'' object).

<p><strong>Homework:</strong> R-2.20

<h5>Quadratic Probing</h5>

<p>All the open addressing schemes work roughly the same.
The difference is which bucket to try if A[h(k)] is full.
One extra disadvantage of linear probing is that it tends to cluster
the items into contiguous runs, which slows down the algorithm.

<p><strong>Quadratic probing</strong> attempts to spread items out by
trying buckets A[h(k)], A[h(k)+1], A[h(k)+4], A[h(k)+9], etc.
One problem is that even if N is prime this scheme can fail to find an
empty slot even if there are empty slots.

<p><strong>Homework:</strong> R-2.21

<h5>Double Hashing</h5>

<p>In <strong>double hashing</strong> we have two hash functions h and
h'.  We use h as above and, if A[h(k)] is full, we try A[h(k)+1],
A[h(k)+h'(k)], A[h(k)+2h'(k)], etc.

<p>The book says h'(k) is often chosen to be q - (k mod q) for some
prime q &lt; N.  I note again that if mod were defined correctly this would
look more natural, namely (q-k) mod q.  We will not consider which
secondary hash function h' is good to use.

<p><strong>Homework:</strong> R-2.22

<h4>Open Addressing vs Separate Chaining</h4>

<p>A hard choice.  Separate chaining seems more space, but that is
deceiving since it all depends on the loading factor.
In general for each scheme the lower the loading factor, the faster
scheme but the more memory it uses.

<h3>2.5.6 Universal Hashing (skipped)</h3>

<h2>2.6 Java Example: Heap (skipped)</h2>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #14 ================
</div></big></strong>

<p><strong>Remark</strong>: From robin simon
<br>The last day for students to withdraw is Nov. 5th.
Therefore the exam should be returned at least a week
before then.

<h1>Chapter 3 Search Trees and Skip Lists</h1>

<h2>3.1 Ordered Dictionaries and Binary Search Trees</h2>

<p>We just studied unordered dictionaries at the end of chapter 2.
Now we want to extend the study to permit us to find the "next" and
"previous" items.
More precisely we wish to support, in addition to findElement(k),
insertItem(k,e), and removeElement(k), the new methods

<ul>
<li>closestKeyBefore(k): Return the key of the item with largest key
less than or equal to k.
<li>closestElemBefore(k): Return the element of the item with largest key
less than or equal to k.
<li>closestKeyAfter(k): Return the key of the item with smallest key
greater than or equal to k.
<li>closestElemAfter(k): Return the element of the item with smallest key
greater than or equal to k.
</ul>

<p>We naturally signal an exception if no such item exists.  For
example if the only keys present are 55, 22, 77, and 88, then
closestKeyAfter(90) or closestElemBefore(2) each signal an exception.

<p>We begin with the most natural implementation.

<h3>3.1.1 Sorted Tables</h3>

<p>We use the sorted vector implementation from chapter 2 (we used it
as a simple implementation of a priority queue).
Recall that this keeps the items sorted in key order.
Hence it is O(n) for inserts and removals, which is slow; however, we
shall see that it is fast for finding and element and for the four new
methods closestKeyBefore(k) and friends.
We call this a <strong>lookup table</strong>.

<p>The space required is &Theta;(n) since we grow and shrink the array
supporting the vector
(see <a href="#extendable-array">extendable arrays</a>).

<p>As indicated the key favorable property of a lookup table is that
it is fast for (surprise) lookups using the binary search algorithm
that we study next.

<h4>Binary Search</h4>

<p>In this algorithm we are searching for the rank of the item
containing a key equal to k.  We are to return a special value if no
such key is found.

<p>The algorithm maintains two variables lo and hi, which are
respectively lower and upper bounds on the rank where k
will be found (assuming it is present).

<p>Initially, the key could be anywhere in the vector so we start
with lo=0 and hi=n-1.  We write key(r) for the key at rank r and
elem(r) for the element at rank r.

<p>We then find mid, the rank (approximately) halfway between lo and hi
and see how the key there compares with our desired key.

<ul>
<li>If k = key(mid), we have found the item and return elem(mid)
<li>If k &lt; key(mid), then we restrict our attention to indexes less
    than mid.
<li>If k &gt; key(mid), then we restrict our attention to indexes greater
    than mid.
</ul>

<p>Some care is need in writing the algorithm precisely as it is easy
to have an ``off by one error''.  Also we must handle the case in
which the desired key is not present in the vector.  This occurs when
the search range has been reduced to the empty set (i.e., when lo
exceeds hi).

<pre>
Algorithm BinarySearch(S,k,lo,hi):
    Input:  An ordered vector S containing (key(r),elem(r)) at rank r
            A search key k
            Integers lo and hi
    Output: An element of S with key k and rank between lo and hi.
            NO_SUCH_KEY if no such element exits

If lo &gt; hi then
    return NO_SUCH_KEY                    // Not present

mid &larr; &lfloor;(lo+hi)/2&rfloor;
if k = key(mid) then
    return elem(mid)                     // Found it
if k &lt; key(mid) then
    return BinarySearch(S,k,lo,mid-1)    // Try bottom ``half''
if k &gt; key(mid) then
    return BinarySearch(S,k,mid+1,hi)    // Try top ``half''
</pre>

<p>Do some examples on the board.

<h4>Analysis of Binary Search</h4>

<p>It is easy to see that the algorithm does just a few operations per
recursive call.  So the complexity of Binary Search is
&Theta;(NumberOfRecursions).  So the question is "How many recursions
are possible for a lookup table with n items?".

<p>The number of eligible ranks (i.e., the size of the range we still
must consider) is hi-lo+1.

<p>The key insight is that when we recurse, we have reduced the range
to at most half of what it was before.  There are two possibilities,
we either tried the bottom or top ``half''.  Let's evaluate hi-lo+1
for the bottom and top half.  Note that the only two possibilities for
&lfloor;(lo+hi)/2&rfloor; are (lo+hi)/2 or (lo+hi)/2-(1/2)=(lo+hi-1)/2

<p>Bottom:
<tt><br>
  (mid-1)-lo+1 = mid-lo = &lfloor;(lo+hi)/2&rfloor;-lo
  &le; (lo+hi)/2-lo = (hi-lo)/2&lt;(hi-lo+1)/2
</tt>

<p>Top:
<tt><br>
  hi-(mid+1)+1 = hi-mid = hi-&lfloor;(lo+hi)/2&rfloor;
  &le; hi-(lo+hi-1)/2 = (hi-lo+1)/2
</tt>

<p>So the range starts at n and is halved each time and remains an
integer (i.e., if a recursive call has a range of size x, the next
recursion will be at most &lfloor;x/2&rfloor;).

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #15 ================
</div></big></strong>

<p>Write on the board 10 times
<br>(X-1)/2 &le; &lfloor;X/2&rfloor; &le X/2
<br>If B &le; A, then Z-A &le; Z-B

<p>How many recursions are possible?  If the range is ever zero, we
stop (and declare the key is not present) so the longest we can have
is the number of times you can divide by 2 and stay at least 1.  That
number is &Theta;(log(n)) showing that binary search is a logarithmic
algorithm.

<p><strong>Problem Set</strong> 3, Problem 1
Write the algorithm closestKeyBefore.  It uses the same idea as
BinarySearch.

<p>When you do question 1 you will see that the complexity is
&Theta;(log(n)).  Proving this is not hard but is <strong>not</strong>
part of the problem set.

<p>When you do question 1 you will see that
closestElemBefore, closestKeyAfter, and closestElemAfter are all very
similar to closestKeyBefore.  Hence they are all logarithmic
algorithms.
Proving this is not hard but is <strong>not</strong>
part of the problem set.

<h4>Performance Summary of the Array-Based Implementations</h4>

<table border="1">
<tr><th>Method<th>Log File<th>Lookup Table
<tbody align="center">
<tr><td>findElement<td>O(n)<td>O(log n)
<tr><td>insertItem<td>O(1)<td>O(n)
<tr><td>removeElement<td>O(n)<td>O(n)
<tr><td>closestKeyBefore<td>O(n)<td>O(log n)
<tr><td>closestElemBefore<td>O(n)<td>O(log n)
<tr><td>closestKeyAfter<td>O(n)<td>O(log n)
<tr><td>closestElemAfter<td>O(n)<td>O(log n)
</table>

<p>Our goal now is to find a better implementation so that all the
complexities are logarithmic.
This will require us to shift from vectors to trees.

<p><img src="figs/binary-search-tree.png" align="right">
<h3>3.1.2 Binary Search Trees</h3>

<p>This section gives a simple tree-based implementation, which alas
fails to achieve the logarithmic bounds we seek.  But it is a good
start and motivates the AVL trees we study in 3.2 that do achieve the
desired bounds.

<p><strong>Definition</strong>: A <strong>binary search tree</strong>
is a tree in which each internal node v stores an item such that the
keys stored in every node in the left subtree of v are less than or
equal to the key at v which is less than or equal to every key stored
in the right subtree.

<p>From the definition we see easily that an inorder traversal of the
tree visits the internal nodes in nondecreasing order of the keys they
store.

<p>You search by starting at the root and going left or right if the
desired key is smaller or larger respectively than the key at the
current node.  If the key at the current node is the key you seek, you
are done.  If you reach a leaf the desired key is not present.

<p>Do some examples using the tree on the right.  E.g. search for 17,
80, 55, and 65.

<p><strong>Homework:</strong> R-3.1 and R-3.2

<h3>3.1.3 Searching in a Binary Search Tree</h3>

<p>Here is the formal algorithm described above.

<pre>
Algorithm TreeSearch(k,v)
Input:  A search key k and a node v of a binary search tree.
Output: A node w in the subtree routed at v such that either
        w is internal and k is stored at w or
        w is a leaf where k would be stored if it existed

if v is a leaf then
    return v
if k=k(v) then
    return v
if k&lt;k(v) then
    return TreeSearch(k,T.leftChild(v))
if k&gt;k(v) then
    return TreeSearch(k,T.rightChild(v))
</pre>

<p>Draw a tree on the board and illustrate both finding a k and no
such key exists.

<h4>Analysis of Binary Tree Searching</h4>

<p>It is easy to see that only a couple of operations are done per
recursive call and that each call goes down a level in the tree.
Hence the complexity is O(height).

<p>So the question becomes "How high is a tree with n nodes?".
As we saw last chapter the answer is "It depends.".

<p>Next section we will learn a technique for keeping trees low.

<h3>3.1.4 Insertion in a Binary Search Tree</h3>

<p>To insert an item with key k, first execute
w&larr;TreeSearch(k,T.root()).  Recall that if w is internal, k is
already in w, and if w is a leaf, k "belongs" in w.  So we proceed as
follows.

<ul>
<li>If w is a leaf, replace w with an internal node containing k
    (having two leaves as children).
<li>If w is internal and duplicate keys are not permitted, signal
    an error.
<li>If w is internal and duplicate keys are permitted, call
    w=TreeSearch(k,T.leftChild(v)) or w=TreeSearch(k,T.rightChild(v))
    and proceed as above.
</ul>

<p>Draw examples on the board showing both cases (leaf and internal
returned).

<p>Once again we perform a constant amount of work per level of the
tree implying that the complexity is O(height).

<h3>3.1.5 Removal in a Binary Search Tree</h3>

<p>This is the trickiest part, especially in one case as we describe
below.  The key concern is that we cannot simply remove an item from
an internal node and leave a hole as this would make future searches
fail.  The beginning of the removal technique is familiar:
w=TreeSearch(k,T.root()).  If w is a leaf, k is not present, which we
signal.

<p>If w is internal, we have found k, but now the fun begins.
Returning the element with key k is easy, it is the element
stored in w.
We need to actually remove w, but we cannot leave a hole.
There are three cases.

<img src="figs/bin-search-tree-removal-1.png" align="right">
<ol>
<li>
    If we are lucky both of w's children are leaves.  Then we can
    simply replace w with a leaf.  (Recall that leaves do not contain
    items.)  This is the trivial case<br><br>
<li>
    The next case is when one child of w is a leaf and the other, call
    it z, is an internal node.
    In this case we can simply replace w by z;
    that is have the parent of w now point to z.
    This removes w as desired and also removes the leaf child of w,
    which is OK since leaves do not contain items.
    This is the easy case.<br><br>
<li>
    Note that the above two cases can be considered the same.
    In both cases we notice that one child of w is a leaf and replace
    w by the other child (and its descendents, if any).<br><br>
<li>
    Now we get to the difficult case: both children of w are internal
    nodes.  What we will do is to replace the item in w with the item
    that has the next highest key.<br><br>
    <ul>
    <li>
        First we must find the item with the next highest key.
        But that is simply the next item in the inorder traversal.
        So we go right and then keep going left until we get a leaf.
        The parent of this leaf is the item we seek.  Call the parent
        y.<br><br>
    <li>
        Store the item in y in the node w.  This removes the old
        item of w, which we wanted to do.
        <ul>
        <li>
            Does the tree still have its items in the correct order?
            That is are parents still bigger than (or equal to if we
            permit duplicate keys) all of the left subtree and smaller
            than all of the right subtree?
        <li>
            Yes.  The only new parent is the item y which has now
            moved to node w.  But this is the item right after the old
            item in w.  Since it came from the right subtree it is
            bigger than the left and since it was the smallest in the
            right, it is smaller than the right.
        </ul>
        <br>
    <li>
        But what about the old node y?  It's left child is a leaf so
        it is the easy or trivial case and we just replace y by the
        other child and its descendants.
    </ul>
</ol>

<br clear="right">
It is again true that the complexity is O(height) but this is
not quite as easy to see as before.  We start with a TreeSearch, which
is &Theta;(height).  This gets us to w.  The most difficult case is
the last one where w has two internal children.  We spend a
non-constant time on node w because we need to find the next item.
But this operation is only O(height) and we simply descend the tree deeper.
<br clear="right">

<h3>3.1.6 Performance of Binary Search Trees</h3>

<table border="1" align="right">
<caption align=bottom>
  Time complexity of the binary search tree ADT.
  We use h for the height and s for the number of elements in the tree.
</caption>
<tr>
  <th align="right">Method<th>Time
<tr>
  <td align="right">size, isEmpty<td>O(1)
<tr>
  <td align="right">findElement, insertItem, removeElement<td>O(h)
<tr>
  <td align="right">findAllElements, removeAllElements<td>O(h+s)
</table>

<p>We have seen that findElement, insertItem, and removeElement have
complexity O(height).  It is also true, but we will not show it, that
one can implement findAllElements and removeAllElements in time
O(height+numberOfElements).  You might think removeAllElements should
be constant time since the resulting tree is just a root so we can
make it in constant time.  But removeAllElements must also return an
iterator that when invoked must generate each of the elements removed.

<h4>Comments on average vs worst case behavior</h4>

<p>In a sense that we will not make precise, binary search trees have
logarithmic performance since `most' trees have logarithmic height.

<p>Nonetheless we know that there are trees with height &Theta;(n).
You produced several of them for problem set 2.
For these trees binary search takes linear time, i.e., is slow.  Our
goal now is to fancy up the implementation so that the trees are never
very high.  We can do this since the trees are not handed to us.
Instead they are build up using our insertItem method.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #16 ================
</div></big></strong>

<img src="figs/avl.png" align="right">
<h2>3.2 AVL Trees</h2>

<p>Named after its inventors Adel'son-Vel'skii and Landis.

<p><strong>Definition</strong>: An <strong>AVL</strong> tree is a
binary search tree that satisfies the <strong>height-balance
property</strong>, which means that for every internal node, the
height of the two children can differ by at most 1.

<p><strong>Homework:</strong> R-3.3

<p>Since the algorithms for an AVL tree require the height to be
computed and this is an expensive operation, each node of an AVL tree
contains its height (say as v.height()).

<p><strong>Remark</strong>: You can actually be fancier and store just
two bits to tell whether the node has the same height as its sibling,
one greater, or one smaller.

<p>We see that the height-balance property prevents the tall skinny
trees you developed in problem set 2.  But is it really true that the
height must be O(log(n))?

<p>Yes it is and we shall now prove it.  Actually we will prove
instead that an AVL tree of height h has at least 2<sup>(h/2)-1</sup>
internal nodes from which the desired result follows easily.

<p><strong>Lemma</strong>: An AVL tree of height h has at least
2<sup>(h/2)-1</sup> internal nodes.

<p><strong>Proof</strong>:
Let n(h) be the minimum number of internal nodes in an AVL tree of height h.

<p>n(1)=1 and n(2)=2.  So the lemma holds for h=1 and h=2.

<p>Here comes the key point.

<p>Consider an AVL tree of height h&ge;3
and the minimum number of nodes.  This tree is composed of a root,
and two subtrees.  Since the whole tree has the minimum number of
nodes for its height so do the subtrees.  For the big tree to be of
height h, one of the subtrees must be of height h-1.  To get the
minimum number of nodes the other subtree is of height h-2.
<br>Why can't the other subtree be of height h-3 or h-4?
<br>The height of siblings can differ by at most 1!

<p>What the last paragraph says in symbols is that for h&ge;3,
<br>n(h) = 1+n(h-1)+n(h-2)

<p>The rest is just algebra, i.e. has nothing to do with trees,
heights, searches, siblings, etc.

<p>n(h) &gt; n(h-1) so n(h-1) &gt; n(h-2).  Hence

<p>n(h) &gt; n(h-1)+n(h-2) &gt; 2n(h-2)

<p>Really we could stop here.  We have shown that n(h) at least
doubles when h goes up by 2.  This says that n(h) is exponential in h
and hence h is logarithmic in n.  But we will proceed slower.
Applying the last formula i times we get

<p>For any i&gt;0, &nbsp; n(h) &gt; 2<sup>i</sup>n(h-2i) &nbsp;&nbsp; (*)

<p>Let's find an i so that h-2i is guaranteed to be 1 or 2, since this
would guarantee that n(h-2i) &ge; 1.
<br>I claim i = &lceil;h/2&rceil;-1 works.

<p>If h is even h-2i = h-(h-2) = 2

<p>If h is odd h-2i = h - (2&lceil;h/2&rceil;-2) = h - ((h+1)-2) = 1

<p>Now we plug this value of i into equation (*) and get for h&ge;3

<p>n(h)    &gt; 2<sup>i</sup>n(h-2i)
<br>&nbsp;&nbsp; =    2<sup>&lceil;h/2&rceil;-1</sup>n(h-2i)
<br>&nbsp;&nbsp; &ge; 2<sup>&lceil;h/2&rceil;-1</sup>(1)
<br>&nbsp;&nbsp; &ge; 2<sup>(h/2)-1</sup>

<p><strong>Theorem</strong>: the height of an AVL tree storing n items
is O(log(n)).

<p><strong>Proof</strong>:
From the lemma we have n(h) &gt; 2<sup>(h/2)-1</sup>.

<p>Taking logs gives log(n(h)) &gt; (h/2)-1 or

<p>h &lt 2log(n(h))+2

<p>Since n(h) is the smallest number of nodes possible for an AVL tree of
height h, we see that h &lt; 2 log(n) for any AVL tree of height h.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #17 ================
</div></big></strong>

<h1 align="center">Midterm Exam</h1>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #18 ================
</div></big></strong>

<h1 align="center">Reviewed Answers for Midterm Exam</h1>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #19 ================
</div></big></strong>

<p><img src="figs/mod.png" align="right">
<h3>A Review of (the Real) Mod</h3>

<p>I was asked to review modulo as it is no longer taught in high
school, which I didn't know.

<p>The top diagram shows an almost ordinary analog clock.  The major
difference is that instead of 12 we have 0.  The hands would be useful
if this was a video, but I omitted them for the static picture.
Positive numbers go clockwise (cw) and negative counter-clockwise
(ccw).  The numbers shown are the values mod 12.  This example is good
to show arithmetic.  (2-5) mod 12 is obtained by starting at 2 and
moving 5 hours ccw, which gives 9.  (-7) mod 12 is (0-7) mod 12 is
obtained by starting at 0 and going 7 hours ccw, which gives 5.

<p>To get mod 8, divide the circle into 8 <q>hours</q> instead of 12.

<p>The bottom picture shows mod 5 in a linear fashion.  In pink are
the 5 values one can get when doing mod 5, namely 0, 1, 2, 3, and 4.
I only illustrate numbers from -3 to 11 but that is just due to space
limitations.  Each blue bar is 5 units long so the numbers at its
endpoints are equal mod 5 (since they differ by 5).  So you just lay
off the blue bar until you wind up in the pink.

<h3>End of Mod Review</h3>

<br clear="right">
<h3>3.2.1 Update Operations</h3>

<h4>Insertion</h4>

<p>Begin by a standard binary search tree insertion.  In the diagrams
on the right, black shows the situation before the insertion; red
after.
The numbers are the heights.
Ignore the blue markings, they are explained in the text as needed.
<img src="figs/avl-ins-2.png" align="right">
<ol>
<li>
    Do a find for the key to be inserted.<br><br>
<li>
    Presumably wind up at a leaf (meaning the key not already in the
    tree).<br><br>
<li>
    Call this leaf w.<br><br>
<li>
    Insert the item by converting w to an internal node at
    height one (i.e., the new internal node has two leaves as
    children).<br><br>
<li>
    Call the new internal node w.  We view the previous operation as
    expanding the leaf w into an internal node (with leaves as
    children).<br><br>
<li>
    In the diagram below, w is converted from a black leaf into a
    red height 1 node.
</ol>

<p>Why aren't we finished?
<ol>
<li>
    The tree may no longer be in balance, i.e. it may no longer by an
    AVL tree.<br><br>
<li>
    We have raised the height of w by 1 (from 0 to 1).<br><br>
<li>
    We <strong>may</strong> have raised the height of w's parent by
    1, depending on the height of w's sibling.
    <ul>
    <li>
        Since w <strong>was</strong> at height 0 and the tree was
        balanced, the sibling must have height -1, 0, or +1.  Since -1
        is impossible, we have just two possibilities.<br><br>
    <li>
        If the sibling was at height 1, their parent was, and still
        is, at height 2.  Hence no heights other than w's changed and
        the tree remains in balance.  So in this case, which is illustrated
        in the figure above, we are indeed done.<br><br>
    <img src="figs/avl-ins-1.png" align="right">
    <li>
        If the sibling was at height 0, their parent was at height 1
        and has had its height changed to 2 (since w is at height 1).
        This situation is illustrated on the right.  Note that not all
        of the tree is shown.
        <ul>
        <li>
            The top node need not be the root of the tree, i.e. it
            could be the right or left child of a node further up.
        <li>
            Nodes not ancestors of w do not have their descendants
            shown.
        <li>
            Recall that the black shows the original tree fragment and
            the red is what happens after we expand w to do the
            insertion. The black and red numbers are the heights
            before and after.
        <li>
            For the moment ignore the blue shading and the blue 2.
        </ul><br>
    <li>
        Since the parent's height has increased, this may affect the
        height of the parent's parent, etc.  This is also shown in the
        diagram.  We see the red numbers uniformly 1 higher than the
        corresponding blue.<br><br>
    <li>
        So we need to start at w and proceed up the tree to find
        possible imbalances.  Imbalance can only occur at an ancestor
        of w since these are the only nodes whose height's have
        changed by the expansion of w to an internal node at height 1.
    </ul><br>
<li>
    What is the problem?  We just proceed up as in the figure on the
    right and eventually we hit the root and are done.<br><br>
<li>
    <strong>WRONG</strong>.<br><br>
<li>
    In the figure, before the insertion, siblings had the same height.
    This is certainly possible but not required.
    We know that the heights of siblings could have differed by 1.<br><br>
<li>
    If an ancestor of w, had height one less than its sibling, then
    the insertion has made them equal.  That is a particularly easy
    case.  The height of the parent doesn't change and we are done.
    Recall that this situation was illustrated in the previous (small)
    figure.<br><br>
<li>
    The difficult case occurs when an ancestor of w had height one
    greater than its sibling, then the insertion has made it two
    greater, which is not permitted for an AVL tree.<br><br>
<li>
    For example the light blue node could have originally been at
    height 2 instead of 3.
</ol>
<br clear="right">

The top left diagram illustrates the problem case from the previous
figure (k was 3 above).
<ul>
<img src="figs/rotations.png" align="right">
<li>
    Node x is an ancestor of w (the node where the insert
    occurred).<br><br>
<li>
    Node x has had it height raised from k-1 to k and node y, the
    parent of x, has had its height raised from k to k+1.<br><br>
<li>
    The height of y was one greater than its sibling (which was the
    blue node above).  The height difference is now 2.
    The parent of y is z.<br><br>
<li>
    Node z was at height k+1 with children at height k and k-1, but now
    the higher child has had its height raised so z's children have
    height differing by 2.<br><br>
<li>
    Thus the tree is out of balance (it is no longer an AVL
    tree).<br><br>
<li>
    In this diagram y is the right child of z and x is the left child
    of y.  There are three other possibilities, which are also shown
    (but with less detail).<br><br>
<li>
    We will isolate on the three nodes x y and z and their subtrees
    and see how to rearrange the diagram to restore balance.
</ul>

<br clear="right">
<strong>Definition</strong>: The operation we will perform when x,
y, and z lie in a straight line is called a <strong>single
rotation</strong>.  When x, y, and z form an angle, the operation is
called a <strong>double rotation</strong>.

<br clear="right">

<p><img src="figs/double-rotation-1.png" align="right">
Let us consider the upper left double rotation, which is the rotation
that we need to apply the example above.
It is redrawn to the right with the subtrees drawn and their heights
labeled.  The colors are so that you can see where they trees go when
we perform the rotation.

<p>Recall that x is
an ancestor of w and has had its height raised from k-1 to k.  The
sibling of x is at height k-1 and so is the sibling of y.
The reason x had its height raised is that one of its siblings (say
the right one) has been raised from k-2 to k-1.
<br>How do I know the other one is k-2?
<br>Ans: we will discuss it later.

<p>The double rotation transforms the picture on top to the one on the
bottom.  We now actually are done with the insertion.  Let's check the
bottom picture and make sure.
<ol>
<li>
    The order relation remains intact.  That is, every node is greater
    than its entire left subtree and less than its entire right
    subtree.<br><br>
<li>
    The tree (now rooted at y) is balanced.<br><br>
<li>
    Nodes x and z are each at height k.  Hence y, the root of this
    tree is at height k+1.<br><br>
<li>
    The tree above, rooted at z, has height k+2.<br><br>
<li>
    But remember that before the insert z was at height k+1.<br><br>
<li>
    So the rotated tree (which is after the insert) has the same
    height as the original tree before the insert.<br><br>
<li>
    Hence every node above z in the original tree keeps its original
    height so the entire tree is now balanced.<br><br>
</ol>

<p>Thus, if an insertion causes an imbalance, <strong>just
one</strong> rotation re-balances the tree <strong>globally</strong>.
We will see that for removals it is not this simple.

<p>Answer to question posed above.  One node was at height k-2 and its
parent was at k-1.  Hence the sibling was either at k-2 or k-3.  But
if it was at k-3, we would have found the tree out of balance when we
got to x (x at k-1 after insertion, sibling at k-3).  However, z is
assumed to be the first time we found the tree out of balance starting
from w (the insertion point) and moving up.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #20 ================
</div></big></strong>

<p>Here are the three pictures for the remaining three possibilities.
That is, the other double rotation and both single rotations.  The
original configuration is shown on top and the result after the
rotation is shown immediately below.

<p><img src="figs/double-rotation-2.png" align="right">
   <img src="figs/single-rotation-1.png" align="left">
   <img src="figs/single-rotation-2.png" align="center">

<p>What is the complexity of insertion?
<ol>
<li>
    Let n be the number of nodes in the tree before the
    insertion.<br><br>
<li>
    Finding the insertion point is &Theta;(log n).<br><br>
<li>
    Expanding the leaf and inserting the item is &Theta;(1).<br><br>
<li>
    Walking up the tree looking for an imbalance is &Theta;(1) per
    level, which is O(log n) since the tree has height &Theta;(log
    n).<br><br>
<li>
    Performing the <strong>one</strong> needed rotation is
    &Theta;(1).<br><br>
</ol>

<p>Hence we have the following

<p><strong>Theorem</strong>: The complexity of insertion in an AVL
tree is &Theta;(log n).

<p><strong>Problem Set</strong> 3, problem 2.
Please read the <strong>entire</strong> problem before beginning.
<ol type="A">
<li>
    Draw an avl tree containing items with integer keys.  Draw the
    internal nodes as circles and write the key inside the circle.
    Draw the leaves as squares; leaves do not contain items.
    You will want to read the remaining parts before drawing this
    tree.
<li>
    Choose a key not present in the tree you drew in part A whose
    insertion will require the "other" double rotation in order to
    restore balance (i.e., the double rotation shown in the diagram
    above showing one double and two single rotations).
    Draw the tree after the insertion, but prior to the rotation.
    Then draw the tree after the rotation.
<li>
    Choose a key not present in the tree you drew in part A whose
    insertion will require a single rotation in order to
    restore balance.
    Draw the tree after the insertion, but prior to the rotation.
    Then draw the tree after the rotation.
<li>
    Choose a key not present in the tree you drew in part A whose
    insertion will require the "other" single rotation in order to
    restore balance
    Draw the tree after the insertion, but prior to the rotation.
    Then draw the tree after the rotation.
</ol>

<h4>Removal</h4>

<p>In order to remove an item with key k, we begin just as we did for
an ordinary binary search tree.  I repeat the procedure here.

<p>The key concern is that we cannot simply remove an item from
an internal node and leave a hole as this would make future searches
fail.  The beginning of the removal technique is familiar:
w=TreeSearch(k,T.root()).  If w is a leaf, k is not present, which we
signal.

<p>If w is internal, we have found k, but now the fun begins.
Returning the element with key k is easy, it is the element
stored in w.
We need to actually remove w, but we cannot leave a hole.
There are three cases.

<img src="figs/bin-search-tree-removal-2.png" align="right">
<ol>
<li>
    If we are lucky both of w's children are leaves.  Then we can
    simply replace w with a leaf.  (Recall that leaves do not contain
    items.)  This is the trivial case<br><br>
<li>
    The next case is when one child of w is a leaf and the other, call
    it z, is an internal node.
    In this case we can simply replace w by z;
    that is have the parent of w now point to z.
    This removes w as desired and also removes the leaf child of w,
    which is OK since leaves do not contain items.
    This is the easy case.<br><br>
<li>
    Note that the above two cases can be considered the same.
    In both cases we notice that one child of w is a leaf and replace
    w by the other child (and its descendents, if any).<br><br>
<li>
    Now we get to the difficult case: both children of w are internal
    nodes.  What we will do is to replace the item in w with the item
    that has the next highest key.<br><br>
    <ul>
    <li>
        First we must find the item with the next highest key.
        But that is simply the next item in the inorder traversal.
        So we go right and then keep going left until we get a leaf.
        The parent of this leaf is the item we seek.  Call the parent
        y.<br><br>
    <li>
        Store the item in y in the node w.  This removes the old
        item of w, which we wanted to do.
        <ul>
        <li>
            Does the tree still have its items in the correct order?
            That is are parents still bigger than (or equal to if we
            permit duplicate keys) all of the left subtree and smaller
            than all of the right subtree?
        <li>
            Yes.  The only new parent is the item y which has now
            moved to node w.  But this is the item right after the old
            item in w.  Since it came from the right subtree it is
            bigger than the left and since it was the smallest in the
            right, it is smaller than the right.
        </ul>
        <br>
    <li>
        But what about the old node y?  It's left child is a leaf so
        it is the easy or trivial case and we just replace y by the
        other child and its descendants.
    </ul>
</ol>

<p>But now we have to possibly restore balance, i.e., maintain the AVL
property.  The possible trouble is that the light green node on the
left has been replaced by the light blue on the right, which is of
height one less.  This <strong>might</strong> cause a problem.  The
sibling of the light green (not shown) might have height equal to, one
less than, or one greater than, the light green.
<ol>
<li>
    If the sibling's height was equal to the light green, it is one
    greater than the light blue, which is still in balance and their
    parent has not changed height so all is well.
<li>
    If the sibling's height was one less than the light green, it is
    equal to the light blue, which remains in balance.  But their
    parent height has dropped by one so we need to look at it to see
    if there is trouble.
<li>
    If the sibling's height was one greater than the light green, it
    is two greater than the light blue so we are out of balance.
</ol>

<p>To summarize the three cases either
<ol>
<li>
    We are balanced and done.
<li>
    We proceed to the parent of the light blue and try again.
<li>
    We must re-balance.
</ol>

<p>In the second case we move up the tree and again have one of the
same three cases so either.
<ol>
<li>
    We will hit a case 1 and are done.
<li>
    We reach the root and are done.
<li>
    We need to re-balance
</ol>

<p>The re-balancing is again a single or double rotation (since the
problem is the same so is the solution).

<p>The rotation will fix the problem but the result has a highest node
whose height is one less the highest prior to the rotation (in the
diagrams for single and double rotation the largest height dropped
from k+2 to k+1).

<p>Unlike the case for insertions this height reduction does not
cancel out a previous height increase.  Thus the lack of balance may
continue to advance up the tree and more rotations may be needed.

<p><strong>Problem Set</strong> 3 problem 3 (end of problem set 3).
Please read the <strong>entire</strong> problem before beginning.
<ol type="A">
<li>
    Draw an avl tree containing items with integer keys.  Draw the
    internal nodes as circles and write the key inside the circle.
    Draw the leaves as squares; leaves do not contain items.
    You will want to read the remaining parts before drawing this
    tree.
<li>
    Choose a key present in the tree you drew in part A whose
    removal will require a double rotation <strong>and</strong> a
    single rotation in order to
    restore balance.
    Draw the tree after the removal, but prior to the rotations.
    Then draw the tree after the double rotation, but prior to the
    single rotation.
    Finally, draw the tree after both rotations.
</ol>

<p>What is the complexity of a removal?  Remember that the height of
an AVL tree is &Theta;(log(N)), where N is the number of nodes.
<ol>
<li>
    We must find a node with the key, which has complexity
    &Theta;(height) = &Theta;(log(N)).
<li>
    We must remove the item: &Theta;(1).
<li>
    We must re-balance the tree, which might involve &Theta;(height)
    rotations.  Since each rotation is &Theta;(1), the complexity is
    &Theta;(log(N))*&Theta;(1) = &Theta;(log(N)).
</ol>

<p><strong>Theorem</strong>: The complexity of removal for an AVL tree
is logarithmic in the size of the tree.

<h3>3.2.2 Performance</h3>

<p>The news is good.
Search, Inserting, and Removing all have logarithmic complexity.

<p>The three operations all involve a sweep down the tree searching
for a key, and possibly an up phase where heights are adjusted and
rotations are performed.  Since only a constant amount of work is
performed per level and the height is logarithmic, the complexity is
logarithmic.

<P><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #21 ================
</div></big></strong> 

<h2>3.3 Bounded-Depth Search Trees</h2>

<p>Might come back to this if time permits.

<h2>3.4 Splay Trees (skipped)</h2>

<h2>3.5 Skip Lists (skipped)</h2>

<h2>3.6 Java Example: AVL and Red-Black Trees (skipped)</h2>

<h1>Chapter 4 Sorting, Sets, and Selection</h1>

<p>We already did a sorting technique in chapter 2.
Namely we inserted items into a priority queue and then removed the
minimum each time.
When we use a heap to implement the priority, the resulting sort is
called heap-sort and is asymptotically optimal.
That is, its complexity of O(Nlog(N)) is as fast as possible if we
only use comparisons (proved in 4.2 below)

<h2>4.1 Merge-Sort</h2>

<h3>4.1.1 Divide-and-Conquer</h3>

<p>The idea is that if you divide an enemy into small pieces, each
piece, and hence the enemy, can be conquered.
When applied to computer problems <strong>divide-and-conquer</strong>
involves three steps.

<ol>
<li>
    Divide the problem into smaller subproblems.
<li>
    Solve each of the subproblems, normally via a recursive call to
    the original procedure.
<li>
    Combine the subproblem solutions into a solution for the original
    problem.
</ol>

<p>In order to prevent an infinite sequence of recursions, we need to
define a stopping condition, i.e., a predicate that informs us when to
stop dividing (because the problem is small enough to solve directly).

<h4>Using Divide-and-Conquer for Sorting</h4>

<p>This turns out to be so easy that it is perhaps surprising that it
is asymptotically optimal.
The key observation is that merging two sorted lists is fast (the time
is linear in the size of the list).

<p>The steps are

<ol>
<li>
    Divide (with stopping condition):  If S has zero or one element,
    simply return S since it is already sorted.
    Otherwise S has n&ge;2 elements: Move the first &lceil;n/2&rceil;
    elements of S into S<sub>1</sub> and the remaining
    &lfloor;n/2&rfloor; elements into S<sub>2</sub>.
<li>
    Solve recursively: Recursively sort each of the two subsequences.
<li>
    Combine: Merge the two (now sorted) subsequences back into S
</ol>

<p><strong>Example:</strong>: Sort {22, 55, 33, 44, 11}.

<ol>
<li>
    Divide {22, 55, 33, 44, 11} into {22, 55, 33} and {44, 11}
<li>
    Recursively sort {22, 55, 33} and {44, 11} getting {22, 33, 55}
    and {11, 44}
<li>
    Merge {22, 33, 55} and {11, 44} getting {11, 22, 33, 44, 55}
</ol>

<p>Expanding the recursion one level gives.

<ol>
<li>
    Divide {22, 55, 33, 44, 11} into {22, 55, 33} and {44, 11}
<li>
    Recursively sort {22, 55, 33} and {44, 11} getting {22, 33, 55}
    and {11, 44}
    <ol>
    <li>
        Divide {22, 55, 33} into {22, 55} and {33}
    <li>
        Recursively sort {22, 55} and {33} getting {22, 55} and {33}
    <li>
        Merge {22, 55} and {33} getting {22, 33, 55}
    </ol><br>
    <ol>
    <li>
        Divide {44, 11} into {44} and {11}
    <li>
        Recursively sort {44} and {11} getting {44} and {11}
    <li>
        Merge {44} and {11} getting {11, 44}
    </ol>
<li>
    Merge {22, 33, 55} and {11, 44} getting {11, 22, 33, 44, 55}
</ol>

<p>Expanding again gives

<ol>
<li>
    Divide {22, 55, 33, 44, 11} into {22, 55, 33} and {44, 11}
<li>
    Recursively sort {22, 55, 33} and {44, 11} getting {22, 33, 55}
    and {11, 44}
    <ol>
    <li>
        Divide {22, 55, 33} into {22, 55} and {33}
    <li>
        Recursively sort {22, 55} and {33} getting {22, 55} and {33}
        <ol>
        <li>
            Divide {22, 55} into {22} and {55}
        <li>
            Recursively sort {22} and {55} getting {22} and {55}
        <li>
            Merge {22} and {55} getting {22, 55}
        </ol><br>
        <ol>
        <li>
            Do <strong>NOT</strong> divide {33} since it has only one
            element and hence is already sorted
        </ol>
    <li>
        Merge {22, 55} and {33} getting {22, 33, 55}
    </ol><br>
    <ol>
    <li>
        Divide {44, 11} into {44} and {11}
    <li>
        Recursively sort {44} and {11} getting {44} and {11}
        <ol>
        <li>
            Do <strong>NOT</strong> divide {44} since it has only one
            element and hence is already sorted
        </ol><br>
        <ol>
        <li>
            Do <strong>NOT</strong> divide {11} since it has only one
            element and hence is already sorted
        </ol>
    <li>
        Merge {44} and {11} getting {11, 44}
    </ol>
<li>
    Merge {22, 33, 55} and {11, 44} getting {11, 22, 33, 44, 55}
</ol>

<p>Finally there still is one recursion to do so we get.

<ol>
<li>
    Divide {22, 55, 33, 44, 11} into {22, 55, 33} and {44, 11}
<li>
    Recursively sort {22, 55, 33} and {44, 11} getting {22, 33, 55}
    and {11, 44}
    <ol>
    <li>
        Divide {22, 55, 33} into {22, 55} and {33}
    <li>
        Recursively sort {22, 55} and {33} getting {22, 55} and {33}
        <ol>
        <li>
            Divide {22, 55} into {22} and {55}
        <li>
            Recursively sort {22} and {55} getting {22} and {55}
            <ol>
            <li>
                Do <strong>NOT</strong> divide {22} since it has only
                one element and hence is already sorted.
            </ol>
            <ol><br>
            <li>
                Do <strong>NOT</strong> divide {55} since it has only
                one element and hence is already sorted.
            </ol>
        <li>
            Merge {22} and {55} getting {22, 55}
        </ol><br>
        <ol>
        <li>
            Do <strong>NOT</strong> divide {33} since it has only one
            element and hence is already sorted
        </ol>
    <li>
        Merge {22, 55} and {33} getting {22, 33, 55}
    </ol><br>
    <ol>
    <li>
        Divide {44, 11} into {44} and {11}
    <li>
        Recursively sort {44} and {11} getting {44} and {11}
        <ol>
        <li>
            Do <strong>NOT</strong> divide {44} since it has only one
            element and hence is already sorted
        </ol><br>
        <ol>
        <li>
            Do <strong>NOT</strong> divide {11} since it has only one
            element and hence is already sorted
        </ol>
    <li>
        Merge {44} and {11} getting {11, 44}
    </ol>
<li>
    Merge {22, 33, 55} and {11, 44} getting {11, 22, 33, 44, 55}
</ol>

<p>Hopefully there is a better way to describe this action.  How about
the following picture.  The left tree shows the dividing.  The right
shows the result of the merging.

<p><img src="figs/merge-sort.png" align="center">

<p><strong>Definition</strong>: We call the above tree the
<strong>merge-sort-tree</strong>.

<p>In a merge-sort tree the left and right children of a node with A
elements have &lceil;A/2&rceil; and &lfloor;A/2&rfloor; elements
respectively.

<p><strong>Theorem</strong>: Let n be the size of the sequence to be
sorted.  If n is a power of 2, say n=2<sup>k</sup>, then the height of
the merge-sort tree is log(n)=k.  In general n = &lceil;log(n)&rceil;.

<P><strong>Proof</strong>: The power of 2 case is part of problem set
4.  I will do n=2<sup>k</sup>+1.  When we divide the sequence into 2, the
larger is &lceil;n/2&rceil;=2<sup>k-1</sup>+1.  When we keep dividing
and look at the larger piece (i.e., go down the leftmost branch of the
tree) we will eventually get to 3=2<sup>1</sup>+1.  We have divided by
2 k-1 times so are at depth k-1.  But we have to go two more times
(getting 2 and then 1) to get one element.  So the depth is of this
element is k+1 and hence the height is at least k+1.

<p>The rest is part of problem set 2.  Here is the idea.  If you
increase the number of elements in the root, you do not decrease the
height.  If n is not a power of 2, it is between two powers of 2 and
hence the height is between the heights for these powers of two.

<p><strong>Problem Set</strong> 4, problem 1.
Prove the theorem when n is a power of 2.
Prove the theorem for n not a power of 2 either by finishing my
argument or coming up with a new one.

<p>The reason we need the theorem is that we will show that merge-sort
spends time O(n) at each level of the tree and hence spends time
O(n*height) in total.  The theorem shows that this is O(nlog(n)).

<h4>Merging Two Sorted Sequences</h4>

<p>This is quite clear.  View each sequence as a deck of cards face up.
Look at the top of each deck and take the smaller card.  Keep going.
When one deck runs out take all the cards from the other.
This is clearly constant time per card and hence linear in the number
of cards &Theta;(#cards).

<p>Unfortunately this doesn't look quite so clear in pseudo code when
we write it in terms of the ADT for sequences.  Here is the algorithm
essentially copied from the book.  At least the comments are clear.

<pre style="font-size:larger">
Algorithm merge(S1, S2, S):
   Input:  Sequences S1 and S2 sorted in nondecreasing order,
           and an empty sequence S
   Output: Sequence S contains the elements previously in S1 and S2
           sorted in nondecreasing order.  S1 and S2 now empty.

   {Keep taking smaller first  element until one sequence is empty}
   while (not(S1.isEmpty() or S2.isEmpty()) do
     if S1.first().element()&lt;S2.first().element() then
        {move first element of S1 to end of S}
        S.insertLast(S1.remove(S1.first())
     else
        {move first element of S2 to end of S}
        S.insertLast(S2.remove(S2.first())

   {Now take the rest of the nonempty sequence.}
   {We simply take the rest of each sequence.}
   {Move the remaining elements of S1 to S
   while (not S1.isEmpty()) do
      S.insertLast(S1.remove(S1.first())
   {Move the remaining elements of S2 to S
   while (not S2.isEmpty()) do
      S.insertLast(S2.remove(S2.first())
</pre>

<p>Examining the code we see that each iteration of each loop removes
an element from either S1 or S2.  Hence the total number of iterations
is S1.size()+S2.size().  Since each iteration requires constant time
we get the following theorem.

<P><strong>Theorem</strong>: Merging two sorted sequences takes time
&Theta;(n+m), where n and m are the sizes of the two sequences.

<a id="merge-sort-analysis">
  <h4>The Running Time of Merge-Sort</h4>
</a>

<p>We characterize the time in terms of the merge-sort tree.  We
assign to each node of the tree the time to do the divide and merge
associated with that node and to invoke (but not to execute) the
recursive calls.  So we are essentially charging the node for the
divide and combine steps, but not the solve recursively.

<p>This does indeed account for all the time.  Illustrate this with
the example tree I drew at the beginning.

<p>For the remainder of the analysis we assume that the size of the
sequence to be sorted is n, which is a power of 2.  It is easy to
extend this to arbitrary n as we did for the theorem that is a part of
problem set 4.

<p><img src="figs/merge-sort-anal.png" align="right">
Now much time is charged to the root?  The divide step takes time
proportional to the number of elements in S, which is n.  The combine
step takes time proportional to the sum of the number of elements in
S1 and the number of elements in S1.  But this is again n.  So the
root node is charged &Theta;(n).

<p>The same argument shows that any node is charged &Theta;(A), where
A is the number of items in the node.

<p>How much time is charged to a child of the root?
Remember that we are assuming n is a power of 2.
So each child has n/2 elements and is charged a constant times n/2.
Since there are two children the entire level is charged &Theta;(n).

<p>In this way we see that each level is &Theta;(n).
Another way to see this is that the total number of elements in a
level is always n and a constant amount of work is done on each.

<p>Now we use the theorem saying that the height is log(n) to conclude
that the total time is &Theta;(nlog(n)).  So we have the following
theorem.

<p><strong>Theorem</strong>: Merge-sort runs in &Theta;(nlog(n)).

<h3>4.1.2 Merge-Sort and Recurrence Equations</h3>

<p>Here is yet another way to see that the complexity is &Theta;(nlog(n)).

<p>Let t(n) be the worst-case running time for merge-sort on n
elements.

<p><strong>Remark</strong>: The worst case and the best case just
differ by a multiplicative constant.  There is no especially easy or
hard case for merge-sort.

<p>For simplicity assume n is a power of two.  Then we have
for some constants B and C

<pre>
t(1) = B
t(n) = 2t(n/2)+Cn    if n&gt;1
</pre>

The first line is obvious the second just notes that to solve a
problem we must solve 2 half-sized problems and then do work (merge)
that is proportional to the size of the original problem.

<p>If we apply the second line to itself we get
<br><tt>
t(n) = 2t(n/2)+Cn = 2[2t(n/4)+C(n/2)]+Cn =
       2<sup>2</sup>t(n/2<sup>2</sup>)+2Cn
</tt>

<p>If we apply this i times we get
<pre>
t(n) = 2<sup>i</sup>t(n/2<sup>i</sup>)+iCn
</pre>

<p>When should we stop?
<br>Ans: when we get to the base case t(1).  This occurs when
2<sup>i</sup>=n, i.e. when i=log(n).

<pre>
t(n)   =  2<sup>log(n)</sup>t(n/2<sup>log(n)</sup>) + log(n)Cn
       =  nt(n/n) + log(n)Cn               since 2<sup>log(n)</sup>=n
       =  nt(1) + log(n)Cn
       =  nB + Cnlog(n)
</pre>

which again shows that t(n) is &Theta;(nlog(n)).

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #22 ================
</div></big></strong>

<h2>4.2 The Set Abstract Data Type</h2>

<p>Skipped for now

<h3>4.2.1 A Simple Set Implementation</h3>

<h3>4.2.2 Partitions with Union-Find Operations</h3>

<h3>4.2.3 A Tree-Based Partition Implementation</h3>

<h2>4.3 Quick-Sort</h2>

<p>It is interesting to compare quick-sort with merge-sort.
Both are divide and conquer algorithms.  So we divide, recursively
sort each piece, and then combine the sorted pieces.

<p>In merge-sort, the divide is trivial: throw half the elements into
one pile and the other half in another pile.  The combine step, while
easy does do comparisons and picks an element from the correct pile.

<p>In quick-sort, the combine is trivial: pick up one pile, then the
other.
The divide uses comparisons to decide which pile each element should
be placed into.

<p>As usual we assume that the sequence we wish to sort contains no
duplicates.  It is easy to drop this condition if desired.

<p><img src="figs/quick-sort.png" align="right">
<pre style="font-size:larger">
Algorithm quick-sort (S)
   Input: A sequence S (of size N).
   Output: A sorted sequence T containing
           the same elements as S.

   Create a empty sequences T, L, G

   { Divide into L and G }
   If n = 1
      then copy the element of S to T
   if n < 2
      return
   Pick an element P from S { called the pivot }
   while (not S.isEmpty())
      x &larr; S.remove(S.first())
      if x < P then
         L.insertLast(x)    { L stands for less }
      if x > P then
         G.insertLast(x)    { G stands for greater }

   { Recursively Sort L and G }
   LS &larr; quick-sort (L)    LS stands for L sorted }
   GS &larr; quick-sort (G)

   { Combine LS, P, and GS }
   while (not LS.isEmpty())
      T.insertLast(LS.remove(LS.first()))
   T.insertLast(P)
   while (not GS.isEmpty())
      T.insertLast(GS.remove(GS.first()))
</pre>

<h4>Running Time of Quick-Sort</h4>
<p><img src="figs/quick-sort-tree.png" align="right">

The running time of quick sort is highly dependent on the choice of
the pivots at each stage of the recursion.
A very simple method is to choose the last element as the pivot.
This method is illustrated in the figure on the right.  The pivot is
shown in red.  This tree is not surprisingly called the
quick-sort tree.

<p>The top tree shows the dividing and recursing that occurs with input
{33,55,77,11,66,88,22,44}.
The tree below shows the combining steps for the same input.

<p>As with merge sort, we assign to each node of the tree the cost
(i.e., running time) of the divide and combine steps.  We also assign
to the node the cost of the two recursive calls, but not their
execution.  How large are these costs?

<p>The two recursive calls (not including the subroutine execution
itself) are trivial and cost &Theta;(1).

<p>The dividing phase is a simple loop whose running time is linear in
the number of elements divided, i.e., in the size of the input
sequence to the node.  In the diagram this is the number of numbers
inside the oval.

<p>Similarly, the combining phase just does a constant amount of work
per element and hence is again proportional to the number of elements
in the node.

<p>We would like to make an argument something like this.  At each
level of each of the trees the total number of elements is n so the
cost per level is O(n).  The pivot divides the list in half so the
size of the largest node is divided by two each level.  Hence the
number of levels, i.e., the height of the tree, is O(log(n)).  Hence
the entire running time is O(nlog(n)).

<p>That argument sound pretty good and perhaps we should try to make
it more formal.  However, I prefer to try something else since the
argument is <strong>WRONG!</strong>

<p><strong>Homework:</strong>
Draw the quick-sort
tree for sorting the following sequence
{222 55 88 99 77 444 11 44 22 33 66 111 333}.
Assume the pivot is always the last element.

<br clear="right">
<h4>The Worst Case for Quick-Sort</h4>
<p><img src="figs/quick-sort-worst.png" align="right">

The tree on the right illustrates the worst case of quick-sort, which
occurs when the input is already sorted!

<p>The height of the tree is N-1 not O(log(n)).  This is because the
pivot is in this case the largest element and hence does not come
close to dividing the input into two pieces each about half the
input size.

<p>It is easy to see that we have the worst case.  Since the pivot
does not appear in the children, at least one element from level i
does not appear in level i+1 so at level N-1 you can have at most 1
element left.  So we have the highest tree possible.
Note also that level i has at least i pivots missing so cna have at
most N-i elements in all the nodes.  Our tree achieves this maximum.
So the time needed is proportional to the total number of numbers
written in the diagram which is N + N-1 + N-2 + ... + 1, which is
again the one summation we know N(N+1)/2 or &Theta;(N<sup>2</sup>.

<p>Hence the <strong>worst case</strong> complexity of quick-sort is
quadratic!  Why don't we call it slow sort?

<p>Perhaps the problem was in choosing the last element as the pivot.
Clearly choosing the first element is no better; the same example on
the right again illustrates the worst case (the tree has its empty
nodes on the left this time).

<p>Since are spending linear time (as opposed to constant time) on the
division step, why not count how many elements are present (say k) and
choose element number k/2?  This would not change the complexity (it
is also linear).  You could do that and now a sorted list is not the
worst case.  But some other list is.  Just put the largest element in
the middle and then put the second largest element in the middle of
the node on level 1.  This does have the advantage that if you
mistakenly run quick-sort on a sorted list, you won't hit the worst
case.  But the worst case is still there and it is still
&Theta;(N<sup>2</sup>).

<p>Why not choose the real middle element as the pivot, i.e., the
median.  That would work!  It would cut the sizes in half as desired.
But how do we find the median?  We could sort, but that is the
original problem.  In fact there is a (difficult) algorithm for
computing the median in linear time and if this is used for the pivot,
quick-sort does take O(nlog(n)) time in the worst case.  However, the
difficult median algorithm is not fast in practice.  That is, the
constants hidden in saying it is &Theta;(N) are rather large.

<p>Instead of studying the fast, difficult median algorithm, we will
consider a randomized quick-sort algorithm and show that the
<strong>expected</strong> running time is &Theta;(Nlog(N)).

<p><strong>Problem Set</strong> 4, Problem 2.
Find a sequence of size N=12 giving the worst case for quick-sort when
the pivot for sorting k elements is element number &lfloor;k/2&rfloor;.

<br clear="right">
<h3>4.3.1 Randomized Quick-Sort</h3>

<p>Consider running the following quick-sort-like experiment.
<ol>
<li>
    Pick a positive integer N
<li>
    Pick a N numbers at random (say without duplicates).
<li>
    Choose at random one of the N numbers and call it the pivot.
<li>
    Split the other numbers into two piles: those bigger than the
    pivot and those smaller than the pivot.
<li>
    Call this a good split if the two piles are "nearly" equal.
    Specifically, the split is good if the larger of the piles has no
    more than 3N/4 elements (which is the same as the smaller pile
    having no fewer than N/4 elements)
</ol>

<p>Are good splits rare or common?

<p><strong>Theorem</strong>: (From probability theory).  The expected
number of times that a fair coin must be flipped until it shows
``heads'' k times is 2k.

<p>We will not prove this theorem, but will apply it to analyze good
splits.

<img src="figs/quick-sort-random.png" align="right">
<p>We picked the pivot at random so if we <strong>imagine</strong> the
N numbers lined up in order, the pivot is equally likely to be
anywhere in this line.

<p>Consider the picture on the right.
If the pivot is anywhere in the pink, the split is good.
But the pink is half the line so the probability that we get
a ``pink pivot'' (i.e., a good split) is 1/2.
This is the same probability that a fair coin comes up heads.

<p>Every good split divides the size of the node by at least 4/3.
Recall that if you divide N by 4/3, log<sub>4/3</sub>(N) times, you
will get 1.  So the maximum number of good splits possible along a
path from the root to a leaf is log<sub>4/3</sub>(N).

<p>Apply the probability theorem above we see that the expected length
of a path from the root to a leaf is at most 2log<sub>4/3</sub>(N),
which is O(log(N)).  That is, the expected height is O(log(N)).

<p>Since the time spent at each level is O(N), the
<strong>expected</strong> running time of randomized quick-sort is O(Nlog(N)).

<h2>4.4 A Lower Bound on Comparison-Based Sorting</h2>

<p><strong>Theorem</strong>: The running time of any comparison-based
sorting algorithm is &Omega;(Nlog(N)).

<p><strong>Proof</strong>: We will not cover this officially.
Unofficially the idea we form a binary tree with each node
corresponding to a comparison and the two children corresponding to
the two possible outcomes.  This is a tree of all possible executions
(with only comparisons used for decisions).  There are N! permutations
of N numbers and each must give a different execution pattern in order
to be sorted.  So there are at least N! leaves.  Hence the height is
at least log(N!).  But N! has N/2 elements that are at least N/2 so
N!&ge;(N/2)<sup>N/2</sup>.  Hence
<br>height &ge; log(N!) &ge; log((N/2)<sup>N/2</sup>) = (N/2)log(N/2)
<br>So the running time, which is at least the height of this tree, is
&Omega;(Nlog(N))

<p><strong>Corollary</strong>: Heap-sort, merge-sort, and quick sort (with the
difficult, linear-time median algorithm) are asymptotically optimal.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #23 ================
</div></big></strong>

<h2>4.5 Bucket-Sort and Radix-Sort</h2>

<p>We have seen that the fastest comparison-based sorting algorithms
run in time &Theta;(Nlog(N)), where N is the number of items to sort.
In this section we are going to develop faster algorithms.  Hence they
must not be comparison-based algorithms.

<p>We make a key assumption, we are sorting items whose keys are
integers in a bounded range [0, R-1].

<p><strong>Question</strong>: Let's start with a special case R=N so
we are sorting N items with integer keys from 0 to N-1.  As usual
we assume there are no duplicate keys.  Also as usual we remark that
it is easy to lift this restriction.  How should we do this sort?

<p><strong>Answer</strong>: That was a trick question.  You don't even
have to look at the input.  If you tell me to sort 10 integers in the
range 0...9 and there are no duplicates, I know the answer is
{0,1,2,3,4,5,6,7,8,9}.  Why?  Because, if there are no duplicates, the
input must consist of one copy of each integer from 0 to 10.

<h3>4.5.1 Bucket-sort</h3>

<p>OK, let's drop the assumption that R=N.  So we have N items (k,e),
with each k an integer, no duplicate ks, and 0&le;k&lt;R.  The trick is
that we can use k to decide where to (temporarily) store e.

<img src="figs/pre-bucket-sort.png" align="right">
<pre style="font-size:larger">
Algorithm preBucketSort(S)
   input:  A sequence S of N items with integer keys in range [0,N)
   output: Sequence S sorted in increasing order of the keys.

   let B be a vector of R elements,
       each initially a special marker indicating empty

   while (not S.isEmpty())
      (k,e) &larr; S.remove(S.first())
      B[k] &larr; e         <==== the key idea (not a comparison)

   for i &larr; 0 to R-1 do
      if (B[i] not special marker) then
         S.insertLast((i,B[i])
</pre>

<p>To convert this algorithm into bucket sort we drop the artificial
assumption that there are not duplicates.  Now instead of a vector of
items we need a vector of buckets, where a bucket is a sequence of
items.

<img src="figs/bucket-sort.png" align="right">
<pre style="font-size:larger">
Algorithm BucketSort(S)
   input:  A sequence S of N items with integer keys in range [0,N)
   output: Sequence S sorted in increasing order of the keys.

   let B be a vector of R sequences of items
       each initially  empty

   while (not S.isEmpty())
      (k,e) &larr; S.remove(S.first())
      B[k].insertLast(e)      <==== the key idea

   for i &larr; 0 to R-1 do
      while (not B[i].isEmpty())
         S.insertLast((i,B[i].remove(B[i].first())))
</pre>

<h4>Complexity of Bucket-Sort</h4>

<p>The first loop has N iterations each of which run in time
&Theta;(1), so the loop requires time &Theta;(N).
The for loop has R iterations so the while statement is
executed R times.  Each while statement requires time &Theta;(1)
(excluding the body of the while) so all of them require time &Theta;(N)
The <strong>total</strong> number of
iterations of all the inner while loops is again N and each again
requires time &Theta;(1), so the time for all
inner iterations is &Theta;(N).

<p>The previous paragraph shows that the complexity is
&Theta;(N)+&Theta;(R) = &Theta;(N+R).

<p>So bucket-sort is a winner if R is not too big.  For example if
R=O(N), then bucket-sort requires time only &Theta;(N).  Indeed if
R=o(Nlog(N)), bucket-sort is (asymptotically) faster than any
comparison based sorting algorithm (using worst case analysis).

<h4>Stable Sorting</h4>

<p><strong>Definition</strong>: We call a sort <strong>stable</strong>
if equal elements remain in the same relative position.  Stated more
formally: for any two items (k<sub>i</sub>,e<sub>i</sub>) and
(k<sub>j</sub>,e<sub>j</sub>) such that item
(k<sub>i</sub>,e<sub>i</sub>) precedes item
(k<sub>j</sub>,e<sub>j</sub>) in S (i.e., i&lt;j), then item
(k<sub>i</sub>,e<sub>i</sub>) precedes item
(k<sub>j</sub>,e<sub>j</sub>) after sorting as well.

<p>Stability is often convenient and will prove so in the next section
on radix-sort.  We note that bucket-sort is stable since we treated
each bucket in a fifo manner inserting at the rear and removing from
the front.

<h3>4.5.2 Radix-Sort</h3>

<p>Let's extend our sorting study from integer keys to keys that are
themselves pairs of integers.  The first question to ask is, given two
keys (k,m) and (k',m'), which is larger?  Note that (k,m) is just the
<strong>key</strong>; an item would be written ((k,m),e).

<p><strong>Definition</strong>:
The <strong>lexicographical</strong> (dictionary) ordering on pairs of
integers is defined by declaring (k,m)&nbsp;&lt;&nbsp;(k',m') if
either
<ul>
<li>k &lt; k' or
<li>k = k' and m &lt; m'
</ul>

<p>Note that this really is dictionary order:
<br>canary &lt; eagle &lt; egret &lt heron

<pre>
Algorithm radix-sort-on-pairs
   input:  A sequence S of N items with keys
           pairs of integers in the range [0,N)
           Write elements of S as ((k,m),e)
   output: Sequence S lexicographically sorted on the keys

   bucket-sort(S) using m as the key
   bucket-sort(S) using k as the key
</pre>

<p>Do an example of radix sorting on pairs and then do it incorrectly,
by reversing the order of the bucket sorts.

<p>What if the keys are triples or in general d-tuples?
<br>The answer is ...

<p><strong>Homework:</strong> R-4.15

<p><strong>Theorem</strong>: Let S be a sequence of N items each of
which has a key (k<sub>1</sub>,k<sub>2</sub>,...k<sub>d</sub>), where
k<sub>i</sub> is in integer in the range [0,R).  We can sort S
lexicographically in time O(n(N+R)) using radix-sort.

<h2>4.6 Comparison of Sorting Algorithms</h2>

<p>Insertion sort or bubble sort are not suitable for general sorting
of large problems because their running time is quadratic in N, the
number of items.  For small problems, when time is not an issue, these
are attractive because they are so simple.
Also if the input is almost sorted, insertion sort is fast since it
can be implemented in a way that is O(N+A), where A is the number of
<strong>inversions</strong>, (i.e., the number of pairs out of order).

<p>Heap-sort is a fine general-purpose sort with complexity
&Theta;(Nlog(N)), which is optimal for comparison-based sorting.
Also heap-sort can be executed in place (i.e., without much extra
memory beyond the data to be sorted).  (The coverage of in-place
sorting was ``unofficial'' in this course.)  If the in-place version
of heap-sort fits in memory (i.e., if the data is less than the size
of memory), heap-sort is very good.

<p>Merge-sort is another optimal &Theta;(Nlog(N)) sort.  It is not
easy to do in place so is inferior for problems that can fit in
memory.  However, it is quite good when the problem must be do
``out-of-core'' since it doesn't fit.  We didn't discuss this issue,
but the merges can be done with two input and one output file (this is
not trivial to do well, you want to utilize the available memory in
the most efficient manner).

<p>Quick-sort is hard to evaluate.  The version with the fast median
algorithm is fine theoretically (worst case again &Theta;(Nlog(N)) but
not used because of large constant factors in the fast median.
Randomized quick-sort has a low expected time but a poor worst-case
time.  It can be done in place and is quite fast in that case, often
the fastest.  But the quadratic worst case is a fear (and a
non-starter for many real-time applications).

<p>Bucket and radix sort are wonderful when they apply, i.e., when the
keys are integers in a modest range (R a small multiple of N).  For
radix sort with d-tuples the complexity is &Theta;(d(N+R)) so if
d(N+R) is o(Nlog(N)), radix sort is asymptotically faster than
<strong>any</strong> comparison based sort (e.g., heap-, insertion-,
merge-, or quick-sort).

<h2>4.7 Selection (officially skipped; unofficial comments follow)</h2>

<p>Selection means the ability to find the kth smallest element.
Sorting will do it, but there are faster (comparison-based) methods.
One example problem is finding the median (N/2 th smallest).

<p>It is not too hard (but not easy) to implement selection with
linear expected time.  The surprising and difficult result is that
there is a version with linear worst-case time.

<h3>4.7.1 Prune-and-Search</h3>

<h3>4.7.2 Randomized Quick-Select</h3>

<h3>4.7.3 Analyzing Randomized Quick-Select</h3>

<h2>4.8 Java Example: In-Place Quick-Sort (skipped)</h2>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #24 ================
</div></big></strong>

<h1>Chapter 5 Fundamental Techniques</h1>

<h2>5.1 The Greedy Method</h2>

<p>The greedy method is applied to maximization/minimization problems.
The idea is to at each decision point choose the configuration that
maximizes/minimizes the objective function so far.
Clearly this does not lead to the global max/min for all problems, but
it does for a number of problems.

<p>This chapter does not make a good case for the greedy method.
since it is used to solve simple cases of standard problems in which
the more normal cases do not use the greedy method.  However, there
are better examples, for example minimal the spanning tree and
shortest path graph problems.  The two algorithms chosen for this
section, fractional knapsack and task scheduling, were (presumably)
chosen because they are simple and natural to solve with the greedy
method.

<h3>5.1.1 The Fractional Knapsack Method</h3>

<p>In the knapsack problem we have a knapsack of a fixed capacity (say
W pounds) and different items i each with a given weight wi and a
given benefit bi.  We want to put items into the knapsack so as to
maximize the benefit subject to the constraint that the sum of the
weights must be less than W.

<p>The knapsack problem is actually rather difficult in the normal
case where one must either put an item in the knapsack or not.
However, in this section, in order to illustrate greedy algorithms, we
consider a much simpler variation in which we can take part of an item
and get a proportional part of the benefit.  This is called the
``fractional knapsack problem'' since we can take a fraction of an
item.  (The more common knapsack problem is called the ``0-1 knapsack
problem'' since we either take all (1) or none (0) of an item.

<p>In symbols, for each item i we choose an amount xi
(0&le;xi&le;wi)
that we will place in the knapsack.
We are subject to the constraint that the sum of the xi is no more
than W since that is all the knapsack can hold.

<p>We again desire to maximize the total benefit.  Since, for item i,
we only put xi in the knapsack, we don't get the full benefit.
Specifically we get benefit (xi/wi)bi.

<p>But now this is easy!
<ul>
<li>
    Item i has benefit bi and weighs wi.
<li>
    So its value (per pound) is vi=bi/wi.
<li>
    We make the greedy choice and pick the most valuable item and take
    all of it or as much as the knapsack can hold.
<li>
    Then we move to the second most valuable and do the same.
<li>
    This clearly is optimal since it never makes sense to leave over
    some valuable item to take some less valuable item.
</ul>

<p>Why doesn't this work for the normal knapsack problem when we must
take all of an item or none of it?
<ul>
<li>
    Example: W=6, w1=4, w2=w3=3, b1=5, b2=b3=3.
<li>
    So v1=5/4, v2=v3=1
<li>
    Start with the most valuable item, number 1 and put it n the
    knapsack.
<li>
    Knapsack can hold 7-4=2 more "pounds" but remaining items won't
    fit
<li>
    So the total benefit carried is 5.
<li>
    The right solution is to take items 2 and 3 for a total benefit of
    6.
<li>
    The difference between this item and fractional knapsack is that
    the knapsack can still hold 2/3 of item 2,
    but we can't take part of an item as we can in the fractional
    knapsack problem.
</ul>

<h4>Algorithm</h4>

<pre style="font-size:larger">
algorithm FractionalKnapsack(S,W):
   Input:  Set S of items i with weight wi and benefit bi all positive.
           Knapsack capacity W&gt;0.
   Output: Amount xi of i that maximizes the total benefit without
           exceeding the capacity.

   for each i in S do
      xi &larr; 0        { for items not chosen in next phase }
      vi &larr; bi/wi    { the value of item i "per pound" }
   w &larr; W            { remaining capacity in knapsack }

   while w > 0 do
      remove from S an item of maximal value   { greedy choice }
      xi &larr; min(wi,w)  { can't carry more than w more }
      w &larr; w-xi
</pre>

<h4>Analysis</h4>

<p>FractionalKnapsack has time complexity O(NlogN) where N is
the number of items in S.
<ul>
<li>
    The book suggests assuming S is a heap-based priority queue and
    then the removal has complexity &Theta;(logN) so the up to N
    removals take O(NlogN).  The rest of the algorithm is O(N).
<li>
    Alternatively, S could be a sequence and we could begin
    FractionalKnapsack by sorting S with a &Theta;(NlogN) sort.  Now
    the removal is simply removing the first element.  If we use a
    circular list for S, the removal is O(1) so the algorithm is
    O(N).  Including the sort we again have O(NlogN).
</ul>

<P><strong>Homework:</strong> R-5.1

<h3>5.1.2 Task Scheduling</h3>
<img src="figs/scheduling.png" align="right">

<p>We again consider an easy case of a well known optimization
problem.
<ul>
<li>
    We have a set T of N <strong>tasks</strong>, each with a
    <strong>start time</strong> si and a <strong>finishing
    time</strong> fi (si&lt;fi).
<li>
    Each task must start at time si and will finish at time fi.
<li>
    Each task is executed on a machine Mj.
<li>
    A machine can execute only one task at a time, but can start a
    task at the same time as the current task ends (red lines on the
    figure to the right).
<li>
    Tasks are <strong>non-conflicting</strong> if they do not
    conflict, i.e., if fi&le;sj or fj&le;si.
    For example, think of tasks as classes.
<li>
    The problem is to schedule all the tasks in T using the minimal
    number of machines.
</ul>

<p> In the figure there are 6 tasks, with start times and finishing
times (1,3), (2,5), (2,6), (4,5), (5,8), (5,7).  They are scheduled on
three machines M1, M2, M3.  Clearly 3 machines are needed as can be
seen by looking at time 4.

<p>In our greedy algorithm we only go to a new machine when we find a
task that cannot be scheduled on the current machines.
<ol>
<li>
    Why is this called greedy?
<li>
    We are trying to minimize so here greedy means stingy.
<li>
    Like the previous greedy algorithm we are making decisions that
    are clearly locally optimal, and in this case is globally optimal
    as well.
</ol>

<h4>Algorithm</h4>

<pre style="font-size:larger">
Algorithm TaskSchedule(T):
   Input:  A set T of tasks, each with start time si and finishing time fi
           (si&le;fi).
   Output: A schedule of the tasks of T on the minimum number of machines.

   m &larr; 0                                   { current number of machines }
   while T is not empty do
      remove from T a task i with smallest start time
      if there is an Mj having all tasks non-conflicting with i then
         schedule i on Mj
      else
         m &larr; m+1
         schedule i on Mm
</pre>

<h4>Correctness (i.e. Minimality of m)</h4>

<p>Assume the algorithm runs and declares m to be the minimum number
of machines needed.  We must show that m are really needed.
<ul>
<li>
    Consider the step when the algorithm increases m to its final
    value and assume the task under consideration is i.
<li>
    At this point the current task conflicts with one (or more)
    task(s) in each of the m-1 machines currently used.
<li>
    But all these tasks have start time no later than si since the
    tasks were processed in order of their start time.
<li>
    Since they conflict with i, they each have finishing time after
    si.
<li>
    Hence they all conflict with each other as well (consider time
    si).
<li>
    Hence we really do need m machines.
</ul>

<h4>Complexity</h4>

<p>The book asserts that it is easy to see that the algorithm runs in
time O(NlogN), but I don't think this is so easy.  It is easy to see
O(N<sup>2</sup>).
<ul>
<li>
    The while loop has n iterations.
<li>
    You just need to compare the current task with all previous tasks,
    which shows that the iteration is O(N) and the algorithm is
    O(N<sup>2</sup>).
<li>
    To get O(log(N)) for each iteration, keep the <strong>machines</strong>
    in a heap using as key the latest finishing time assigned to that
    machine.  This tells you when that machine will be free (remember
    that all tasks assigned so far start no later than si, the current
    job's start time).
<li>
    Check the min element of the tree.  If it is free at si, then it
    is free forever starting at si.  We
    <ol>
    <li>
        Remove the machine from the heap (removeMin).
    <li>
        Assign the current job to the removed machine.
    <li>
        Now this machine is free at fi and we re-insert it into the heap.
    </ol>
<li>
    If it is not free at si, then no machine is free at si so
    <ol>
    <li>
        Increase m generating a new machine.
    <li>
        Assign i to the new machine m
    <li>
        Insert machine m (which has key fi) into the heap.
    </ol>
</ul>

<P><strong>Homework:</strong> R-5.3

<p><strong>Problem Set</strong> 4, Problem 3.
<br>Part A. C-5.3 (Do not argue why your algorithm is correct).
<br>Part B. C-5.4.

<p><strong>Remark</strong>: Problem set 4 is now assigned and due
4 Dec 02.

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #25 ================
</div></big></strong>

<h2>5.2 Divide-and-Conquer</h2>

<p>The idea of divide and conquer is that we solve a large problem by
solving a number of smaller problems and then we apply this idea
recursively.

<ul>
<li>
    A common example would be that to solve a single problem of size N
    we need to solve two problems each of size approximately N/2.
<br><br><li>
    When we apply the recursion again, we wind up with four problems
    each of size approximately N/4.
<br><br><li>
    In addition to the time required to solve the subproblems, we need
    to include the time to split the original problem into pieces and
    the time required to combine the solutions to the subproblems into
    a solution for the original problem. 
</ul>

<h3>5.2.1 Divide-and-Conquer Recurrence Equations</h3>

<p>From the description above we see that the complexity of a divide
and conquer solution has three parts.
<ol>
<li>
    The time required to split the problem into subproblems.
<li>
    The time required to solve all the subproblems.
<li>
    The time required to combine the subproblem solutions
</ol>

<p>Let T(N) be the time required to solve an instance of the
problem having time N.  The time required to split the problem and
combine the subproblems is typically a function of N, say f(N).

<p>More interesting is the time required to solve the subproblems.  If
the problem has been split in half then the time required for each
subproblem is T(N/2).

<p>Since the total time required includes splitting, solving both
subproblems, and combining we get.

<pre style="font-size:larger">
    T(N) = 2T(N/2)+f(N)
</pre>

<p>Very often the splitting and combining are fast, specifically
linear in N.  Then we get

<pre style="font-size:larger">
    T(N) = 2T(N/2)+bN
</pre>

for some constant b.  (We probably should say &le; rather than =, but
we will soon be using big-Oh and friends so we can afford to be a
little sloppy.
<p>What if N is not divisible by 2?  We should be using floor or
ceiling or something, but we won't.  Instead we will be assuming for
recurrences like this one that N is a power of two so that we can keep
dividing by 2 and get an integer.  (The general case is not more
difficult; but is more tedious)

<p>But that is crazy!  There is <strong>no</strong> integer that can
be divided by 2 forever and still give an integer!  At some point we
will get to 1, the so called base case.  But when N=1, the problem is
almost always trivial and has a O(1) solution.  So we write either

<pre style="font-size:larger">
           b              if N = 1
    T(N) = 
           2T(N/2)+bN   if N &gt; 1
</pre>

or

<pre style="font-size:larger">
    T(1) = b
    T(N) = 2T(N/2)+bN   if N &gt; 1
</pre>

<p>No we will now see three techniques that, when cleverly applied,
can solve a number of problems.  We will also see a theorem that, when
its conditions are met, gives a solution without our being clever.

<h4>The Iterative Substitution Method</h4>

<p>Also called ``plug and chug'' since we plug the equation into
itself and chug along.

<pre style="font-size:larger">
    T(N) = 2T(N/2)+bN
         = 2[    ]+bN
         = 2[2T((N/2)/2)+b(N/2)]+bN
         = 4T(N/4)+2bN    now do it again
         = 8T(N/8)+3bN
</pre>

<p>A flash of inspiration is now needed.  When the smoke clears we get

<pre style="font-size:larger">
    T(N) = 2<sup>i</sup>T(N/2<sup>i</sup>)+ibN
</pre>

<p>When i=log(N), N/2<sup>i</sup>=1 and we have the base case.  This
gives the final result

<pre style="font-size:larger">
    T(N) = 2<sup>log(N)</sup>T(N/2<sup>log(N)</sup>)+ibN
         = N    T(1)      +log(N)bN
         = Nb +log(N)bN
</pre>

Hence T(N) is O(Nlog(N))

<h4>The Recursion Tree</h4>
<img src="figs/merge-sort-anal.png" align="right">

<p>The idea is similar but we use a visual approach.

<p>We already studied the recursion tree when we analyzed merge-sort.
Let's look at it again.

<p>The diagram shows the various subproblems that are executed, with
their sizes.
(It is not always true that we can divide the problem so evenly as shown.)
Then we show that the splitting and combining that occur at each node
(plus calling the recursive routine) only take time linear in the
number of elements.  For each level of the tree the number of elements
is N so the time for all the nodes on that level is &Theta;(N) and we
need to find the height of the tree.
When the tree is split so evenly the sizes of all the nodes on each
level go down by a factor of two so we reach a node with size 1 in
logN levels (assuming N is a power of 2).
Thus T(N) is &Theta;(Nlog(N)).


<h4>The Guess and Test Method</h4>

<p>This method is really only useful <strong>after</strong>
you have practice in recurrences.  The idea is that, when confronted
with a new problem, you recognize that it is similar to a problem you
have seen the solution to previously and you guess that the solution
to the new problem is similar to the old.

<p>But we don't have enough experience for this to be very useful.

<h4>The Master Method</h4>

<p>In this section, we apply the heavy artillery.  The following
theorem, which we will not prove, enables us to solve some problems by
just plugging in.

<p>We will only be considering complexities of the form

<pre style="font-size:larger">
    T(1) = c
    T(N) = aT(N/b)+f(N)   if N &gt; 1
</pre>

<p>The idea is that we have done some sort of divide and conquer where
there are a subproblems of size at most N/b.  As mentioned earlier
f(N) accounts for the time to divide the problem into subproblems and
to combine the subproblem solutions.
<html>
<p><strong>Theorem</strong> [The Master Theorem]: Let f(N) and T(N) be
as above.
<ol>
<li>
    (f is small) If there is a constant &epsilon;&gt;0 such that f(N)
    is <tt style="font-size:larger">
    O(N<sup>log<sub>b</sub>a</sup>/n<sup>&epsilon;</sup>)</tt>, then
    T(N) is
    <tt style="font-size:larger">&Theta;(N<sup>log<sub>b</sub>a</sup>)</tt>.
<li>
    (f is medium) If there is a constant k&ge;0 such that f(N) is
    <tt style="font-size:larger">
    &theta;(N<sup>log<sub>b</sub>a</sup>(logN)<sup>k</sup>)</tt>,
    then T(N) is <tt style="font-size:larger">
    &theta;(N<sup>log<sub>b</sub>a</sup>(logN)<sup>k+1</sup>)</tt>.
<li>
    (f is large) If there are constants &epsilon;&gt;0 and &delta&lt;1
    such that f(N) is <tt style="font-size:larger">
    &Omega;(N<sup>log<sub>b</sub>a</sup>N<sup>&epsilon;</sup>)</tt> and
    af(N/b)&le;&delta;f(N), then T(N) is &Theta;(f(N)).
</ol>

<p><strong>Proof</strong>: Not given.

<p><strong>Remarks</strong>:
<ol>
<li>
    When we say f is small/medium/large we are comparing it to the
    mysterious N<sup>log<sub>b</sub>a</sup>, which is the star of the
    show.
<br><br><li>
    So when f is small, T is polynomial; when f is medium, T is
    polynomial times a log power; and when f is big, T is f.
<br><br><li>
    N<sup>log<sub>b</sub>a</sup>/N<sup>&epsilon;</sup> can also be
    written as N<sup>log<sub>b</sub>a-&epsilon;</sup> and
    N<sup>log<sub>b</sub>a</sup>N<sup>&epsilon;</sup> can also be
    written as N<sup>log<sub>b</sub>a+&epsilon;</sup>.
</ol>
<html>
<p>Now we can solve some problems easily and will do two serious
problems after that.

<p><strong>Example:</strong> T(N) = 4T(N/2)+N.<br>
<ul>
<li>
    a=4, b=2, and f(N)=N
<li>
    So N<sup>log<sub>b</sub>a</sup> =
    N<sup>log<sub>2</sub>4</sup> = N<sup>2</sup>.
<li>
    f is small compared to this.  Specifically, f(N) = N =
    N<sup>2-1</sup>; so we are in case 1 with &epsilon;=1.
<li>
    Hence T(N) = &Theta;(N<sup>log<sub>b</sub>a</sup>) =
    &Theta;(N<sup>2</sup>).
</ul>

<p><strong>Example</strong>: T(N) = 2T(N/2) = Nlog(N)
<ul>
<li>
    a=2, b=2, and f(N)=Nlog(N)
<li>
    So we have case 2 of the master theorem with k=1.
<li>
    Hence T(N) = &Theta;(N(logN)<sup>2</sup>).
</ul>

<p><strong>Example</strong>: T(N) = T(N/4) + 2N
<ul>
<li>
    a=1, b=4, and f(N)=2N
<li>
    log<sub>b</sub>a = log<sub>4</sub>1 = 0
<li>
    N<sup>log<sub>b</sub></sup> = N<sup>0</sup> = 1
<li>
    We have case 3 since
    f(N) is &Omega;(N<sup>0+&epsilon;</sup>) for &epsilon=1 and
    af(N/b) = (1)2(N/4) = N/2 = (1/4)f(N)
<li>
    Hence T(N) = &Theta;(N)
</ul>

<p><strong>Example</strong>: T(N) = 9T(N/3) + N<sup>2.5</sup>
<ul>
<li>
    a=9, b=3, f(N)=N<sup>2.5</sup>
<li>
    log<sub>b</sub>a = 2, so N<sup>log<sub>b</sub></sup> =
    N<sup>2</sup>
<li>
    We have case 3 since
    f(N) is &Omega;(N<sup>0+&epsilon;</sup>) for &epsilon=.5 and
    af(N/b)=9f(N/3)= 9(N/3)<sup>2.5</sup>=(1/3)<sup>0.5</sup>f(n).
<li>
    Hence T(n) is O(N<sup>2.5</sup>)
</ul>

<p><strong>Homework:</strong> R-5.4

<h3>5.2.2 Integer Multiplication</h3>

<p>We want to multiply <strong>big</strong> integers.

<p>When we multiply without machine help, we use as knowledge the
times tables for all 1 digit numbers and then do the standard
5th-grade algorithm that requires &Theta;(N<sup>2</sup>) steps for N
digit numbers.  On a computer the known tables are for all 32-bit
numbers (or 64-bit on some machines).  If we want to multiply two
32N-bit numbers on the computer we could write the 5-th grade
algorithm in software (using 32-bit numbers as ``digits'') and again
compute the result in &Theta;(N<sup>2</sup>) time.

<p>To make the wording easier lets assume we only know how to multiply
1-bit numbers and we want to multiply two N-bit numbers X and Y.
Using the 5th grade algorithm we can do this in time
&Theta;(N<sup>2</sup>).  We want to go faster.

<p>If we simply divide the bits in half (the high order and the low
order), we see that to multiply X and Y we need to compute 4
sub-products
<tt style="font-size:larger">Xhi*Yhi+Xhi*Ylo+Xlo*Yhi+Xlo*Ylo</tt>.
(Note that when I write the products above I don't include the
powers of 2 needed.  This corresponds to writing the numbers in the
correct column with the 5th grade algorithm.)

<p>Since addition of K-bit numbers is &Theta;(K) and our multiplications
are only of N/2-bit values we get
<pre style="font-size:larger">
   T(N) = 4T(N/2)+cn
</pre>
But when we apply the master theorem (case 1) we just get
T(N)=N<sup>2</sup>, which is no improvement.  We try again and with
cleverness are able to get by with three instead of four
multiplications.

<p>We compute Xhi*Yhi, Xlo*Ylo, and the miraculous
(Xhi-Xlo)*(Ylo-Yhi).  The miracle multiplication produces the two
remaining terms we need but has added to this two other terms.  But
those are just the (negative of) the two terms we did compute so we
can add them back and they cancel.

<p>The summary is that T(N) = 3T(N/2)+cN, which by the master theorem
(case 1) gives T(N) is &Theta;(N<sup>log<sub>2</sub>3</sup>N</sup>).
But log<sub>2</sub>3&lt;1.585 so we get the surprising

<p><strong>Theorem</strong>: We can multiple two N-bit numbers in
O(N<sup>1.585</sup>) time.
<html>
<h3>5.2.3 Matrix Multiplication</h3>

<p>If you thought the integer multiplication involved pulling a rabbit
out of our hat, get ready.

<p>This algorithm was a sensation when it was discovered by Strassen.
The standard algorithm for multiply two NxN matrices is
&Theta;(N<sup>3</sup>).  We want to do better

<p><img src="figs/matmult.png" align="right">
First try.  Assume N is a power of 2 and break each matrix into 4
parts as shown on the right.  Then X = AE+BG and similarly for Y, Z,
and W.  This gives 8 multiplications of half size matrices plus
&Theta;(N<sup>2</sup>) scalar addition so
<pre style="font-size:larger">
    T(N) = 8T(N/2) + bN<sup>2</sup>
</pre>
We apply the master theorem and get T(N) = &Theta;(N<sup>3</sup>),
i.e., no improvement.

<p>But strassen found the way!  He (somehow, I don't know how) decided
to consider the following 7 (not 8) multiplications of half size
matrices.
<pre style="font-size:larger">
S1 = A(F-H)       S2 = (A+B)H       S3 = (C+D)E       S4 = D(G-E)
S5 = (A+D)(E+H)   S6 = (B-D)(G+H)   S7 = (A-C)(E+F)
</pre>

<p>Now we can compute X, Y, Z, and W from the S's

<pre style="font-size:larger">
X = S5+S6+S4-S2     Y = S1+S2    Z = S3+S4    W = S1-S7-S3+S5
</pre>

<p>This computation shows that
<pre style="font-size:larger">
    T(N) = 7T(N/2) + bN<sup>2</sup>
</pre>

<p>Thus the master theorem now gives

<p><strong>Theorem</strong>(Strassen): We can multiply two NxN
matrices in time O(n<sup>log7</sup>).

<p><strong>Remarks</strong>: 
<ol>
<li>
    log7&lt;2.808 so we can multiply NxN matrices in time
    O(N<sup>2.808</sup>), which is o(N<sup>3</sup>). Amazing.
<li>
    There are even better algorithms, which are more complicated.  I
    think they break the matrix into more pieces.  The current best is
    O(N<sup>2.376</sup>).
</ol>

<p><strong><big><div align=center style="background:#C0C0FF">
    ================ Start Lecture #26 ================
</div></big></strong>

<h2>5.3 Dynamic Programming</h2>

<p>This is a little hard to explain abstractly.

<p>One idea is that we compute a max by maximizing over a bunch of
smaller possibilities (and apply this recursively).  We then compute
these maxima backwards.  That is, instead of breaking the big one into
small ones, we start with the small ones.

<p>Another idea is that we save many intermediate results since they
will be reused.  So we really should analyze the space complexity as
well as the time complexity.

<h3>5.3.1 Matrix Chain-Product</h3>

<p>In this problem we want to multiple n matrices
<pre style="font-size:larger">
           A<sub>0</sub> * A<sub>1</sub> * ... * A<sub>n-1</sub>
</pre>

<p>Unlike the matrix multiplication from last lecture, these matrices
are not all of the same size.  But to be able to multiply the
matrices, the number of columns in A<sub>i</sub> must be the same as the number
of rows in the next matrix A<sub>i+1</sub>.
Specifically A<sub>i</sub> is of size d<sub>i</sub> by d<sub>i+1</sub>.

<p>Matrix multiplication is associative, that is
<pre style="font-size:larger">
         X * (Y * Z) = (X * Y) * Z
</pre>
Hence in computing the matrix chain-product above, we can do the
multiplications is any order we want and get the same answer.  Perhaps
surprisingly, some orders are much faster than others.

<P><strong>Example:</strong> Consider A * B * C where
<br>A is a 20x60 matrix
<br>B is a 60x50 matrix
<br>C is a 50x10 matrix

<p>First we perform the computation as A * (B * C).

<p>B * C requires 50 multiplications for each entry in the output.
Since the output is 60x10, there are 600 entries and hence 30,000
multiplications are required to produce the 60x10 result.

<p>We now must multiple A by this new 60x10 matrix.  This matrix
product requires 60 multiplications for each entry in the output.
Since the output is 20x10, there are 200 entries and hence 12,000
multiplications are required.  So the overall computation requires
42,000 multiplications.

<p>Now we perform the computation as (A * B) * C.

<p>A * B requires 60 multiplications for each entry.  Since the output
is 20x50, there are 1000 entries and hence 60,000 multiplications.  We
now multiply this result by C.  This matrix product requires 50
multiplications for each entry.  Since the output is 20x10, there are
200 entries and hence 10,000 multiplications are required.  So the
overall computation requires 70,000 multiplication.

<p><img src="figs/matrix-chain-prod.png" align="right">
<strong>Example:</strong>
Do on the board the example on the right.  Notice that the answers are
the same for either order of parenthesizing but that the numbers of
multiplications are different.

<p>The <strong>matrix chain-product</strong> problem is to determine
the parenthesization that minimizes the number of multiplications.

<p>(Obvious) solution.  Try every parenthesization and pick the best one.

<p>The obvious solution, often called the <em>brute force</em>
solution, has exponential complexity since the number of ways to
parenthesize an associative expression with n terms is the nth
so-called <em>Catalan number</em>, which is
&Omega;(4<sup>n</sup>/n<sup>3/2</sup>).

<p>We want to do better and will do so using dynamic programming.
We need to define subproblems, subsubproblems, etc and then compute
the answer backwards taking advantage of common subproblems, the
solutions to which we store.

<h4>Defining Subproblems</h4>

<p>Recall that the matrix product whose multiplications we are trying
to minimize is
<pre style="font-size:larger">
A<sub>0</sub> * ... * A<sub>n-1</sub>
</pre>
We want an convenient notation to describe various subproblems and
define N<sub>i,j</sub> to be the minimum number of multiplications
needed to compute the subexpression
<pre style="font-size:larger">
A<sub>i</sub> * ... * A<sub>j</sub>
</pre>
In particular N<sub>0,n-1</sub> is the original problem.


<h4>Characterizing Optimal Solutions</h4>

<p>What follows is the key point of dynamic programming.

<p>The key observation about matrix chain-product is that we can
characterize the optimal solution in terms of optimal solutions of the
subproblems.
(The book calls this the <strong>subproblem optimality</strong> condition.)

<p>We note specifically that when computing
A<sub>i</sub>*...*A<sub>j</sub> there must be a last matrix
multiplication so that
<pre style="font-size:larger">
A<sub>i</sub>*...*A<sub>j</sub> = (A<sub>i</sub>*...*A<sub>k</sub>)*(A<sub>k+1</sub>*...*A<sub>j</sub>)
</pre>
and (the <strong><big>KEY</big></strong> point)
to get the minimum number of multiplications for Ai*...*Aj we must
have the minimum number of multiplications for Ai*...*Ak and for
Ak+1*...Aj.
This says that N<sub>i,j</sub> is just the minimum over all possible
k's of
<br>N<sub>i,k</sub> + N<sub>k,j</sub> + the cost of the final matrix
multiplication.
<br>

<p>What is the cost of the final matrix multiplication?
<ul>
<li>
    The shape of A<sub>i</sub> * ... * A<sub>k</sub> is
    d<sub>i</sub>xd<sub>k+1</sub>
<li>
    The shape of A<sub>k+1</sub> * ... * A<sub>j</sub> is
    d<sub>k+1</sub>xd<sub>j</sub>
<li>
    Hence d<sub>k+1</sub> multiplications are required for each entry
<li>
    The shape of A<sub>i</sub> * ... * A<sub>j</sub> is
    d<sub>i</sub>xd<sub>j+1</sub> so there are
    d<sub>i</sub>d<sub>j+1</sub> entries in the product.
<li>
    So the total number of multiplications is
    d<sub>i</sub>d<sub>k+1</sub>d<sub>j+1</sub>
</ul>

<h4>Designing a Dynamic Programming Algorithm</h4>

<p>We have just seen that

<p>N<sub>i,j</sub> = min<sub>i&le;k&lt;j</sub>
                  {N<sub>i,k</sub> + N<sub>k+1,j</sub>
                  + d<sub>i</sub>d<sub>k+1</sub>d<sub>j+1</sub>}

<p>Let's call this the <em>fundamental equation</em>.

<p>Although the fundamental equation does not look simple, let's not
be discouraged and keep going.  First of all if we apply this
recursively we will get terms in which the subscripts of N are close
together.  We can stop the recursion when the subscripts are equal
since N<sub>i,i</sub>=0 because no multiplications are required since
no matrix product is being computed.

<p>Remember that the idea of dynamic programming is to run the
recursion backwards and start with the smallest problems.

<ul>
<li>
    We have N<sub>i,i</sub>=0 for all i so the smallest problems are
    trivial to solve.
<li>
    What about N<sub>i,i+1</sub>?
<li>
    From the fundamental equation this is min<sub>i&le;k&lt;i+1</sub>
    {N<sub>i,k</sub> + N<sub>k+1,i+1</sub> +
    d<sub>i</sub>d<sub>k+1</sub>d<sub>i+1+1</sub>}
<li>
    But there is only one k satisfying i&le;k&lt;i+1, namely k=i so we
    get that <br>
    N<sub>i,i+1</sub> = {N<sub>i,i</sub> + N<sub>i+1,i+1</sub> +
    d<sub>i</sub>d<sub>i+1</sub>d<sub>i+2</sub>}
<li>
    But N<sub>i,i</sub>=0 so we get<br>
    N<sub>i,i+1</sub> = {d<sub>i</sub>d<sub>i+1</sub>d<sub>i+2</sub>},
    which is the cost of multiplying A<sub>i</sub> by A<sub>i+1</sub>
<li>
    So we can keep building up N<sub>i,i+g</sub> for larger and larger
    g, which gives the following algorithm.
</ul>

<pre style="font-size:larger">
algorithm MatrixChain(d<sub>0</sub>,...,d<sub>n</sub>) (dynamic programming)
   Input:  Sequence d<sub>0</sub>,...,d<sub>n</sub> of positive integer
           corresponding to the dimensions of a chain of matrices
           A<sub>0</sub>,...,A<sub>n-1</sub>
   Output: N<sub>i,j</sub>, the minimum number of multiplications
           needed to compute A<sub>i</sub>*...*A<sub>j</sub>

   for i &larr; 0 to n-1 do           { the simple base case }
      N<sub>i,i</sub> &larr; 0

   for b &larr; 1 to n-1 do           { keep doing larger gaps }
      for i &larr; 0 to n-1-b         { to keep j in bounds )
         j &larr; i+b
         { Calculate N<sub>i,j</sub> from the fundamental equation }
         N<sub>i,j</sub> &larr; +infinity
         for k &larr; i to j-1 do
            N<sub>i,j</sub> &larr; min(N<sub>i,j</sub>,N<sub>i,k</sub>+N<sub>k+1,j</sub>+d<sub>i</sub>d<sub>k+1</sub>d<sub>j+1</sub>)
</pre>

<html>
<p>This algorithm calculates the minimum number of parenthesis needed
for any contiguous subset of the matrices.  In particular
N<sub>0,n-1</sub> gives the minimum number of parenthesis needed for
the entire matrix chain-product.

<p><strong>Example</strong>: Compute, on the board, the minimum number
of multiplications needed for the example drawn previous.  We have
three matrices of shape 2x2, 2x2 and 2x1 respectively so n=3 and the d
vector is 2,2,2,1.

<p><strong>Homework:</strong> R-5.9

<h4>Analyzing the Matrix Chain-product Algorithm</h4>

<p>This is easy now that we know the algorithm.

<p>The algorithm clearly takes O(n<sup>3</sup>) time since it has a
triply nested loop and each loop has O(n) iterations.

<p>But the algorithm gives the number of parentheses not where they
should be placed.  We can fix this problem by storing in
N<sub>i,j</sub> not just the number of parentheses but the index k
that gave us the solution.

<p><strong>Theorem</strong>: We can compute a parenthesization that
minimizes the number of multiplications in a matrix chain-product in
O(n) time, where n is the number of matrices.

<p><img src="figs/dyn-prog.png" align="right">
The diagram on the right illustrates the way dynamic program generates
the elements of N.
<ul>
<li>
    Remember that the diagonal elements are trivial, they are all
    zero.
<li>
    Also remember that everything below the diagonal is not defined:
    It's first subscript is larger than its second so it doesn't
    corresponding to a sequence of matrices.
<li>
    The diagram gets filled in from the diagonal toward the upper
    right.  Look in the algorithm and notice that j=i+b so b is the
    distance from the diagonal.  The algorithm first does b=1, then
    b=2, etc.
<li>
    The figure shows the work involved in calculating the red square,
    N<sub>i,j</sub>.  A minimization is taken over k.  Two values of k
    are illustrated.
    <ol>
    <li>
        (k=i+1) Toward the left N<sub>i,i+1</sub> meets
        N<sub>i+2,j</sub>.
    <li>
        (k=i+4) Toward the right N<sub>i,i+4</sub> meets
        N<sub>i+5,j</sub>.
    </ol>
<li>
    Note that as part of the minimization a product of the d's is
    computed.  This is not shown in the diagram.
</ul>

<p><strong>Remark</strong>: The space complexity is
&Theta;(N<sub>2</sub>) since the only storage used is the array N
(plus a few scalars).

<h3>5.3.2 The General Technique</h3>

<p>There are three properties needed for successful application of
dynamic programming.

<ol>
<li>
    <strong>Splitting into Subproblems:</strong> Must be able to split
    the original into subproblems in a recursive manner (i.e., so that
    subproblems can also be split into sub-subproblems, etc.).  When
    the problem is split enough times, easy subproblems must result.
<li>
    <strong>Subproblem Optimality:</strong> An optimal solution to the
    problem must result from optimal solutions to the subproblems via
    some simple combining operations.
<li>
    <strong>Subproblem Overlap:</strong> Many subproblems
    themselves contain <em>common</em> sub-subproblems.  Thus once we
    have solved the sub-subproblem, we can reuse this solution for
    many subproblems.
</ol>

<h3>5.3.3 The 0-1 Knapsack Problem</h3>

<p>This is the real knapsack problem, a well-known NP-complete
problem.  So clearly we will not get a polynomial time solution (but
we will get close).

<p>Consider a knapsack that can hold a maximum capacity W and a set S
of n items with item i weighing w<sub>i</sub> and giving benefit
b<sub>i</sub>.  All the w's, b's and W are positive integers.

<p>The problem is to maximize the benefit of the items carried subject
to the constraint that the total weight cannot exceed W.  It is called
the 0-1 knapsack problem because, for each item, you leave it behind
(0) or take all of it (1).  You cannot choose to take for example half
of the item as you could in the fractional knapsack problem discussed
at the beginning of this chapter.

<p><strong>Homework:</strong> R-5.12

<h4>A First Attempt at Characterizing Subproblems</h4>

<p>Let Si consist of items 1,2,...,i.  So S<sub>n</sub> is all of S.
The idea is that subproblem k will be to find the optimal way to load
the knapsack using only items in S<sub>k</sub>, i.e., items 1,...,k.
Then the overall answer is obtained by solving S<sub>n</sub>.

<p>It is indeed possible to split the problem this way and when split
enough we get S<sub>1</sub>, which is trivial to optimize since there
is only one element.

<p>There is subproblem overlap as desired.

<p>But it is not at all clear how to extend an optimal solution found
for S<sub>k</sub> to an optimal solution for S<sub>k+1</sub>.  Indeed
finding the optimal solution for S<sub>1</sub>, then S<sub>2</sub>,
then S<sub>3</sub>, corresponds to deciding on each element one at a
time.  This would be a greedy solution and doesn't give a global
optimal.

<p>One could also define S<sub>i,j</sub>, analogous to N<sub>i,j</sub>
above.  However, we again don't have a way to extend an optimal
solutions to subproblems into an optimal solution for a bigger
subproblem.

<h4>A Better Subproblem Characterization</h4>

<p>The problem is that by defining the subproblem simply in terms of
k, we do not get enough information to construct a solution that is
helpful to obtaining a <em>global</em> maximum.

<p>Instead we define subproblems in a more complicated manner.  This
is a stroke of inspiration and is not at all obvious.

<p>Let B[k,w] be the maximum benefit obtainable from items in
S<sub>k</sub> having total weight <em>exactly</em> w.  Our overall
goal is to find B[n,W], the original 0-1 knapsack problem.

<p>This does split the problems into subproblems and when we get down
to B[0,w] we have an trivial problem with solution 0 since we have no
items to select from.  (Also B[k,0] is zero since we are not permitted
to choose any items since our weight limit is 0, but we don't use this
fact.)

<p>The key that makes this solution work is the observation that

<pre style="font-size:larger">
        /      B[k-1,w]                   if w<sub>k</sub>&gt;w
B[k,w] =
        \ max {B[k-1,w], B[k-1,w-w<sub>k</sub>]+b<sub>k</sub>}   otherwise
</pre>

<p>This looks formidable, but is not.  We need to choose a subset of
the first k items with weight exactly w.  If the kth item weighs more
than w, it cannot be used so the best we can do with the first k items
is the same as the best we can do with the first k-1 items.  That was
the easier case.

<p>If the kth item is not heavier than w, it <em>can</em>
be used, but need not be (that gives the two possibilities in the
max).  If we choose not to use the kth item, we get the previous case
(the first term in the max).  If we use the kth item and get total
weight w, the items chosen from the first k-1 must weight
w-w<sub>k</sub>.  Also in this case we gain the benefit of the kth item.

<p>This looks pretty good.  We can loop on k and inside that on w.
B[k,*] just depends on B[k-1,*] so it works fine.

<pre style="font-size:larger">
Algorithm 01knapsack(S,W)
   Input:  A set S of n items each with weight w<sub>i</sub> and benefit b<sub>i</sub>.
           A max weight W.  All are positive; all weights integers.
   Output: B[k,w] the maximum benefit obtainable from a subset of
           the first k items in S having total weight w.

   for w &larr; 0 to W          { base case }
      B[0,w] &larr; 0

   for k &larr; 1 to n
      for w &larr; 0 to W
         if w<sub>k</sub> &gt; w then
            B[k,w] &larr; B[k-1,w]
         else
            B[k,w] =  max (B[k-1,w], B[k-1,w-w<sub>k</sub>]+b<sub>k</sub>)
</pre>

<h4>Analyzing the 0-1 Knapsack Dynamic Programming Algorithm</h4>

<p>This is easy, each loop iteration is constant time so the
complexity is the number of iterations.  The base case loop has
&Theta;(W) iterations and the nested loops have &Theta;(nW).

<p><strong>Theorem</strong>: Given a positive integer W and a set S of
n items each with positive benefit and positive integer weight, we can
find the highest benefit subset of S with total weight at most W in
time &Theta;(nW).

<p><strong>Remarks</strong>:
<ol>
<li>
    The algorithm given only computes the maximum not the subset.  It
    is fairly easy to adapt the algorithm to give the items as well
    (the max computation tells you weather or not to use item k).
<li>
    My simple program needs space &Theta;(nW).  The book gives a more
    subtle argument and an algorithm that only needs space &Theta;(W).
</ol>
<p><strong>End of Remarks</strong>

<h4>Pseudo-Polynomial-Time Algorithms</h4>

<p>At first glance the theorem above seems to say that there is a
polynomial time solution to the 0-1 knapsack problem.  The time
complexity is &Theta;(nW), the product of two numbers characteristic of
the input.  But this is wrong!

<p>The definition of polynomial time is that the algorithm takes time
that is polynomial in the size of the input.  What is the size of the
input?
<ol>
<li>
    We have n items each with two values a benefit b and a weight w.
    We should consider the size of these items to be &Theta;(n) in
    total.  That is we should consider each item to be of constant
    size since otherwise all the operations we have used are not
    constant time.  So that means the factor of n in the time
    complexity is ok.
<li>
    But what about the factor of W?  Well W is an input, doesn't that
    settle it?  No!  The input value W requires only log(W) bits, so
    the size of the input W is &Theta;(log(W)).
<li>
    Hence the input size is &Theta(n+log(W)) and the complexity is
    &Theta;(nW).  Is this a polynomial relationship?
<li>
    Consider a big W say W=2<sup>n</sup>.  Then the input size is
    &Theta;(n+n)=&Theta;(n) and the complexity is
    &Theta;(n2<sup>n</sup>).  So the complexity is exponential in this
    case.
<li>
    Indeed for large enough W, nW exceeds the time for the brute-force
    solution.
</ol>

<p><strong>Remark</strong>: It is common to refer to algorithms like
ours as requiring <strong>pseudo-polynomial time</strong>.  That is it
is polynomial the value of a number in the input, but not on its input
size (the size of its binary representation).

<h1 align="center" style="background:pink">Part II: Graph Algorithms</h1>

<h1>Chapter 6 Graphs</h1>

<!--
<h1>Chapter 7 Weighted Graphs</h1>

LocalWords:  allan gottlieb html nyu Asymptotics extendable descendent
Local Variables:
tab-width: 4
indent-tabs-mode: nil
eval: (defun strong-ajg (arg)
  "Put ARG within <strong></strong> tags"
  (interactive "sstring: ")
  (insert "<strong>" arg "</strong> "))
eval: (define-key mode-specific-map "s" 'strong-ajg)
eval: (define-key html-mode-map "\C-c\C-cl"
                  (lambda () (interactive)
                             (html-list-item) (split-line) (next-line 1)))
eval: (global-set-key [C-f9]
                  (lambda () (interactive) (kill-region (point) (mark))
                             (insert "<sub>") (yank) (insert "</sub>")))
eval: (define-abbrev text-mode-abbrev-table "egeg"
                     "<p><strong>Example</strong>:")
eval: (define-abbrev text-mode-abbrev-table "defdef"
                     "<p><strong>Definition</strong>:")
eval: (define-abbrev text-mode-abbrev-table "remrem"
                     "<p><strong>Remark</strong>:")
eval: (define-abbrev text-mode-abbrev-table "thmthm"
                     "<p><strong>Theorem</strong>:")
eval: (define-abbrev text-mode-abbrev-table "corcor"
                     "<p><strong>Corollary</strong>:")
eval: (define-abbrev text-mode-abbrev-table "pfpf"
                     "<p><strong>Proof</strong>:")
eval: (define-abbrev text-mode-abbrev-table "hw"
                     "<p><strong>Homework:</strong>")
eval: (define-abbrev text-mode-abbrev-table "psps"
                     "<p><strong>Problem Set</strong>")
eval: (define-abbrev text-mode-abbrev-table "lecture"
    (concat "<p><strong><big><div align=center style=\"background:#C0C0FF\">\n"
            "    ================ Start Lecture # ================\n"
            "</div></big></strong>"))
eval: (abbrev-mode 1)
eval: (flyspell-mode 1)
eval: (setq     paragraph-start "[ \t]*$\\|\
\[ \t]*</?\\([A-Za-z]\\([-.A-Za-z0-9= \t\n]\\|\"[^\"]*\"\\|'[^']*'\\)*\\)?>")
End:
-->
<!--  LocalWords:  descendents
 -->
